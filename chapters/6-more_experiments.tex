\minitoc

\vfill

\clearpage

\section{Scalability analysis}
    \label{sec::more_experiments::scalability}
    In the previous chapter, error detection was proven to depend on the scene composition.
    This fact motivates studying training the classifier and testing prediction on different scenes.
    The goal is to prove the resilience of error detection to unseen urban scenes.
    As the annotation process requires a lot of effort, this trait is crucial to guarantee the scalability of this method under the \textbf{large-scale} constraint.\\

    Different configurations are possible, as depicted in Figure~\ref{fig::scalability_study}.
    In the first type of experiments, we train on one urban scene and test on another one.
    The goal is to examine the \texttt{transferability} of the classifier model.
    Experimental results are reported and analyzed in Section~\ref{subsec::more_experiments::scalability::transferability}.
    In a second configuration, the classifier is trained on two scenes and tested on the last one: the objective is to investigate the classifier \texttt{generalization}.
    The results of such experiments are shown in Section~\ref{subsec::more_experiments::scalability::generalization}.
    The last experiment class, whose results are presented in Section~\ref{subsec::more_experiments::scalability::representativeness}, targets the \texttt{representativeness} of a single 3-area dataset by trying multiple train-test split sizes.\\

    \begin{figure}[htbp]
        \ffigbox[\FBwidth]{
            \includestandalone[mode=buildnew, width=.8\textwidth]{figures/scalabitity_graph}
        }
        {
            \caption{
                \label{fig::scalability_study}
                A graph representing possible experiments: arrow origins represent training scenes while test ones are depicted as targets.
                \(Z_i, i=1,2,3\) represent the urban zones.
                All these nodes are assembled in one, meaning that all urban scenes were aggregated in on train/test node.
                The numbers indicate in which section each experiment is analyzed.
            }
        }
    \end{figure}

    We will see how \texttt{Building errors} depend on the training scene, in contrast to \texttt{Facet errors}.
    The latter will prove to be more transferable and generalizable than the first one.
    We will also discuss how every modalities play a role in error prediction.
    Image-based features will demonstrate to be the most valuable compared to height-based ones.
    Eventually, we will review each \texttt{atomic} error prediction sensitivity provided the training set.

    \subsection{Transferability study}
        \label{subsec::more_experiments::scalability::transferability}
        In this configuration, we test how transferable the learned classifiers are from one urban scene to another.
        We train on a zone $Z_i$ and test on another one $Z_j$.
        We will denote each transferability experiment by the couple $(Z_i, Z_j)$ or by $Z_i \rightarrow Z_j$.
        Six transferability couples are possible and are tested with all possible feature configurations.
        Mean and standard deviation of F-scores are plotted, for each label and experiment, in Figure~\ref{fig::f_score_transferability_f3}.
        For more details, refer to Tables~\ref{tab::transferability_f3} and~\ref{tab::all_f-scores_transferability_f3}, which compiles all precision, recall and F-score test results.\\
        
        \thisfloatsetup{subfloatrowsep=none}
        \begin{figure}[htbp]
            \ffigbox[\textwidth]{
                \begin{subfloatrow}
                    \centering
                    \ffigbox[\textwidth]{
                        \includestandalone[mode=buildnew, width=\textwidth]{figures/results/transferability/building}
                    }{
                        \caption{
                            \label{subfig::f_score_transferability_f3_building}
                            \texttt{Building errors.}
                        }
                    }
                \end{subfloatrow}
                \vskip2em
                \begin{subfloatrow}
                    \ffigbox[\textwidth]{
                        \includestandalone[mode=buildnew, width=\textwidth]{figures/results/transferability/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_transferability_f3_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_transferability_f3}
                    Mean F-score and standard deviation for the transferability study.
                    \textbf{El.} (\textit{resp.} \textbf{Na.} and \textbf{P13.}) corresponds to \textbf{Elancourt} (\textit{resp.} \textbf{Nantes} and \textbf{Paris-13}).
                    This is a vizualization of results reported in Table~\ref{tab::f_score_transferability_f3}.
                }
            }
        \end{figure}

        First, a \texttt{coherence} analysis is performed.
        We compare the results of the transferability experiments to the ablation results with the same training scene.
        This is achieved by looking, for a given area $Z_i$ in all couples $(Z_i, Z_j)_{\forall j \neq i}$, at the differences between Tables~\ref{tab::all_f-scores_transferability_f3} and~\ref{tab::all_f-scores_ablation_f3}.
        The goal is to see to which extent the test zone looks like the training one.\\

        All these comparisons are provided in Table~\ref{tab::transferability_comparison}.
        A color scheme was devised to encode the amplitude of change.
        The resemblance between \textbf{Paris-13} and \textbf{Nantes} are stricking.
        In fact, when training on one of the two zones and testing on \textbf{Elancourt}, out of the nine labels only two are different.
        \texttt{BUS} has a gain between \SIrange[range-phrase={ and }]{5}{15}{\percent} when trained on \textbf{Paris-13} while its F-score records a drop in the range \SIrange{-15}{-5}{\percent} if trained on \textbf{Nantes}.
        \texttt{FUS}, however, looses between \SIrange[range-phrase={ and }]{-45}{-35}{\percent} when trained on \textbf{Paris-13} and \SIrange[range-phrase={ and }]{-35}{-25}{\percent} for \textbf{Nantes}.
        Conversely when training on \textbf{Elancourt} and testing on the other two areas, the comparisons are not identical.
        However, from a general standpoint, we can see how the \texttt{building errors} are always recording losses when testing on both \textbf{Paris-13} and \textbf{Nantes}.
        Except for \texttt{FIT}, the inverse situation is obseved for \texttt{Facet errors} where labels are either better detected or stable.\\

        \begin{table}[htbp]
            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{| c | c | c c c c |c c c c c|}
                \hline
                && \texttt{BOS} & \texttt{BUS}&\texttt{BIB}&\texttt{BIT}&\texttt{FOS} & \texttt{FUS}&\texttt{FIB}&\texttt{FIT}&\texttt{FIG}\\
                \hline
                \multirow{6}{*}{\rotatebox{90}{\texttt{Coherence}}} & \textbf{El.} \(\rightarrow\) \textbf{Na.} & \cellcolor{LOSS2535} & \cellcolor{LOSS1525} \textbf{All} & \cellcolor{LOSS2535} & \cellcolor{LOSS1525} \textbf{All} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{Im.} & \cellcolor{GAIN3545} \textbf{Im.} & \cellcolor{STBL} \textbf{Im.} & \cellcolor{GAIN0515} \\
                & \textbf{El.} \(\rightarrow\) \textbf{P13} & \cellcolor{LOSS1525} & \cellcolor{LOSS1525} \textbf{Im.} & \cellcolor{LOSS1525} & \cellcolor{LOSS2535} & \cellcolor{STBL} & \cellcolor{GAIN3545} \textbf{Im.} & \cellcolor{GAIN2535} \textbf{Im.} & \cellcolor{LOSS1525} & \cellcolor{GAIN0515} \\
                & \textbf{Na.} \(\rightarrow\) \textbf{P13} & \cellcolor{LOSS1525} & \cellcolor{LOSS2535} \textbf{Im.} &  & \cellcolor{GAIN1525} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{GAIN2535} \textbf{Geom.} & \cellcolor{GAIN0515} &  & \cellcolor{GAIN0515} \textbf{Hei.} \\
                & \textbf{Na.} \(\rightarrow\) \textbf{El.} & \cellcolor{GAIN2535} & \cellcolor{LOSS0515} \textbf{Im.} & \cellcolor{GAIN1525} \textbf{Im.} & \cellcolor{GAIN0515} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{LOSS2535} \textbf{Im.} & \cellcolor{STBL} \textbf{Im.} & \cellcolor{GAIN0515} & \cellcolor{LOSS0515} \\
                & \textbf{P13} \(\rightarrow\) \textbf{Na.} & \cellcolor{LOSS1525} & \cellcolor{LOSS1525} \textbf{Im.} &  & \cellcolor{GAIN2535} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{LOSS2535} & \cellcolor{STBL} \textbf{Im.} &  & \cellcolor{LOSS1525} \\
                & \textbf{P13} \(\rightarrow\) \textbf{El.} & \cellcolor{GAIN1525} & \cellcolor{GAIN0515} \textbf{All} & \cellcolor{GAIN1525} \textbf{Im.} & \cellcolor{GAIN0515} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{LOSS3545} & \cellcolor{STBL} \textbf{All} &\cellcolor{GAIN0515} & \cellcolor{LOSS0515}\\
                \specialrule{.2em}{.1em}{.1em}
                \multirow{6}{*}{\rotatebox{90}{\texttt{Projectivity}}} & \textbf{El.} \(\rightarrow\) \textbf{Na.} & \cellcolor{LOSS1525} & \cellcolor{LOSS1525} \textbf{All} &  & \cellcolor{LOSS0515} \textbf{All} & \cellcolor{STBL} & \cellcolor{LOSS0515} \textbf{Im.} & \cellcolor{LOSS0515} \textbf{Im.} & \cellcolor{GAIN0515} \textbf{Im.} & \cellcolor{STBL} \\
                & \textbf{El.} \(\rightarrow\) \textbf{P13} & \cellcolor{STBL} & \cellcolor{STBL} \textbf{Im.} &  & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{LOSS2535} \textbf{Im.} & \cellcolor{LOSS0515} \textbf{Im.} &  & \cellcolor{STBL} \\
                & \textbf{Na.} \(\rightarrow\) \textbf{P13} & \cellcolor{LOSS2535} & \cellcolor{LOSS1525} \textbf{Im.} &  & \cellcolor{GAIN1525} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{LOSS1525} \textbf{Geom.} & \cellcolor{STBL} &  & \cellcolor{STBL} \textbf{Hei.} \\
                & \textbf{Na.} \(\rightarrow\) \textbf{El.} & \cellcolor{LOSS0515} & \cellcolor{LOSS1525} \textbf{Im.} & \cellcolor{STBL} \textbf{Im.} & \cellcolor{LOSS1525} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{Im.} & \cellcolor{GAIN1525} \textbf{Im.} & \cellcolor{LOSS0515} & \cellcolor{STBL} \\
                & \textbf{P13} \(\rightarrow\) \textbf{Na.} & \cellcolor{LOSS0515} & \cellcolor{LOSS2535} \textbf{Im.} &  & \cellcolor{GAIN1525} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{LOSS1525} & \cellcolor{STBL} \textbf{Im.} &  & \cellcolor{STBL} \\
                & \textbf{P13} \(\rightarrow\) \textbf{El.} & \cellcolor{LOSS0515} & \cellcolor{LOSS1525} \textbf{All} & \cellcolor{GAIN0515} \textbf{Im.} & \cellcolor{LOSS1525} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{GAIN1525} \textbf{All} & \cellcolor{LOSS0515} & \cellcolor{STBL}\\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption[
                Evolution of the F-score value, for each error, between transferability experiments and the ablation study.
            ]{
                \label{tab::transferability_comparison}
                Evolution of the F-score value, for each error, between transferability experiments and the ablation study (\textit{cf.} Section~\ref{subsec::experiments::baseline_feature_analysis::ablation}).
                Feature sets having a significant impact on the classification results are mentioned in the corresponding cell (\textit{cf.} Table~\ref{tab::all_f-scores_transferability_f3}).
                The color indicates the magnitude of change: 
                \textcolor{LOSS45}{\(\blacksquare\)}: (\SIrange[range-phrase={,  }]{-100}{-45}{\percent}]--
                \textcolor{LOSS3545}{\(\blacksquare\)}: [\SIrange[range-phrase={,  }]{-45}{-35}{\percent})--
                \textcolor{LOSS2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                \textcolor{LOSS1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                \textcolor{LOSS0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-15}{-5}{\percent}) --
                \textcolor{STBL}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-5}{5}{\percent}) --
                \textcolor{GAIN0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{5}{15}{\percent}) --
                \textcolor{GAIN1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{15}{25}{\percent}) --
                \textcolor{GAIN2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{25}{35}{\percent}) --
                \textcolor{GAIN3545}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{35}{45}{\percent}) --
                \textcolor{GAIN45}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{45}{100}{\percent}] --
                When two null F-scores are compared, the cell is colored in white \(\square\): neither positive nor negative.
            }
        \end{table}

        Second, we investigate how the urban scene composition helps predicting defects in an unseen one.
        This is called the \texttt{projectivity} comparison.
        For a given test scene $Z_j$ in couples $(Z_i, Z_j)_{\forall i \neq j}$, we compare again results from Tables~\ref{tab::all_f-scores_transferability_f3} and~\ref{tab::all_f-scores_ablation_f3}.
        A gain in performance for a label can be interpreted by the transferability power of its learning process.\\

        Comparisons are detailed in Table~\ref{tab::transferability_comparison} with the same color scheme as for \texttt{coherence} comparisons.
        Error family wise, we can summarize these comparisons as follows:
        \begin{description}
            \item[\texttt{Building errors}:] Out of 20 possible \texttt{projectivity} comparisons, 13 yield negative results.
                    This proves how hard it is to transfer learning for this error family.
            \item[\texttt{Facet errors}:] Conversely, only 7 out of 27 \texttt{projectivity} scores are worse compared to training and testing on the same test area.
                    One notable pattern is the stability of \texttt{FOS} and \texttt{FIB} errors no matter the chosen transferability couple.
        \end{description}

        When looking at \texttt{Facet errors} transferability power, we can see how training on \textbf{Nantes} and testing on \textbf{Paris-13} yields the same scores as when the opposite experiment is conducted.
        We can also see how \textbf{Elancourt} is best for learning \texttt{FIT} as it transfers well to \textbf{Nantes} while having no effect on \textbf{Paris-13}.
        In contrast, both comparisons are negative when training of the last two areas.
        With the exception of this label, training on the latter dense zones proves to be advantageous than learning on \textbf{Elancourt}, \texttt{projectivity} wise.
        In terms of \texttt{Building errors}, clear discernible patterns are hard to find.
        This agrees with the previous conclusion that this error family is not easy to learn.
        The only exceptions were \texttt{BIT} and \texttt{BUS}.
        For these two labels, training on \textbf{Elancourt} proved to be slightly better than on the other scenes.\\

        As we can suspect from looking at the large standard deviations illustrated in Figure~\ref{fig::f_score_transferability_f3}, modalities play an instrumental role in insuring the transferability experiments.
        All modalities that were standing out in terms of the F-score were mentioned in the corresponding cell in Table~\ref{tab::transferability_comparison}.
        As intended (\textit{cf.} Section~\ref{subsec::learned_evaluation::baseline::image}), image based features are instrumental in transferability for \texttt{FUS}, \texttt{BUS}, \texttt{FIB} and \texttt{BIB}.
        Geometric features, on the other hand, play a lesser role helping mostly with \texttt{BIT} transferability when training on dense urban scenes as well as \texttt{FUS} for the \textbf{Nantes} \(\rightarrow\) \textbf{Paris-13} experiment.
        Actually, \texttt{BIT} is a purely topological error that can be learned with the help of these structural features.
        On the contrary, adding more modalities would only confuse the classifier as also seen in the ablation study in Table~\ref{tab::ablation_f3}.
        Height based errors alone are not crucial for transferability, intervening only once in the case of \texttt{FIB} in the \textbf{Nantes} \(\rightarrow\) \textbf{Paris-13} experiment.
        However, it has a bigger impact when added to image based ones (\textit{cf.} cells with \textbf{All} in Table~\ref{tab::transferability_comparison}).
        All these previous findings further justify why we did not leave out any modality, as they are more frequently critical for transferability than in the ablation study (\textit{cf.} Table~\ref{tab::all_f-scores_ablation_f3}).\\

    \subsection{Generalization study}
        \label{subsec::more_experiments::scalability::generalization}
        We try to find out how omitting one urban zone from the training dataset affects the test results on that same area.
        An other way to look at it is, from an operational point of view, to find out how much learning on a union of many urban scenes is helpful when testing on an unseen one.
        Experiments that merge all zones except $Z_i$ ($\underset{\forall j \neq i}{\bigcup} Z_j$) for training and test on $Z_i$ are denoted by the couple $(\underset{\forall j \neq i}{\bigcup} Z_j, Z_i)$ or by $ \underset{\forall j \neq i}{\bigcup} Z_j \rightarrow Z_i$.
        There are three possibilities: \textbf{Elancourt} $\cup$ \textbf{Nantes} \(\rightarrow\) \textbf{Paris-13}, \mbox{\textbf{Paris-13}}\,$\cup$\,\textbf{Nantes}\,\(\rightarrow\)\,\textbf{Elancourt} and \mbox{\textbf{Paris-13}}\,$\cup$\,\textbf{Elancourt}\,\(\rightarrow\)\,\textbf{Nantes}.
        The F-score evolution, per experiment and label, is depicted in Figure~\ref{fig::f_score_generalization_f3}.\\
    
        \thisfloatsetup{subfloatrowsep=none}
        \begin{figure}[htbp]
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \centering
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=7.5cm]{figures/results/generalization/building}
                    }{
                        \caption{
                            \label{subfig::f_score_generalization_f3_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=7.5cm]{figures/results/generalization/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_generalization_f3_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_generalization_f3}
                    Mean F-score and standard deviation for the generalization study per test zone.
                    Scores are also provided in Table~\ref{tab::f_score_generalization_f3}.
                }
            }
        \end{figure}

        \begin{table}[htbp]
            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{| c | c c c c |c c c c c|}
                \hline
                & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG} \\
                \hline
                \textbf{Elancourt} & \cellcolor{LOSS3545} \textbf{Im.} & \cellcolor{LOSS3545} \textbf{Im.} & \cellcolor{LOSS2535} & \cellcolor{LOSS1525} \textbf{Hei.} & \cellcolor{STBL} & \cellcolor{GAIN45} & \cellcolor{GAIN2535} \textbf{Im.} & \cellcolor{LOSS1525} & \cellcolor{GAIN0515} \\
                \textbf{Nantes} & \cellcolor{STBL} \textbf{Im.} & \cellcolor{LOSS1525} \textbf{Im.} &  & \cellcolor{GAIN1525} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{LOSS0515} \textbf{Im.} & & \cellcolor{STBL} \\
                \textbf{Paris-13} & \cellcolor{LOSS0515} & \cellcolor{STBL} \textbf{Im.} & \cellcolor{GAIN1525} \textbf{Im.} & \cellcolor{GAIN0515} \textbf{Geom.} & \cellcolor{STBL} & \cellcolor{LOSS45} \textbf{Im.} & \cellcolor{STBL} \textbf{Im.} & \cellcolor{GAIN0515} & \cellcolor{LOSS0515} \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption[
                Evolution of the F-score value, for each error and test area, between generalization experiments and the ablation study.
            ]{
                \label{tab::generalization_comparison}
                Evolution of the F-score value, for each error and test area, between generalization experiments and the ablation study (\textit{cf.} Section~\ref{subsec::experiments::baseline_feature_analysis::ablation}).
                Feature sets having a significant impact on the classification results are mentioned in the corresponding cell (\textit{cf.} Table~\ref{tab::all_f-scores_generalization_f3}).
                The color indicates the magnitude:
                \textcolor{LOSS45}{\(\blacksquare\)}: (\SIrange[range-phrase={,  }]{-100}{-45}{\percent}]--
                \textcolor{LOSS3545}{\(\blacksquare\)}: [\SIrange[range-phrase={,  }]{-45}{-35}{\percent})--
                \textcolor{LOSS2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                \textcolor{LOSS1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                \textcolor{LOSS0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-15}{-5}{\percent}) --
                \textcolor{STBL}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-5}{5}{\percent}) --
                \textcolor{GAIN0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{5}{15}{\percent}) --
                \textcolor{GAIN1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{15}{25}{\percent}) --
                \textcolor{GAIN2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{25}{35}{\percent}) --
                \textcolor{GAIN3545}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{35}{45}{\percent}) --
                \textcolor{GAIN45}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{45}{100}{\percent}] --
                When two null F-scores are compared, the cell is colored in white \(\square\): neither positive nor negative.
            }
        \end{table}
            
        We compare the F-score ratios of these experiments, shown in Table~\ref{tab::all_f-scores_generalization_f3}, with ones from the ablation study for each test area.
        These comparisons are compiled in Table~\ref{tab::generalization_comparison}.\\
    
        We start again with a comparison with respect to error families:
        \begin{description}
            \item[\texttt{Building errors}:] Out of the 11 possibilities, 6 yield worse results.
                    This is approximatly \SI{10}{\percent} less than the transferability study.
            \item[\texttt{Facet errors}:] 4 out of 14 comparisons exhibit the same trend.
                    This is sensibly the same ratio as the transferability comparisons with only a \SI{2}{\percent} jump.
        \end{description}
        This confirms again how difficult \texttt{Building errors} are to learn compared to \texttt{Facet errors}.
        However, this time combining urban scenes yielded better results.
        We can suspect that this is the consequence of these studied zones being complementary in terms of learning.\\
        
        Similarly to the previous study, image and height modalities play a major role in error detection.
        Image based features are crucial for \texttt{FIB}, \texttt{FUS}, \texttt{BOS}, \texttt{BIB} and \texttt{BUS} detection (Table~\ref{tab::generalization_comparison}).
        Height based attributes has always a minor role contributing only once for \texttt{BIT} as well as geometric features.
        Image based errors proves again how instrumental they are in learning, even for topological errors.\\
        
        Based on the generalization study we can deduce the best zones to train each error label on.
        In fact, if a zone yields the worst scores it means that it contains the hardest instances that are beneficial for training.
        Conversely, if taken into account in training, it will help the classifier yield better results when testing on other scenes.\\

        For \texttt{Building errors} a clear pattern can be observed compared to the transferability study.
        \textbf{Elancourt} when left out suffers the most.
        In fact, as \textbf{Nantes} and \textbf{Paris-13} are very dense, buildings are homogeneous in terms of shape with moslty Haussman style or parallelepiped shaped buildings.
        On the other hand, \textbf{Elancourt} has more heterogeneous buildings allowing a better learning.
        However, the differences are not that big between the latter and \textbf{Nantes} as was proved in the \texttt{projectivity} comparisons.
        \textbf{Nantes} has some interesting observations to learn on, especially for \texttt{BIB}.\\

        Regarding \texttt{Facet errors}, the differences are sharper if we set aside \texttt{FOS} as it is always well learned over all sets.
        For \texttt{FUS} it is \textbf{Paris-13} that suffers the most when left out.
        This is understandable as Haussman style buildings are usually under-segmented and hence allow for a better learning than other sets, especially \textbf{Elancourt}.
        \textbf{Paris-13} is also the better alternative for \texttt{FIG}.
        Next, we can observe how \texttt{FIB} is better learned on \textbf{Nantes} as it also presents compact buildings with a lot of small facets that can be easily misestimated.
        However, \texttt{FIT} is the exception as \textbf{Elancourt} offers the better alternative, as it comprises various building types that cannot be easily modeled.
    
    \subsection{Representativeness study}
        \label{subsec::more_experiments::scalability::representativeness}
        The objective is to find out, after merging all training samples from all datasets, what is the minimal amount of data that can guaranty stable predictions.
        Figure~\ref{fig::f_score_representativeness_f3} depicts the F-score as a function of training ratios (between \SIrange[range-phrase={ and }]{20}{70}{\percent}) and \texttt{atomic} errors.\\
                
        \thisfloatsetup{subfloatrowsep=none}
        \begin{figure}[htbp]
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/representativeness/building}
                    }{
                        \caption{
                            \label{subfig::f_score_representativeness_f3_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/representativeness/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_representativeness_f3_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{\label{fig::f_score_representativeness_f3} Mean F-score and standard deviation for the representativeness experiments depending on the training set size.}
            }
        \end{figure}
            
        We note the high stability of the F-score.
        This indicates that having a small heterogeneous dataset is not detrimental to the learning capacity and can be even the most suitable solution.
        \texttt{BOS}, \texttt{FOS}, and \texttt{FIG} have a standard deviation under \SI{2}{\percent}, as opposed to \texttt{FIB}, \texttt{BIT} and \texttt{FIT}.
        Indeed, the latter vary greatly with respect to the training size, and even a larger standard deviation according to the feature configurations.\\

        However, when looking at the best feature configurations, the variability of the F-scores towards the training size dwindles.
        Extrinsic modalities play again a big role in these experiments as shown in Table~\ref{tab::all_f-scores_representativeness_f3}.
        The issue, in this setting, is the lack of consistency when it comes to which modality is the most important.
        This may be explained by the fact that training samples do not always contain the same instances.\\

        For each error we can then determine the minimum training size needed to achieve stability.
        As seen \texttt{BOS}, \texttt{FOS}, and \texttt{FIG} are very stable and a \SI{20}{\percent} training set is sufficient to retieve similar performance to the initial ablation study.
        For \texttt{BUS} (\textit{resp.} \texttt{FUS}), using the best performing features, \SI{20}{\percent} is also enough to achieve stability in the range \SIrange{48}{52}{\percent} (\textit{resp.} \SIrange{47}{53}{\percent}).
        \texttt{FIB} can also be stabilized between \SIrange[range-phrase={ and }]{42}{46}{\percent} but requires a \SI{30}{\percent} of instances to train on.
        However, \texttt{BIT}, \texttt{FIT} and \texttt{BIB} prove to be harder to stabilize with mean F-scores around \SI{16}{\percent}, \SI{9}{\percent} and \SI{19}{\percent} respectively.
        We can hence observe how, understandably, mixing the training sets produces scores that averages out the ones from the ablation study.
        On the other hand, more sophisticated features are still required to help predicting the less frequent and more semantic labels.
    
\section{The \texorpdfstring{\acrlong*{acr::efin}}{eFin} impact on predictions}
    \label{sec::more_experiments::finesse}
    In this section, we reproduce the experimental settings described in Section~\ref{sec::experiments::baseline_feature_analysis} and Section~\ref{sec::more_experiments::scalability}.
    First, the \textbf{\gls{acr::efin}} is set to be 2.
    The goal is to find out how good, transferable and stable are the predictions of modeling error families: \texttt{Building errors} and \texttt{Facet errors}.
    Afterwhat, \textbf{\gls{acr::efin}} is fixed at level 1.
    The idea is to observe the predictability of defectuous model compared to \texttt{Valid} ones.
    
    \subsection{Error family detection}
        \label{subsec::more_experiments::finesse::2}
        \begin{table}[htbp]
            \footnotesize
            \centering
            \begin{tabular}{|c | c c | c c | c c | c c |}
                \hline
                & \multicolumn{8}{c|}{\textbf{Elancourt}}\\
                \hline
                & \multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{1.8cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{Building errors} & \textbf{99.76} & \textbf{85.96} & 99.82 & 85.88 & 99.88 & 85.57 & \textbf{100} & 85.55 \\
                \hline
                \texttt{Facet errors} & 91.79 & 89.79 & 92.65 & 89.40 & \textbf{93.21} & \textbf{89.45} & 93.46 & 89.16 \\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Nantes}}\\
                \hline
                & \multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{1.8cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{Building errors} & 85.98 & 67.27 & 87.59 & 67.79 & 85.75 & 68.32 & \textbf{86.90} & \textbf{69.23} \\
                \hline
                \texttt{Facet errors} & 91.20 & 94.01 & 91.37 & 94.36 & 91.20 & 94.35 & \textbf{91.73} & \textbf{94.21}\\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Paris-13}}\\
                \hline
                & \multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{1.8cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{Building errors} & 97.36 & 68.76 & 97.36 & 68.76 & 97.36 & 68.76 & 97.36 & 68.76 \\
                \hline
                \texttt{Facet errors} & 99.03 & 91.26 & 99.03 & 91.26 & 99.03 & 91.26 & 99.03 & 91.26 \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::ablation_f2}
                Feature ablation study on the three datasets for the \textbf{\gls{acr::efin}} = 2 case.
                Results are expressed in percentage.
                All four modality configurations are compared across both family errors.
            }
        \end{table}
    
        \begin{figure}[htbp]
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[0.5\textwidth]{
                        \centering
                        \includestandalone[mode=buildnew, width=.45\textwidth]{figures/results/finesse_2/ablation}
                    }{
                        \caption{
                            \label{subfig::f_score_ablation_f2}
                            Test scores for classifiers trained on the same zone.
                        }
                    }
                    \ffigbox[0.5\textwidth]{
                        \centering
                        \includestandalone[mode=buildnew, width=.45\textwidth]{figures/results/finesse_2/transferability}
                    }{
                        \caption{
                            \label{subfig::f_score_transferability_f2}
                            Transferability experiments.
                        }
                    }
                \end{subfloatrow}
                \vskip2em
                \begin{subfloatrow}
                    \ffigbox[\FBwidth]{
                        \centering
                        \includestandalone[mode=buildnew, width=.45\textwidth]{figures/results/finesse_2/representativeness}
                    }{
                        \caption{
                            \label{subfig::f_score_representativeness_f2}
                            Representativeness experiments.
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_f2}
                    F-score mean and standard deviation for the feature ablation study outcomes per zone for \textbf{\gls{acr::efin}} level 2.
                }
            }
        \end{figure}
    
        We start by the ablation study.
        Table~\ref{tab::ablation_f2} reveals that adding more remote sensing modalities do not change the prediction results dramatically.
        This is perfectly illustrated, in Figure~\ref{fig::f_score_f2}, by the low variance of F-scores for the three areas of interest.
        We can explain this by referring to the analysis at the \textbf{\gls{acr::efin}} level 3 in Section~\ref{subsec::experiments::baseline_feature_analysis::ablation}.
        Two main reasons could be noticed:
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item Out of all \texttt{atomic} errors, only \texttt{BUS}, on all datasets, and both, \texttt{BIT} and \texttt{FIB}, on \textbf{Elancourt}, have been greatly impacted by a change in feature configurations.
                    Both occur under \SI{25}{\percent}, \SI{5}{\percent} and \SI{12}{\percent} respectively.
            \item On the other hand, all the other errors are unaffected, especially, \texttt{FOS} and \texttt{FIG} which are prevalent on all datasets.
        \end{itemize}
        The last observation added to the fact that these labels are, in a large capacity, easily detected individually\footnote{
            \texttt{FOS} (\textit{resp.} \texttt{FIG}) achieves more than \SI{90}{\percent} (\textit{resp.} \SI{80}{\percent}) in F-score, as depicted in Figure~\ref{subfig::f_score_ablation_f3_facet}.
        } helps understanding why the F-score reaches at least \SI{90}{\percent} for the \texttt{Facet errors} family (Figure~\ref{subfig::f_score_ablation_f2}) in constrast with \texttt{Building errors}.
        Moreover, we can see a smaller discrepancy (below \SI{5}{\percent}) between F-scores on different scenes for \texttt{Facet errors} compared to \texttt{Building errors} with \SI{15}{\percent}.\\

        The transferability study (Figure~\ref{subfig::f_score_transferability_f2}) compares the F-scores with the ablation study provided in Figure~\ref{subfig::f_score_ablation_f2}.
        Out of all 12 possible \texttt{projectivity} comparisons, only 2 exhibit a decrease in error discrimination.
        Both happen when training \texttt{Building errors} on \textbf{Nantes} and \textbf{Paris-13} and testing on \textbf{Elancourt} with approximatly \SI{-4}{\percent} and \SI{-3}{\percent} respectively.
        This confirms the fact that \textbf{Elancourt} was best for training \texttt{Building errors} as established in Section~\ref{subsec::more_experiments::scalability::generalization}.
        Same as with the ablation study we can see how the stability in \texttt{FOS} and \texttt{FIG} is reflected by a stability of \texttt{Facet errors}.
        For this reason, we skip the generalization study, all together, at this section.\\

        The representativeness study conducted for the \textbf{\gls{acr::efin}} level 2 results in the F-scores that are illustrated in Figure~\ref{subfig::f_score_representativeness_f2}.
        Family detection scores are very stable across all different tested split ratios.
        Moreover, in contrast to \texttt{atomic} errors results (cf. Figure~\ref{fig::f_score_representativeness_f3}), F-scores do not vary by more than \SI{1}{\percent} in mean and standard deviation.
        This further proves that at \textbf{\gls{acr::efin}} level 2, error family prediction is evened out independent of different split ratios, as opposed to higher \texttt{finesse} errors.
        Again, it benefits from the higher heterogeneity of the training set with multiple areas.
    
    \subsection{Detection of erroneous models}
        \label{subsec::more_experiments::finesse::1}
        \begin{table}
            \footnotesize
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{|c | c c | c c | c c | c c |}
                \hline
                & \multicolumn{8}{c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} \\
                \hline
                \texttt{Erroneous} & 99.95 & $\frac{1}{57}$ & 99.95 & $\frac{1}{57}$ & 99.95 & $\frac{0}{57}$ & 99.95 & $\frac{1}{57}$ \\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Nantes}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} \\
                \hline
                \texttt{Erroneous} & 99.84 & $\frac{0}{55}$ & 99.84 & $\frac{0}{55}$ & 100 & $\frac{0}{55}$ & \textbf{100} & $\frac{0}{55}$ \\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Paris-13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} &  \(\bm{Rec}\) & \texttt{Valid} \\
                \hline
                \texttt{Erroneous} & 99.77 & $\frac{3}{21}$ & 99.77 & $\frac{3}{21}$ & 99.77 & $\frac{3}{21}$ & 99.77 & $\frac{3}{21}$ \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption{
                \label{tab::ablation_f1}
                Test results expressed in percentage for the \textbf{\gls{acr::efin}} = 1 case.
                All four modality configurations are compared across both family errors.
            }
        \end{table}
        
        For the level 1 in \textbf{\gls{acr::efin}}, we start with the feature ablation experiments.
        Since valid samples are very rare in our case, it is expected that it will be very difficult to detect these instances.
        In consequence, in Table~\ref{tab::ablation_f1}, we choose to report correctly \texttt{Valid} buildings instead of computing the precision score in percentage.\\
            
        At this level, even more that the error family semantic degree, feature configurations have virtually no impact on test results: \textbf{Elancourt} was the only exception when image features are added to geometric ones.
        Furthermore, we confirm expectations as, at most, only 1 out of 57 (\textit{resp.} 0 out of 55 and 3 out of 21) valid instances are detected for \textbf{Elancourt} (\textit{resp.} \textbf{Nantes} and \textbf{Paris-13}).
        As a consequence, we do not report the rest of previously conducted experiments for this \textbf{\gls{acr::efin}} level.
        Indeed, There is no interest in comparing prediction transferability, generalization or representativeness if we hardly detect them at all on the same training scene.

\section{Classifier choice analysis}
    \label{sec::more_experiments::classifier}
    The aim of this section is to find out how beneficial the use of \glspl{acr::svm} can be if used instead of \glspl{acr::rf}.
    Two reasons motivate this experimental comparison:
    \begin{enumerate}[label=\roman*)]
        \item \glspl{acr::svm} are more adapted to kernels, as we plan trying graph kernels as feature extractors;
        \item \glspl{acr::svm} are better suited for unbalanced labels, which is the case of \texttt{BIT} and \texttt{FIT} for instance.
    \end{enumerate}
    Hereafter, we first describe the urban scenes that are studied.
    Next, we compare the results obtained using the \gls{acr::svm} classifier to the ones resulting from the \gls{acr::rf}.
    We end with a comparison of feature importances computed for the two classifiers.

    \subsection{The studied urban areas}
        \label{subsec::more_experiments::classifier::dataset}
        According to the findings of the previous sections (\textit{cf.} Sections~\ref{subsec::more_experiments::scalability::transferability} and~\ref{subsec::more_experiments::finesse::2}), \textbf{Paris-13} and \textbf{Nantes} are similar compared to \textbf{Elancourt}.
        Moreover, the latter area contains a lot more instances than the others which is not ideal for comparisons.
        As a consequence, both \textbf{Paris-13} and \textbf{Nantes} were fused in one set denoted from now by \textbf{Na-P13}.
        It contains 1226 buildings compared to 2007 instances of \textbf{Elancourt}.
        In Figure~\ref{fig::error_fused_statistics}, we remind the reader of the \textbf{Elancourt} area error distributions which are compared this time to statistics from the fused set \textbf{Na-P13}.\\
    
        \begin{figure}[htpb]
            \centering
            \ffigbox[\textwidth]{
                \begin{subfloatrow}
                    \ffigbox[\textwidth]{
                        \includestandalone[mode=buildnew, height=8cm]{figures/datasets/fused/lod1_stats}
                    }{
                        \caption{
                            \label{subfig::lod1_errors_fused}
                            Occurence statistics for \texttt{Building errors}.
                        }
                    }
                \end{subfloatrow}
                \vskip2em
                \begin{subfloatrow}
                    \ffigbox[\textwidth]{
                        \includestandalone[mode=buildnew, width=\textwidth]{figures/datasets/fused/lod2_stats}
                    }{
                        \caption{
                            \label{subfig::lod2_errors_fused}
                            Occurence statistics for \texttt{Facet errors}.
                        }
                    }
                \end{subfloatrow}
            }{
                \caption[
                    Detailed error statistics depending on the urban scenes.
                ]{
                    \label{fig::error_fused_statistics}
                    Detailed error statistics depending on the new experimental sets.
                    The height of bars indicates the frequency of each errors while the number of occurences is displayed over.
                }
            }
        \end{figure}

        Naturally, as \textbf{Nantes} contains more (around 1.56 times more) instances than \textbf{Paris-13}, \textbf{Na-P13} error statistics profile looks a bit more like the one of \textbf{Nantes} than the other area as shown in Figure~\ref{fig::error_statistics}.
        According to Sections~\ref{subsec::more_experiments::scalability::transferability} and~\ref{subsec::more_experiments::finesse::2}, we can expect that \textbf{Na-P13} would be better suited to learn \texttt{Facet errors}, with the exception of \texttt{FIT}, compared to \textbf{Elancourt}, while the latter is also the best alternative for \texttt{Building errors}.
        Next, the two classifiers, \gls{acr::svm} and \gls{acr::rf}, are trained on both sets \textbf{Elancourt} and \textbf{Na-P13} using always the baseline features (\textit{cf.} Section~\ref{sec::learned_evaluation::baseline}).

    \subsubsection{\texorpdfstring{\acrshort*{acr::rf}}{RF} results}
        \label{subsec::more_experiments::classifier::rf}
        For \textbf{Elancourt} results remain unchanged using the \gls{acr::rf} classifier and are reported along the newer results on \textbf{Na-P13} in Table~\ref{tab::rf_f3}.\\

        \begin{table}[htpb]
            \footnotesize
            \centering
            \begin{tabular}{| c | c c | c c | c c | c c |}
                \hline
                & \multicolumn{8}{c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{93.96} & \textbf{76.15} & 91.43 & 77.76 & 91.51 & 76.08 & 90.83 & 76.14 \\
                \hline
                \texttt{BUS} & 32.98 & 76.47 & \textbf{41.86} & \textbf{75.57} & 40.38 & 71.00 & 39.32 & 71.81 \\
                \hline
                \texttt{BIB} & 12.32 & 67.57 & 12.81 & 68.42 & 16.26 & 67.35 & \textbf{16.75} & \textbf{68.0} \\
                \hline
                \texttt{BIT} & \textbf{25.25} & \textbf{92.59} & 20.20 & 90.91 & 20.20 & 95.24 & 11.11 & 91.67 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.91 & 99.07 & \textbf{98.91} & \textbf{99.30} & 98.99 & 98.84 & 98.91 & 98.84 \\
                \hline
                \texttt{FUS} & \textbf{1.90} & \textbf{54.55} & 0.63 & 66.67 & 1.61 & 50 & 1.27 & 66.67 \\
                \hline
                \texttt{FIB} & \textbf{9.17} & \textbf{87.5} & 0 & --- & 8.30 & 82.61 & 7.42 & 100 \\
                \hline
                \texttt{FIT} & 6.67 & 100 & \textbf{8.73} & \textbf{95.24} & 3.33 & 100 & 3.33 & 100 \\
                \hline
                \texttt{FIG} & \textbf{80.54} & \textbf{73.14} & 80.45 & 72.62 & 78.69 & 72.12 & 79.02 & 71.82 \\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Na-P13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{51.65} & \textbf{78.93} & 47.84 & 81.75 & 48.15 & 77.74 & 47.43 & 78.57 \\
                \hline
                \texttt{BUS} & 19.85 & 100 & 22.90 & 100 & \textbf{36.64} & \textbf{92.31} & 34.61 & 93.75 \\
                \hline
                \texttt{BIB} & \textbf{1.96} & \textbf{100} & 0.65 & 100 & 0.65 & 100 &  1.31 & 100 \\
                \hline
                \texttt{BIT} & \textbf{5.32} & \textbf{100} & 3.19 & 100 & 2.13 & 100 & 1.06 & 100 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.62 & 98.22 & 98.62 & 98.21 & 98.48 & 98.76 & \textbf{98.62} & \textbf{98.76} \\
                \hline
                \texttt{FUS} & 68.80 & 77.44 & 68.18 & 77.10 & \textbf{68.80} & \textbf{78.54} & 67.83 & 78.15 \\
                \hline
                \texttt{FIB} & 55.23 & 78.60 & 53.59 & 78.47 & 65.47 & 74.44 & \textbf{65.35} & \textbf{74.63} \\
                \hline
                \texttt{FIT} & 6.25 & 100 & 6.25 & 100 & 6.25 & 100 & \textbf{11.76} & \textbf{100} \\
                \hline
                \texttt{FIG} & 94.55 & 82.54 & 95.15 & 82.72 & 94.55 & 83.43 & \textbf{95.15} & \textbf{83.60} \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::rf_f3}
                \gls{acr::rf} results on the two datasets of interest at \textbf{\gls{acr::efin}} level 3.
                Test results are expressed in percentage.
                All possible modality configurations are tested using baseline features.
            }
        \end{table}

        Regarding \textbf{Na-P13}, one natural prediction is that scores would average around the same ratios as seen with the error statistics (\textit{cf.} Figure~\ref{fig::error_fused_statistics}).
        Accounting for the random nature of the choice in training instances during the cross validation, this could be argued to be true for \texttt{FOS} or \texttt{FIG} and, in a lesser extent, for \texttt{BUS} too.
        However, it is far from being true for the rest of errors as shown in Table~\ref{tab::all_f-scores_rf_f3}.
        In fact, for the other siw error labels, F-scores, on the fused set, are better than those on both \textbf{Nantes} and \textbf{Paris-13}.
        Notably, some instances of \texttt{BIT} and \texttt{FIT} are now detected, in best cases, at arounf \SI{11}{\percent} and \SI{21}{\percent} respectively.
        On the contrary, they were not detected at all on the separate areas (\textit{cf.} Table~\ref{tab::ablation_f3}).
        This can be explained by the fact that, although having around the same statistical distribution of errors as \textbf{Nantes} and \textbf{Paris-13} in a lesser extent, the fused set contains enough instances to better learn than before.
        It can also be the result of the fact that the two areas complement each other, as better shown in Figure~\ref{tab::transferability_comparison}, with \texttt{BIt}, where training on \textbf{Nantes} and testing on the other and vice versa proved to be better than training and testing on the same zone.\\
        
        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/rf/building}
                    }{
                        \caption{
                            \label{subfig::f_score_rf_bl_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/rf/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_rf_bl_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_rf_bl}
                    Mean F-score and standard deviation obtained with an \gls{acr::rf} using baseline features.
                }
            }
        \end{figure}
    
        Mean and standard deviation F-scores are vizualized in Figure~\ref{fig::f_score_rf_bl}.
        Obviously, everything remains unchanged for \textbf{Elancourt}.
        However, we see how the standard deviations on the fused set seem to be greater than what observed previosuly in Figure~\ref{fig::f_score_ablation_f3}.
        Moreover, added to the same labels \texttt{BUS} and \texttt{FIB} that were already improved by the use of image based features as shown in Table~\ref{tab::all_f-scores_ablation_f3}, \texttt{BIT} and \texttt{FIT} F-scores are also greatly impacted on the new fused set.
        In fact, the first better performs when only geometric features are used as previously explained before in Section~\ref{subsec::experiments::baseline_feature_analysis::ablation}.
        For the second, it was image based features that proved to be better suited.
        This agrees with the fact that for the \textbf{Elancourt} \(\rightarrow\) \textbf{Nantes} experiment, image based features were instrumental in better detecting \texttt{FIT} than training on \textbf{Nantes} itself (\textit{cf.} Figure~\ref{tab::transferability_comparison}).
        
    \subsection{\texorpdfstring{\acrshort*{acr::svm}}{SVM} results}
        \label{subsec::more_experiments::classifier::svm}
        Now that we discussed the \gls{acr::rf} results on the fused set, we can move on to the \gls{acr::svm} experimental results on both identified urban sets: \textbf{Elancourt} and \textbf{Na-P13}.
        Results are reported in Table~\ref{tab::svm_f3}.\\

        \begin{table}[htpb]
            \footnotesize
            \centering
            \begin{tabular}{| c | c c | c c | c c | c c |}
                \hline
                & \multicolumn{8}{c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{97.67} & \textbf{86.44} & 91.29 & 91.57 & 91.29 & 91.56 & 41.51 & 76.70 \\
                \hline
                \texttt{BUS} & 32.27 & 86.85 & 30.15 & 90.45 & 30.14 & 90.45 & \textbf{42.89} & \textbf{92.66} \\
                \hline
                \texttt{BIB} & 97.02 & 52.27 & \textbf{91.09} & \textbf{89.75} & 91.08 & 89.75 & 67.98 & 45.10 \\
                \hline
                \texttt{BIT} & 100 & 73.88 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 53.88 & 99.71 & 51.87 & 99.70 & 51.87 & 99.70 & \textbf{63.06} & \textbf{94.08} \\
                \hline
                \texttt{FUS} & \textbf{96.49} & \textbf{52.24} & 98.73 & 21.86 & 98.73 & 21.87 & 90.79 & 17.06 \\
                \hline
                \texttt{FIB} & 33.77 & 74.03 & 17.54 & 88.89 & 17.54 & 88.89 & \textbf{71.93} & \textbf{93.71} \\
                \hline
                \texttt{FIT} & 100 & 88.24 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & \textbf{84.57} & \textbf{88.47} & 65.59 & 83.14 & 65.76 & 83.08 & 52.20 & 62.99 \\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Na-P13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{44.86} & \textbf{54.09} & 29.98 & 42.69 & 29.98 & 42.69 & 29.98 & 42.69 \\
                \hline
                \texttt{BUS} & \textbf{98.46} & \textbf{27.35} & 36.15 & 13.51 & 41.54 & 15.21 & 30.0 & 11.44 \\
                \hline
                \texttt{BIB} & \textbf{82.35} & \textbf{17.31} & 70.59 & 13.53 & 70.58 & 13.53 & 70.59 & 13.53 \\
                \hline
                \texttt{BIT} & \textbf{95.74} & \textbf{30.93} & 50.26 & 16.67 & 50.26 & 16.67 & 50.26 & 16.67 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.90 & 75.08 & 99.31 & 74.77 & 99.31 & 74.69 & \textbf{99.17} & \textbf{81.98} \\
                \hline
                \texttt{FUS} & \textbf{87.40} & \textbf{65.08} & 30.79 & 43.70 & 30.79 & 43.70 & 30.79 & 43.70 \\
                \hline
                \texttt{FIB} & \textbf{97.06} & \textbf{38.17} & 71.90 & 27.88 & 71.90 & 27.88 & 70.36 & 27.07 \\
                \hline
                \texttt{FIT} & 100 & 89.47 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & \textbf{95.64} & \textbf{77.89} & 71.39 & 77.91 & 71.27 & 77.88 & 60.36 & 72.81 \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::svm_f3}
                \gls{acr::svm} results on the two datasets at \textbf{\gls{acr::efin}} level 3.
            }
        \end{table}

        Two observations are worth noting herein:
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item First is the fact that contrarily to \gls{acr::rf} results, adding height or image based features only to the intrinsic featurtes seem to produce the same scores.
                    However, when adding both, it produces different results as can be shown in Table~\ref{tab::all_f-scores_svm_f3} for all errors but \texttt{BIT} and \texttt{FIT} for both sets as well as \texttt{FUS} for \textbf{NA-P13}.
                    This may be explained by the fact that both features have the same dynamic as they represent, in reality, histogram values as discussed in Section~\ref{sec::learned_evaluation::baseline}.
                    As a consequence, in this case, the \gls{acr::svm} considers both feature configurations to be similar unless when fed together to the \gls{acr::svm}.
                    Another explanation is that the parameterization of the classifier was not ideal as it underfits when external modalities are added.
            \item Second, and more importantly, in some cases, the \gls{acr::svm}, in particular configurations, yields results that exceed the other ones by a large margin.
                    This was the case of \texttt{FUS} on \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}) with a jump of around \SI{31}{\percent} with the \textbf{Geom.} configuration (\textit{resp.} \SI{38}{\percent}) in F-score and \SI{35}{\percent} for \texttt{FIB} on \textbf{Na-P13} with the \textbf{All} configuration.
                    This could be owed to two possible reasons.
                    Either these feature configurations are actually the best alternatives which is not conflicting with previous findings.
                    Indeed, the extrinsic features were designed in the first place to detect fidelity errors such as \texttt{FIB} while \texttt{FUS} is a topological error that can be suitably detected using intrinsic features only.
                    However, these large margins could be also explained by the fact that the \gls{acr::svm} overfitted in these special cases.
                    The last reason cannot be ruled out either as the \(\gamma\) and \(C\) hyper-parameters were not optimized for this features as seen in Section~\ref{subsec::experiments::setup::classification}, due to the high number of possible combinations.
            \end{itemize}
            
        As with the \gls{acr::rf} classifier, mean and standard deviation F-scores are computed and vizualized in Figure~\ref{fig::f_score_svm_bl}.
        Further commentary is left for the next sub-subsection as these results are compared to the \gls{acr::rf} ones.

        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/svm/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_bl_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/svm/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_bl_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_svm_bl}
                    Mean F-score and standard deviation obtained with an \gls{acr::svm} using baseline features.
                }
            }
        \end{figure}

    \subsection{\texorpdfstring{\acrshort*{acr::svm}}{SVM} compared to \texorpdfstring{\acrshort*{acr::rf}}{RF}}
        \label{subsec::more_experiments::classifier::svm_rf}
        In Table~\ref{tab::rf_vs_svm_comparison}, are compared the \gls{acr::svm} F-scores (\textit{cf.} Table~\ref{tab::all_f-scores_svm_f3}) to \gls{acr::rf} ones (\textit{cf.} Table~\ref{tab::all_f-scores_rf_f3}).
        The same color sche            the fact that , conveys how the building typology is crucial for error prediction, so much so that even with , these features are still important.
rison} and~\ref{tab::generalization_comparison}.\\

        \begin{table}[htbp]            the fact that , conveys how the building typology is crucial for error prediction, so much so that even with , these features are still important.

            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                \hline
                & \texttt{BOS} & \texttt{BUS}&\texttt{BIB}&\texttt{BIT}&\texttt{FOS} & \texttt{FUS}&\texttt{FIB}&\texttt{FIT}&\texttt{FIG}\\
                \hline
                \textbf{Elancourt} & \cellcolor{GAIN0515} & \cellcolor{STBL} \textbf{All} & \cellcolor{GAIN45} & \cellcolor{GAIN45} & \cellcolor{LOSS1525} \textbf{All} & \cellcolor{GAIN2535} & \cellcolor{GAIN2535}  \textbf{Geom.} & \cellcolor{GAIN45} & \cellcolor{GAIN0515} \textbf{Geom.} \\
                \textbf{Na-P13} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{GAIN1525} \textbf{Geom.} & \cellcolor{GAIN3545} \textbf{Geom.} & \cellcolor{LOSS0515} \textbf{All} & \cellcolor{LOSS3545} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{GAIN45} & \cellcolor{STBL} \textbf{Geom.} \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption{
                \label{tab::rf_vs_svm_comparison} Evolution of the F-score value, for each error, using \gls{acr::svm} compared to \gls{acr::rf}.
                Feature sets having a significant impact on the classification results are mentioned in the corresponding cell (\textit{cf.} Table~\ref{tab::all_f-scores_svm_f3}).
                The color indicates the magnitude:
                \textcolor{LOSS45}{\(\blacksquare\)}: (\SIrange[range-phrase={,  }]{-100}{-45}{\percent}]--
                \textcolor{LOSS3545}{\(\blacksquare\)}: [\SIrange[range-phrase={,  }]{-45}{-35}{\percent})--
                \textcolor{LOSS2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                \textcolor{LOSS1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                \textcolor{LOSS0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-15}{-5}{\percent}) --
                \textcolor{STBL}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-5}{5}{\percent}) --
                \textcolor{GAIN0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{5}{15}{\percent}) --
                \textcolor{GAIN1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{15}{25}{\percent}) --
                \textcolor{GAIN2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{25}{35}{\percent}) --
                \textcolor{GAIN3545}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{35}{45}{\percent}) --
                \textcolor{GAIN45}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{45}{100}{\percent}] --
                When two null F-scores are compared, the cell is colored in white \(\square\): neither positive nor negative.
            }
        \end{table}

        As suspected, \gls{acr::svm} yields better, or at least stable, results on highly unbalanced labels.
        In fact, \texttt{BIB} (\textit{resp.} \texttt{BIT} and \texttt{FIT}) with less than \SI{16}{\percent} (\textit{resp.} \SI{16}{\percent} and \SI{1.5}{\percent}) occurence ratio in both sets gained in terms of F-scores when training was conducted using an \gls{acr::svm}.
        The same pattern is observed for \texttt{BUS} (\textit{resp.} \texttt{FUS} and \texttt{FIB}) with less than \SI{25}{\percent} (\textit{resp.} \SI{19}{\percent} and \SI{12}{\percent}) frequency but on \textbf{Elancourt} only.
        However, the same set of labels, as well as \texttt{BOS}, performs worse when an \gls{acr::svm} is used on \textbf{Na-P13}, eventhough they are all have a presence ratio strictly under \SI{41}{\percent}.
        On the other hand, regarding the labels that are present with more than \SI{50}{\percent}\footnote{
            These are \texttt{BOS} on \textbf{Elancourt}, in addition to \texttt{FOS} and \texttt{FIG} on both sets.
        }, one could expect that they would be underperforming with the use of \glspl{acr::svm}.
        Although this is true for \texttt{FOS}, it is not the case for the other cases.\\

        \begin{table}[htbp]
            \footnotesize
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{| c | x{2cm} x{2cm} | x{2cm} x{2cm} |}
                \hline
                & \multicolumn{2}{c |}{\textbf{Elancourt}} & \multicolumn{2}{c |}{\textbf{Na-P13}}\\
                \hline
                & \gls{acr::rf} & \gls{acr::svm} & \gls{acr::rf} & \gls{acr::svm}\\
                \hline
                \texttt{BOS} & \textbf{Geom.} & \textbf{Geom.} & \textbf{Geom.} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{BUS} & \textbf{Hei.} & \underline{\textbf{All}} & \underline{\textbf{Im.}} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{BIB} & \underline{\textbf{All}} & \underline{\textbf{Hei.}}, \textbf{Im.} & \textbf{Geom.} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{BIT} & \underline{\textbf{Geom.}} & \textbf{Hei.}, \textbf{Im.} &  \underline{\textbf{Geom.}} & \underline{\textbf{Geom.}} \\
                \hline
                \hline
                \texttt{FOS} & \textbf{Hei.} & \underline{\textbf{All}} & \textbf{All} & \underline{\textbf{All}} \\
                \hline
                \texttt{FUS} & \textbf{Geom.} & \textbf{Geom.} & \textbf{Im.} & \textbf{Geom.} \\
                \hline
                \texttt{FIB} & \textbf{Geom.} & \textbf{All} & \underline{\textbf{All}} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{FIT} & \textbf{Hei.} & \textbf{Hei.}, \textbf{Im.} & \underline{\textbf{All}} &  \textbf{Hei.}, \textbf{Im.} \\
                \hline
                \texttt{FIG} & \textbf{Geom.} & \underline{\textbf{Geom.}} & \textbf{All} & \underline{\textbf{Geom.}} \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption{
                \label{tab::svm_rf_best_features_f3}
                The best performing feature configuration per zone, label and classifier.
                This summarizes all comparisons between Tables~\ref{tab::all_f-scores_rf_f3} and~\ref{tab::all_f-scores_svm_f3}.
                The features, that stand out compared to the others in these last tables, are underlined.
            }
        \end{table}

        In order to explain these observed exceptions, we tried to look at the best performing modalities per label and set comparing between the two classifiers.
        Most labels perform best with the same kind of feature configurations, as shown in Table~\ref{tab::svm_rf_best_features_f3}.
        Most notably, we have previously suspected that using only geometric features results in overfitting with an \gls{acr::svm} when training on \texttt{FUS} for both sets.
        Comparing to the \gls{acr::rf}, the same feature configuration was also the best alternative.
        This leads us to speculate that, contrarily to what was discussed in Section~\ref{subsec::more_experiments::classifier::svm}, it is not a case of overfitting for this modality but rather the trained \gls{acr::svm} underfitting on the other feature configurations.\\

        This is further confirmed by the cases where \gls{acr::rf} and \gls{acr::svm} did not share the same highest performing feature configuration.
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item On \textbf{Elancourt}, for \texttt{BUS}, \texttt{FOS} and \texttt{FIT}, the \gls{acr::rf} performed better with height based features, compared to the \gls{acr::svm} which works better with other feature configurations containing extrinsic features.
                    Moreover, \texttt{BIT} was better detected with intrinsic features only (\textit{resp.} any extrinsic feature based configuration) when trained with an \gls{acr::rf} (\textit{resp.} \gls{acr::svm}).
                    These findings do not rise any red flags, as they remain consistent with what was discussed earlier in other experiments.
            \item On \textbf{Na-P13}, the only labels where the better performing feature configuration changed were \texttt{BUS}, \texttt{FUS}, \texttt{FIB} and \texttt{FIG}.
                    In fact, only geometric features yielded better results using an \gls{acr::svm}, in contrast with the fact that \textbf{All} or \textbf{Im.} were the best alternative with an \gls{acr::rf}.
        \end{itemize}
        This reinforces the idea that the \gls{acr::svm} was actually not adequately parameterized for the extrinsic features as was announced in Section~\ref{subsec::experiments::setup::classification}.
        In fact, as discussed earlier in Section~\ref{subsubsec::state_of_the_art::mlpr::classifiers::svm}, \glspl{acr::svm} are very hard to parameterize compared to \glspl{acr::rf}.
        This seems to be a reasonable cause why the \gls{acr::svm} classifier underperforms compared to the \gls{acr::rf} one on \textbf{Na-P13}.
            
    \subsection{Modality contributions comparison}
        \label{subsec::more_experiments::classifier::feature_importance}
        In a linear \gls{acr::svm}, feature importance can be natively computed by looking at the weight vector \(\bm{w}\) as shown by~\textcite{guyon2002gene}.
        This is, unfortunatly, not our case.
        Instead we operate with Kernel \glspl{acr::svm} where features are fused using \gls{acr::mkl} with weights \(\left(\mu_i\right)_{i=1, 2, 3}\) (\textit{cf.} Equation~\ref{eq::mkl}) that are on the simplex, as discussed in Section~\ref{subsubsec::state_of_the_art::mlpr::classifiers::svm}.
        Consequently, when training with the last feature configuration, the resulting weights could be interpreted as feature importance ratios.
        These are vizualized in Figure~\ref{fig::feature_importances_svm_bl}.\\

        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/svm_rf/feature_importance/building}
                    }{
                        \caption{
                            \label{subfig::feature_importances_svm_bl_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/svm_rf/feature_importance/facet}
                    }{
                        \caption{
                            \label{subfig::feature_importances_svm_bl_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::feature_importances_svm_bl}
                    Modality contribution for the \gls{acr::svm} classifier based on the coefficients computed by EasyMKL.
                    The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                }
            }
        \end{figure}

        We can see how these ratios stay mostly around 1/3 with more leeway compared to \gls{acr::rf} (\textit{cf.} Figure~\ref{fig::feature_importances_rf_bl}).
        This actually confirms how these \gls{acr::mkl} weights could be interpreted as features importances.
        The larger margins to the 1/3 ratio could be explained by the sensibility of the weights to the noise when selecting training instances.
        There is, however, one exception that could be noted between Figures~\ref{fig::feature_importances_svm_bl} and~\ref{fig::feature_importances_rf_bl}.
        In fact, for \texttt{FOS}, geometric features have a larger importance when training with \gls{acr::svm} compared to the \gls{acr::rf} case.
        This is actually not an issue as it could be explained by the topological nature of the error label.\\

        Eventhough geometric features alone yield better results in some cases, the \gls{acr::mkl} cannot ignore the extrinsic features.
        As in the \gls{acr::rf} case, this may be prove to be helpful for the transferability of learning.
        Consequently, it would be interesting to rerun the same scalanility experiments, as in Section~\ref{sec::more_experiments::scalability}, with an \gls{acr::svm}.
        This was not the case, due to time limitations.
    
\section{Advanced features contributions}
    \label{sec::more_experiments::richer_features}
    Herein, some advanced feature extractors are tried.
    The goal is to find out if it is possible to achieve better results than the ones obtained with our handcrafted baseline features.
    As we do not have enough learning instances to leverage deep learning methods we choose instead to use graph kernels as well as \glspl{acr::scatnet} as shown in Section~\ref{sec::learned_evaluation::richer_features} and implemented in Section~\ref{subsec::experiments::setup::feature_configurations}.\\

    Three possibilities are investigated:
    \begin{enumerate}[label=\roman*)]
        \item We keep the baseline geometric features and replace the basic image and height based features by the \gls{acr::scatnet} derived extractors, as explained in Section~\ref{subsec::learned_evaluation::richer_features::image}.
        \item We compare only geometric features test results using both the baseline features and graph kernels.
        \item We combine both the graph kernels and the \gls{acr::scatnet} derived extractors and compare them to the baseline results.
    \end{enumerate}
    As with the previous section (\textit{cf.} Section~\ref{sec::more_experiments::classifier}), the experiments are conducted on two sets: \textbf{Elancourt} and \textbf{Na-P13}.

    \subsection{\texorpdfstring{\acrshort*{acr::scatnet}}{ScatNet} to baseline comparison}
        \label{subsec::more_experiments::richer_features::scatnet_baseline}
        We start by the \gls{acr::scatnet} comparisons.
        We run the same experiments as in Section~\ref{sec::more_experiments::classifier}, where this time height and image based features are replaced by derived ones.
        There are two options when employing \gls{acr::scatnet} with image based features: \texttt{channel} and \texttt{deletion} (\textit{cf.} Section~\ref{subsec::experiments::setup::feature_configurations}).
        This makes the number of possible feature configurations equal to six.\\

        Both the \gls{acr::rf} and \gls{acr::svm} classifiers are used.
        Results from both are first compared to the baseline results reported in Section~\ref{sec::more_experiments::classifier}.
        Afterwhat, we examine the differences between results from both classifiers.

        \subsubsection{\texorpdfstring{\acrshort*{acr::rf}}{RF} results}
            \label{subsubsec::more_experiments::richer_features::scatnet_baseline::rf}
            Results, using the \gls{acr::rf} classifier, on both sets, are reported in Table~\ref{tab::stats_scat_rf_f3}.
            In general, we can observe how external modalities seem to play a more important role in detecting errors.
            This can be confirmed with the larger standard deviations depicted in Figure~\ref{fig::f_score_rf_scat_bl} compared to the baseline features (\textit{cf.} Figure~\ref{fig::f_score_rf_bl}).\\

            \begin{sidewaystable}[htpb]
                \footnotesize
                \centering
                \begin{tabular}{| c | c c | c c | c c | c c | c c | c c |}
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Elancourt}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{S(d)-All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & 93.96 & 76.15 & 91.97 & 79.13 & 97.75 & 74.49 & 94.52 & 78.49 & 96.55 & 77.24 & \textbf{94.89} & \textbf{78.80} \\
                    \hline
                    \texttt{BUS} & 32.98 & 76.47 & \textbf{52.44} & \textbf{84.88} & 29.51 & 92.67 & 50.32 & 87.13 & 36.09 & 91.89 & 49.79 & 90.38 \\
                    \hline
                    \texttt{BIB} & 12.32 & 67.57 & 11.38 & 100 & 5.45 & 100 & 5.45 & 100 & 13.37 & 100 & \textbf{14.36} & \textbf{100} \\
                    \hline
                    \texttt{BIT} & 25.25 & 92.59 & \textbf{42.86} & \textbf{100} & 20.41 & 100 & 39.80 & 100 & 34.69 & 100 & 36.73 & 100 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.91 & 99.07 & 99.14 & 99.14 & 99.30 & 97.25 & 99.46 & 96.82 & 99.61 & 99.23 & \textbf{99.69} & \textbf{99.23} \\
                    \hline
                    \texttt{FUS} & 1.90 & 54.55 & 3.18 & 100 & 4.78 & 100 & 5.73 & 100 & 5.41 & 100 & \textbf{12.42} & \textbf{100} \\
                    \hline
                    \texttt{FIB} & 9.17 & 87.5 & 0.87 & 100 & 0.44 & 100 & 0 & --- & \textbf{12.28} & \textbf{100} & 11.84 & 100 \\
                    \hline
                    \texttt{FIT} & 6.67 & 100 & 20.69 & 100 & \textbf{27.59} & \textbf{100} & 20.69 & 100 & 10.34 & 100 & 6.90 & 100 \\
                    \hline
                    \texttt{FIG} & 80.54 & 73.14 & 94.83 & 74.85 & 96.27 & 73.57 & \textbf{97.12} & \textbf{74.13} & 93.98 & 74.23 & 95.17 & 74.87 \\
                    \hline
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Na-P13}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & \textbf{51.65} & \textbf{78.93} & 39.92 & 85.84 & 39.71 & 96.5 & 40.95 & 90.87 & 44.65 & 95.59 & 43.21 & 93.75 \\
                    \hline
                    \texttt{BUS} & 19.85 & 100 & 0.76 & 100 & \textbf{27.69} & \textbf{100} & 25.95 & 100 & \textbf{27.69} & \textbf{100} & 26.15 & 100 \\
                    \hline
                    \texttt{BIB} & 1.96 & 100 & \textbf{2.61} & \textbf{100} & 0 & --- & 0 & --- & 2.60 & 100 & 1.96 & 100 \\
                    \hline
                    \texttt{BIT} & 5.32 & 100 & 0 & --- & \textbf{8.51} & \textbf{100} & 6.91 & 100 & 7.41 & 100 & 5.82 & 100 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.62 & 98.22 & 99.31 & 95.87 & 99.31 & 94.99 & 99.17 & 92.65 & 98.62 & 98.62 & \textbf{98.62} & \textbf{98.75} \\
                    \hline
                    \texttt{FUS} & 68.80 & 77.44 & 70.10 & 76.75 & 61.57 & 84.18 & 61.57 & 82.55 & \textbf{65.08} & \textbf{84.0} & 65.57 & 82.81 \\
                    \hline
                    \texttt{FIB} & 55.23 & 78.60 & 26.80 & 95.35 & 46.41 & 94.04 & 43.46 & 94.33 & 68.62 & 85.71 & \textbf{68.95} & \textbf{86.12} \\
                    \hline
                    \texttt{FIT} & 6.25 & 100 & 6.25 & 100 & 12.5 & 100 & 12.5 & 100 & \textbf{25.0} & \textbf{100} & \textbf{25.0} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & 94.55 & 82.54 & 98.42 & 83.45 & 97.70 & 83.71 & 98.18 & 83.85 & 97.58 & 85.10 & \textbf{97.94} & \textbf{84.96} \\
                    \hline
                \end{tabular}
                \caption{
                    \label{tab::stats_scat_rf_f3}
                    \gls{acr::rf} applied to \gls{acr::scatnet} based features.
                    Results are expressed in percentage on the two datasets at \textbf{\gls{acr::efin}} level 3.
                }
            \end{sidewaystable}
            
            In addition, based on the same Figure~\ref{fig::f_score_rf_scat_bl}, we can deduce the best option to be used for \gls{acr::scatnet} image based features.
            In most cases, the best options seems to be almost always \texttt{channel}.
            This is understandable as this option do not modify the signal, from the get go, and preserves both the image and model informations until the last possible opportunity letting the classifier handle the fusion.
            The early fusion conducted when choosing the \texttt{deletion} option deforms the input signal.
            As the \gls{acr::scatnet} convolves filters and applies non linear functions to the input, the classifier cannot separate, in this case, between information that is derived from the image or from the evaluated model.\\

            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/deletion/building}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_del_bl_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/deletion/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_del_bl_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip1em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/channel/building}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_chan_bl_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/channel/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_chan_bl_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption{
                        \label{fig::f_score_rf_scat_bl}
                        Mean F-score and standard deviation obtained with an \gls{acr::rf} based on \gls{acr::scatnet} features.
                        This is a vizualization of scores recorder in Table~\ref{tab::f_score_scat_bl}.
                    }
                }
            \end{figure}

            There are however exceptions that we distinguish into two kinds.
            The first is where \texttt{deletion} was better with a small margin that can be explained by the noise of training data selection.
            This was the case of \texttt{BIT} on \textbf{Elancourt} and \texttt{FIG} on \textbf{Na-P13}.
            On the contrary, the second, and more important, case is where the margin is large which was the case of \texttt{FIT} on \textbf{Elancourt} where \textbf{Geom. \(\otimes\) S(d)-Im.} had at least \SI{9}{\percent} more in F-score than the other configurations.
            At this point we do not have any explication why this was the case.
            This requires scalability experimentations as with Section~\ref{sec::more_experiments::scalability} to see if this feature configuration is robust enough.\\

            \begin{table}[htbp]
                \footnotesize 
                \centering
                \renewcommand{\arraystretch}{2}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS}&\texttt{BIB}&\texttt{BIT}&\texttt{FOS} & \texttt{FUS}&\texttt{FIB}&\texttt{FIT}&\texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S-Hei.} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{GAIN1525} \textbf{S-Hei.} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S(d)-Im.} & \cellcolor{LOSS1525} \textbf{Geom.} & \cellcolor{GAIN2535} \textbf{S(d)-Im.} & \cellcolor{GAIN0515} \\
                        \textbf{Na-P13} & \cellcolor{STBL} \textbf{Geom.} & \cellcolor{LOSS0515} \textbf{S(d)-Im.} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S(d)-Im.} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{LOSS0515} & \cellcolor{STBL} \textbf{S(d)-Im.} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::rf_scat_bl_comparison_del}
                        Comparison with \texttt{deletion} option.
                    }
                \end{subtable}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS}&\texttt{BIB}&\texttt{BIT}&\texttt{FOS} & \texttt{FUS}&\texttt{FIB}&\texttt{FIT}&\texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S-Hei.} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{S-Hei.} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{S(c)-All} & \cellcolor{GAIN0515} \textbf{S(c)-Im.} & \cellcolor{GAIN1525} \textbf{S-Hei.} & \cellcolor{GAIN0515} \\
                        \textbf{Na-P13} & \cellcolor{STBL} & \cellcolor{LOSS0515} \textbf{S(c)-Im.} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S(c)-Im.} & \cellcolor{GAIN1525} \textbf{S(c)-Im.} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::rf_scat_bl_comparison_chan}
                        Comparison with \texttt{channel} option.
                    }
                \end{subtable}
                \renewcommand{\arraystretch}{1}
                \caption[
                    Evolution of the F-score value, for each error, with the \gls{acr::rf} classifier, using \gls{acr::scatnet} compared to baseline features.
                ]{
                    \label{tab::rf_scat_bl_comparison}
                    Evolution of the F-score value, for each error, with the \gls{acr::rf} classifier, using \gls{acr::scatnet} compared to baseline features.
                    Feature sets having a significant impact on the classification results are mentioned in the corresponding cell (\textit{cf.} Table~\ref{tab::all_f-scores_scat_rf_f3}).
                    The color indicates the magnitude:
                    \textcolor{LOSS45}{\(\blacksquare\)}: (\SIrange[range-phrase={,  }]{-100}{-45}{\percent}]--
                    \textcolor{LOSS3545}{\(\blacksquare\)}: [\SIrange[range-phrase={,  }]{-45}{-35}{\percent})--
                    \textcolor{LOSS2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-15}{-5}{\percent}) --
                    \textcolor{STBL}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-5}{5}{\percent}) --
                    \textcolor{GAIN0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{5}{15}{\percent}) --
                    \textcolor{GAIN1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{15}{25}{\percent}) --
                    \textcolor{GAIN2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{25}{35}{\percent}) --
                    \textcolor{GAIN3545}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{35}{45}{\percent}) --
                    \textcolor{GAIN45}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{45}{100}{\percent}] --
                    When two null F-scores are compared, the cell is colored in white \(\square\): neither positive nor negative.
                }
            \end{table}

            The new results are compared, for each \gls{acr::scatnet} option, error label and set.
            These are shown in Table~\ref{tab::rf_scat_bl_comparison} with the same color scheme as in previous comparisons.
            On \texttt{Building errors}, \gls{acr::scatnet} with \texttt{deletion} yields better (\textit{resp.} worse and stable) results 3 (\textit{resp.} 3 and 2) times.
            On \texttt{Facet errors}, the same option yields better (\textit{resp.} worse and stable) results 3 (\textit{resp.} 2 and 5) times.
            In constrast, the \texttt{channel} option helps \gls{acr::scatnet} achieve, better (\textit{resp.} worse and stable) results 2 (\textit{resp.} 1 and 5) times.
            On \texttt{Facet errors}, the same option yields better (\textit{resp.} worse and stable) results 6 (\textit{resp.} 0 and 3) times.
            This confirms again the fact that the last option was better than \texttt{deletion}.\\

            We can also observe how height based features with \gls{acr::scatnet} are more instrumental than the baseline features, espacially for \texttt{BIT} and \texttt{FIT} on \textbf{Elancourt}.
            For \texttt{BUS} on \textbf{Elancourt}, baseline height based features were also instrumental but in the same capacity as image based ones.
            It is not the case anymore when adding \gls{acr::scatnet} based ones: it is better than both baseline and advanced image based features.
            On \textbf{Na-P13}, this modality does not play an important role even with the more advanced feature extractor.
            This may be attributed to the fact that on both zones, \textbf{Nantes} and \textbf{Paris-13}, building height profiles are mostly the same, especially since the types of these buildings are less heterogeneous than on \textbf{Elancourt}.\\

            Regarding image based features using \gls{acr::scatnet}, in both cases, they are more decisive in error prediction.
            Although it fails to give better results for \texttt{BIB} for both sets and \texttt{BUS} on \textbf{Na-P13}, it is more helpful than baseline features, especially for topological error labels.
            In fact, contrarily to baseline image features, it is crucial in better detecting \texttt{BIT} on \textbf{Na-P13} (with \texttt{deletion}), \texttt{FUS} and \texttt{FIG} on \textbf{Elancourt}, as well as, \texttt{FIB} (with \texttt{channel}) and \texttt{FIT} on both sets.\\

            As with previous experiments, we also computed feature importances using the new configurations.
            These are normalized and shown for both sets and options in Figure~\ref{fig::feature_importances_scat_rf}.
            The normalization consists in weighting the importance ratios by the inverse of the corresponding feature vector length.
            This is conducted in order to put all modalities at an equal footing eventhough they \gls{acr::scatnet} produces a lot of features.\\

            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/deletion/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_rf_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/deletion/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_rf_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip2em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/channel/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_rf_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/channel/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_rf_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption[
                        Normalized modality importance using the \gls{acr::rf} classifier and \gls{acr::scatnet} features.
                    ]{
                        \label{fig::feature_importances_scat_rf}
                        Normalized modality importance using the \gls{acr::rf} classifier and \gls{acr::scatnet} features.
                        Height and Image based features use the \gls{acr::scatnet} with both \texttt{deletion} and \texttt{channel} options.
                        The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                    }
                }
            \end{figure}

            First of all, we see how the depth based modality plays a more important role on \textbf{Elancourt} compared to the other set as seen in Table~\ref{tab::rf_scat_bl_comparison}.
            In general, it has an importance ratio less than \SI{20}{\percent} except for \texttt{BOS} and \texttt{FIB} on both sets, as well as \texttt{BUS} on \textbf{Elancourt}.\\

            Regarding image based features, we can notice that the \texttt{channel} option results in less importance than the \texttt{deletion} one.
            This can be explained by the fact that the latter contains 3/4 times less coefficients than the other case.
            However, this reason fails to describe the case where the discrepancy between the two cases is too important.
            In fact, for \texttt{FIB} on \textbf{Na-P13}, as well as \texttt{FIT} and \texttt{FIG} on both sets, the image based features are too important with \texttt{deletion} than with \texttt{channel}.
            This is actually not beneficial for learning as height based features were essential to achieve better results.
            This means that the fact that the \texttt{channel} option was better not only because it is on its own better than the alternative, but also because it works better with height based features.\\

            When examnining the \texttt{channel} option (\textit{cf.} Figures~\ref{subfig::feature_importances_scat_chan_rf_building} and~\ref{subfig::feature_importances_scat_chan_rf_facet}) more closely, we see how for both sets the image based modality importance falls in the range \SIrange{5}{10}{\percent} in most cases.
            There are two cases where this is not the case:
            \begin{itemize}[label=\(\blacktriangleright\)]
                \item \texttt{FOS}, on both sets, with an importance below \SI{1}{\percent}.
                        This is understandable as this error is of topological nature and is better detected with intrinsic features.
                        Indeed, for this error even height based features score less than \SI{1}{\percent} in importance ratio.
                \item \texttt{FIB}, on \textbf{Na-P13} with a ration around 1/3.
                        This can be explained by the fact that this modality was decisive in getting the best F-score possible (\textit{cf.} Table~\ref{tab::rf_scat_bl_comparison}).
            \end{itemize}
            On another note, building typology is so important that, even with advanced extrinsic features, baseline geometric features have a very large importance ratio.

        \subsubsection{\texorpdfstring{\acrshort*{acr::svm}}{SVM} results}
            \label{subsubsec::more_experiments::richer_features::scatnet_baseline::svm}
            \begin{sidewaystable}[htpb]
                \footnotesize
                \centering
                \begin{tabular}{| c | c c | c c | c c | c c | c c | c c |}
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Elancourt}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{S(d)-All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & \textbf{97.67} & \textbf{86.44} & 97.07 & 81.99 & 95.12 & 86.43 & 97.15 & 83.16 & 91.67 & 89.71 & 93.32 & 86.56 \\
                    \hline
                    \texttt{BUS} & 32.27 & 86.85 & 89.81 & 46.48 & \textbf{60.08} & \textbf{92.18} & 89.60 & 47.42 & 30.79 & 90.63 & 89.41 & 44.42 \\
                    \hline
                    \texttt{BIB} & 97.02 & 52.27 & 89.60 & 44.80 & 98.51 & 25.68 & 97.03 & 30.53 & \textbf{91.13} & \textbf{90.69} & 94.06 & 71.43 \\
                    \hline
                    \texttt{BIT} & 100 & 73.88 & 91.84 & 38.96 & 98.98 & 82.20 & 94.90 & 46.5 & \textbf{100} & \textbf{100} & 93.88 & 49.46 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 53.88 & 99.71 & 70.22 & 87.42 & \textbf{94.17} & \textbf{97.04} & 78.46 & 88.05 & 54.59 & 99.72 & 69.98 & 83.96 \\
                    \hline
                    \texttt{FUS} & 96.49 & 52.24 & 96.82 & 40.92 & \textbf{95.59} & \textbf{62.26} & 94.27 & 58.85 & 98.09 & 51.85 & 97.13 & 60.52 \\
                    \hline
                    \texttt{FIB} & 33.77 & 74.03 & 31.58 & 74.23 & \textbf{89.04} & \textbf{81.53} & 87.72 & 81.97 & 17.98 & 89.13 & 18.42 & 89.36 \\
                    \hline
                    \texttt{FIT} & 100 & 88.24 & 100 & 88.24 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & \textbf{84.57} & \textbf{88.47} & 86.28 & 85.56 & 64.92 & 91.30 & 68.31 & 91.49 & 68.56 & 83.75 & 63.05 & 76.00 \\
                    \hline
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Na-P13}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{S(d)-All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & \textbf{44.86} & \textbf{54.09} & 44.65 & 53.71 & 29.98 & 42.69 & 29.98 & 42.69 & 29.98 & 42.69 & 29.98 & 42.69 \\
                    \hline
                    \texttt{BUS} & 98.46 & 27.35 & 97.69 & 25.76 & 93.08 & 29.30 & \textbf{93.07} & \textbf{34.28} & 37.40 & 14.04 & 71.54 & 27.84 \\
                    \hline
                    \texttt{BIB} & \textbf{82.35} & \textbf{17.31} & 79.74 & 16.55 & 70.59 & 13.55 & 70.59 & 13.55 & 70.59 & 13.53 & 70.59 & 13.53 \\
                    \hline
                    \texttt{BIT} & \textbf{95.74} & \textbf{30.93} & 95.21 & 30.76 & 53.49 & 17.63 & 52.66 & 17.71 & 50.26 & 16.67 & 50.26 & 16.67 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.90 & 75.08 & 95.45 & 82.78 & \textbf{99.72} & \textbf{74.54} & 99.17 & 81.89 & 99.45 & 74.64 & 98.34 & 81.67 \\
                    \hline
                    \texttt{FUS} & \textbf{87.40} & \textbf{65.08} & 87.60 & 60.31 & 30.79 & 43.70 & 59.50 & 61.02 & 30.79 & 43.70 & 45.45 & 54.73 \\
                    \hline
                    \texttt{FIB} & \textbf{97.06} & \textbf{38.17} & 97.39 & 36.79 & 79.08 & 30.75 & 81.37 & 31.60 & 71.90 & 27.88 & 71.90 & 27.78 \\
                    \hline
                    \texttt{FIT} & 100 & 89.47 & 100 & 89.47 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & 95.64 & 77.89 & 95.39 & 78.62 & 97.70 & 78.63 & \textbf{97.82} & \textbf{78.65} & 72.12 & 78.08 & 83.76 & 81.68 \\
                    \hline
                \end{tabular}
                \caption{
                    \label{tab::stats_scat_svm_f3}
                    \gls{acr::svm} applied to \gls{acr::scatnet} based features.
                    Results are expressed in percentage on the two datasets at \textbf{\gls{acr::efin}} level 3.
                }
            \end{sidewaystable}
    
            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/deletion/building}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_del_bl_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/deletion/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_del_bl_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip1em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/channel/building}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_chan_bl_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/channel/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_chan_bl_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption{
                        \label{fig::f_score_svm_scat}
                        Mean F-score and standard deviation obtained with an \gls{acr::svm} based on \gls{acr::scatnet} features.
                    }
                }
            \end{figure}

            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/deletion/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_svm_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/deletion/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_svm_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip2em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/channel/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_svm_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/channel/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_svm_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption{
                        \label{fig::feature_importances_scat_svm} Modality contribution for the \gls{acr::svm} classifier based on the coefficients computed by EasyMKL.
                        Height and Image based features use the \gls{acr::scatnet} with both \texttt{deletion} and \texttt{channel} options.
                        The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                    }
                }
            \end{figure}

    \subsection{Graph kernels to baseline comparison}
        \begin{table}[htpb]
            \footnotesize
            \centering
            \begin{tabular}{| c | c c c | c c c |}
                \hline
                \multicolumn{7}{|c|}{\textbf{Elancourt}}\\
                \hline
                & \multicolumn{3}{c|}{\textbf{Geom.}} & \multicolumn{3}{c|}{\textbf{K-Geom.}} \\
                \cline{2-7}
                & \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) \\
                \hline
                \texttt{BOS} & 97.67 & 86.44 & \textbf{91.71} & 87.99 & 86.11 & 87.04 \\
                \hline
                \texttt{BUS} & 32.27 & 86.85 & 47.06 & 92.99 & 51.29 & \textbf{66.11} \\
                \hline
                \texttt{BIB} & 97.02 & 52.27 & \textbf{67.94} & 72.28 & 60.08 & 65.62 \\
                \hline
                \texttt{BIT} & 100 & 73.88 & \textbf{84.98} & 96.94 & 41.67 & 58.29 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 53.88 & 99.71 & 69.96 & 97.12 & 99.60 & \textbf{98.34} \\
                \hline
                \texttt{FUS} & 96.49 & 52.24 & \textbf{67.78} & 84.39 & 30.18 & 44.46 \\
                \hline
                \texttt{FIB} & 33.77 & 74.03 & 46.38 & 94.74 & 32.34 & \textbf{48.22} \\
                \hline
                \texttt{FIT} & 100 & 88.24 & 93.75 & 100 & 100 & \textbf{100} \\
                \hline
                \texttt{FIG} & 84.57 & 88.47 & \textbf{86.48} & 69.66 & 80.91 & 74.86 \\
                \hline
                \hline
                \multicolumn{7}{|c|}{\textbf{Na-P13}}\\
                \hline
                & \multicolumn{3}{c|}{\textbf{Geom.}} & \multicolumn{3}{c|}{\textbf{K-Geom.}} \\
                \cline{2-7}
                & \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) \\
                \hline
                \texttt{BOS} & 44.86 & 54.09 & 49.04 & 43.42 & 61.34 & \textbf{50.85} \\
                \hline
                \texttt{BUS} & 98.46 & 27.35 & 42.81 & 86.15 & 31.73 & \textbf{46.38} \\
                \hline
                \texttt{BIB} & 82.35 & 17.31 & 28.61 & 75.16 & 31.94 & \textbf{44.83} \\
                \hline
                \texttt{BIT} & 95.74 & 30.93 & 46.76 & 87.23 & 32.67 & \textbf{47.54} \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.90 & 75.08 & 85.36 & 95.45 & 98.86 & \textbf{97.13} \\
                \hline
                \texttt{FUS} & 87.40 & 65.08 & \textbf{74.61} & 79.34 & 67.13 & 72.73 \\
                \hline
                \texttt{FIB} & 97.06 & 38.17 & 54.79 & 92.16 & 53.71 & \textbf{67.87} \\
                \hline
                \texttt{FIT} & 100 & 89.47 & 94.44 & 100 & 100 & \textbf{100} \\
                \hline
                \texttt{FIG} & 95.64 & 77.89 & \textbf{85.86} & 80.73 & 89.40 & 84.84 \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::stats_gk_svm_f3}
                Comparison between the baseline geometric features and graph kernels using \gls{acr::svm}.
                Results, expressed in percentage, on the two datasets at \textbf{\gls{acr::efin}} level 3.
            }
        \end{table}
        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk_vs_bl/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk_vs_bl/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_svm_gk}
                    F-score obtained with an \gls{acr::svm} using graph kernels features.
                }
            }
        \end{figure}

    \subsection{Graph kernels and \texorpdfstring{\acrshort*{acr::scatnet}}{ScatNet} to baseline comparison}
        \begin{sidewaystable}[htpb]
            \footnotesize
            \begin{tabular}{| c | c c | c c | c c | c c | c c | c c |}
                \hline
                \multicolumn{13}{|c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{K-Geom.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(d)-All}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(c)-All}}\\
                \cline{2-13}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & 87.99 & 86.11 & 87.16 & 86.19 & 88.29 & 86.85 & 87.84 & 86.80 & \textbf{88.66} & \textbf{86.58} & 88.30 & 86.54 \\
                \hline
                \texttt{BUS} & 92.99 & 51.29 & \textbf{93.42} & \textbf{53.27} & 93.84 & 49.50 & 93.86 & 50.92 & 93.63 & 49.94 & 93.63 & 51.04 \\
                \hline
                \texttt{BIB} & 72.28 & 60.08 & 73.40 & 59.36 & 70.94 & 61.80 & 72.77 & 61.51 & 72.27 & 61.86 & \textbf{73.76} & \textbf{61.83} \\
                \hline
                \texttt{BIT} & 96.94 & 41.67 & 96.94 & 41.67 & \textbf{97.96} & \textbf{42.86} & 96.94 & 42.60 & 96.94 & 42.22 & 95.96 & 42.41 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & \textbf{97.12} & \textbf{99.60} & \textbf{97.12} & \textbf{99.60} & \textbf{97.12} & \textbf{99.60} & \textbf{97.12} & \textbf{99.60} & 97.05 & 99.60 & 97.05 & 99.60 \\
                \hline
                \texttt{FUS} & 84.39 & 30.18 & 84.39 & 30.08 & \textbf{85.03} & \textbf{30.83} & 84.71 & 30.75 & 84.71 & 30.61 & 84.71 & 30.61 \\
                \hline
                \texttt{FIB} & 94.74 & 32.34 & 94.32 & 32.24 & \textbf{96.49} & \textbf{32.31} & 96.05 & 32.06 & 96.49 & 32.26 & 96.51 & 32.12 \\
                \hline
                \texttt{FIT} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & 69.66 & 80.91 & 69.92 & 81.28 & 70.93 & 80.64 & 71.10 & 80.91 & 71.19 & 80.92 & \textbf{71.44} & \textbf{81.06} \\
                \hline
                \hline
                \multicolumn{13}{|c|}{\textbf{Na-P13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{K-Geom.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(d)-All}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(c)-All}}\\
                \cline{2-13}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{43.42} & \textbf{61.34} & 43.42 & 60.81 & 38.89 & 56.59 & 38.89 & 56.59 & 38.89 & 56.93 & 38.89 & 56.59 \\
                \hline
                \texttt{BUS} & 86.15 & 31.73 & 85.38 & 31.36 & 87.69 & 32.11 & 87.69 & 32.11 & \textbf{87.02} & \textbf{32.39} & 87.69 & 32.11 \\
                \hline
                \texttt{BIB} & 75.16 & 31.94 & 75.16 & 31.86 & 79.74 & 33.80 & 79.74 & 33.80 & \textbf{79.74} & \textbf{33.89} & 79.74 & 33.80 \\
                \hline
                \texttt{BIT} & 87.23 & 32.67 & 87.23 & 32.48 & 89.89 & 33.40 & 89.89 & 33.40 & \textbf{89.89} & \textbf{33.53} & 89.89 & 33.40 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 95.45 & 98.86 & \textbf{96.59} & \textbf{98.86} & 95.86 & 98.86 & 95.86 & 98.86 & 95.59 & 98.86 & 95.86 & 98.86 \\
                \hline
                \texttt{FUS} & 79.34 & 67.13 & 79.55 & 66.61 & 80.79 & 66.61 & 80.79 & 66.61 & \textbf{80.79} & \textbf{66.95} & 80.79 & 66.61 \\
                \hline
                \texttt{FIB} & \textbf{92.16} & \textbf{53.71} & 92.16 & 53.51 & 92.81 & 53.18 & 92.81 & 53.18 & 92.51 & 53.48 & 92.81 & 53.18 \\
                \hline
                \texttt{FIT} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & 80.73 & 89.40 & 81.33 & 89.35 & \textbf{83.03} & \textbf{89.19} & \textbf{83.03} & \textbf{89.19} & 81.94 & 89.18 & \textbf{83.03} & \textbf{89.19} \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::stats_gk_scat_svm_f3}
                \gls{acr::svm} results using graph kernels and \glspl{acr::scatnet}, expressed in percentage, on the two datasets at \textbf{\gls{acr::efin}} level 3.
            }
        \end{sidewaystable}

        \begin{figure}[htpb]
            \centering
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/deletion/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_del_bl_building}
                            \texttt{Building errors} (with \texttt{deletion}).
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/deletion/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_del_bl_facet}
                            \texttt{Facet errors (with \texttt{deletion}).}
                        }
                    }
                \end{subfloatrow}
                \vskip1em
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/channel/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_chan_bl_building}
                            \texttt{Building errors (with \texttt{channel}).}
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/channel/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_chan_bl_facet}
                            \texttt{Facet errors (with \texttt{channel}).}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_svm_gk_scat}
                    Mean F-score and standard deviation obtained with an \gls{acr::svm}.
                    The geometric modality is based on graph kernels while height and image based features use the \gls{acr::scatnet} with \texttt{deletion} and \texttt{channel} options.
                }
            }
        \end{figure}

        \begin{figure}[htpb]
            \centering
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/deletion/feature_importance/building}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_del_svm_building}
                            \texttt{Building errors} (with \texttt{deletion}).
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/deletion/feature_importance/facet}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_del_svm_facet}
                            \texttt{Facet errors (with \texttt{deletion}).}
                        }
                    }
                \end{subfloatrow}
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/channel/feature_importance/building}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_chan_svm_building}
                            \texttt{Building errors (with \texttt{channel}).}
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/channel/feature_importance/facet}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_chan_svm_facet}
                            \texttt{Facet errors (with \texttt{channel}).}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::feature_importances_gk_scat_svm} Modality contribution for the \gls{acr::svm} classifier based on the coefficients computed by EasyMKL.
                    The geometric modality is based on graph kernels while height and image based features use the \gls{acr::scatnet} with \texttt{deletion} and \texttt{channel} options.
                    The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                }
            }
        \end{figure}
