\minitoc

\vfill

\clearpage

\section{\textsc{Scalabilitys analysis}}
    \label{sec::more_experiments::scalability}
    In the previous chapter, error detection was proven to be dependent on the scene composition
    This fact motivates studying training the classifier and testing prediction on different scenes.
    The goal is to prove the resilience of the prediction to unseen urban scenes.
    As the annotation process requires a lot of effort, this trait is crucial to guarantee the scalability of this method under the \textbf{large-scale} constraint.
    Different configurations are possible, as depicted in Figure~\ref{fig::scalability_study}.
    In the first type of experiments, we train on one urban scene and test on another one.
    The goal is to examine the \texttt{transferability} of the classifier model.
    Experimental results are reported and analyzed in Subsection~\ref{subsec::more_experiments::scalability::transferability}.
    In a second configuration, the classifier is trained on two scenes and tested on the last one: the objective is to investigate the classifier \texttt{generalization}.
    The results of such experiments are shown in Subsection~\ref{subsec::more_experiments::scalability::generalization}.
    The last experiment class, whose results are presented in Subsection~\ref{subsec::more_experiments::scalability::representativeness}, targets the \texttt{representativeness} of a single 3-area dataset by trying multiple train-test split sizes.

    \begin{figure}[htbp]
        \ffigbox[\FBwidth]{
            \includestandalone[mode=buildnew, width=.8\textwidth]{figures/scalabitity_graph}
        }
        {
            \caption{
                \label{fig::scalability_study}
                A graph representing possible experiments: arrow origins represent training scenes while test ones are depicted as targets.
                \(Z_i, i=1,2,3\) represent the urban zones.
                All these nodes are assembled in one, meaning that all urban scenes were aggregated in on train/test node.
                The numbers indicate in which section each experiment is analyzed.
            }
        }
    \end{figure}

    We will see how \texttt{Building errors} depend on the training scene, in contrast to \texttt{Facet errors}.
    The latter will prove to be more transferable and generalizable than the first one.
    We will also discuss how every modalities play a role in error prediction.
    Image-based features will demonstrate to be the most valuable compared to height-based ones.
    Eventually, we will review each \texttt{atomic} error prediction sensitivity provided the training set.

    \subsection{\textsc{Transferability study}}
        \label{subsec::more_experiments::scalability::transferability}
        In this configuration, we test how transferable are the learned classifiers from one urban scene to another.
        We train on a zone $Z_i$ and test on another one $Z_j$.
        We will denote each transferability experiment by the couple $(Z_i, Z_j)$ or by $Z_i \rightarrow Z_j$.
        Six transferability couples are possible.
        F-scores are shown, per label, and per experiment, in Figure~\ref{fig::f_score_transferability_f3}.\\
        
        \thisfloatsetup{subfloatrowsep=none}
        \begin{figure}[htbp]
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \centering
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=7.5cm]{figures/results/transferability/building}
                    }{
                        \caption{
                            \label{subfig::f_score_transferability_f3_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=7.5cm]{figures/results/transferability/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_transferability_f3_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_transferability_f3}
                    Mean F-score and standard deviation for the transferability study.
                }
            }
        \end{figure}

        First, a \texttt{coherence} analysis is performed.
        We compare the results of the transferability experiments to the ablation results with the same training scene.
        This is achieved by looking, for a given area $Z_i$ in all couples $(Z_i, Z_j)_{\forall j \neq i}$, at the differences between Figure~\ref{fig::f_score_transferability_f3} and Table~\ref{tab::ablation_f3}/Figure~\ref{fig::f_score_ablation_f3}.
        Secondly, we investigate how an urban scene composition helps predicting defects in an unseen one.
        This is called the \texttt{projectivity} comparison.
        For a given test scene $Z_j$ in couples $(Z_i, Z_j)_{\forall i \neq j}$, we compare results from Figure~\ref{fig::f_score_transferability_f3} with Table~\ref{tab::ablation_f3}/Figure~\ref{fig::f_score_ablation_f3}.
        All these comparisons are provided in Table~\ref{tab::transferability_comparison}.
        In both settings, if a feature type appears, it means it is, by a large margin, the most decisive one.
        A color scheme was devised to encode the amplitude of change.
        All various feature configurations are tested these experiments.
        If a modality stands out, in terms of the F-score, it is mentioned in the corresponding cell in Table~\ref{tab::transferability_comparison}.\\

        \begin{table}[htbp]
            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{ c c | c c c c |c c c c c}
                \hline
                &&\texttt{BOS} & \texttt{BUS}&\texttt{BIB}&\texttt{BIT}&\texttt{FOS} & \texttt{FUS}&\texttt{FIB}&\texttt{FIT}&\texttt{FIG}\\
                \hline
                \multirow{6}{*}{\rotatebox{90}{\texttt{Coherence}}}&\textbf{Elancourt} $\rightarrow$ \textbf{Nantes} &\cellcolor{LOSS2535} & \cellcolor{LOSS1525}&\cellcolor{LOSS1525}& \cellcolor{LOSS1525}& \cellcolor{STBL}& \cellcolor{GAIN0515} \textbf{Im.}&\cellcolor{GAIN15} \textbf{Im.} & \cellcolor{LOSS0515} \textbf{Im.}&\cellcolor{GAIN0515}\\
                & \textbf{Elancourt} $\rightarrow$ \textbf{Paris-13}  & \cellcolor{LOSS2535}& \cellcolor{LOSS1525}& \cellcolor{LOSS1525}& \cellcolor{LOSS1525}& \cellcolor{STBL}& \cellcolor{GAIN0515}\textbf{Im.}& \cellcolor{GAIN15}\textbf{Im.}& \cellcolor{LOSS0515}&\cellcolor{GAIN15}\\
                & \textbf{Nantes} $\rightarrow$ \textbf{Paris-13}  & \cellcolor{LOSS0515}& \cellcolor{LOSS1525}& & \cellcolor{GAIN0515} \textbf{Geom.}& \cellcolor{STBL}& \cellcolor{GAIN15}& \cellcolor{GAIN0515}& &\cellcolor{STBL} \textbf{Hei.}\\
                & \textbf{Nantes} $\rightarrow$ \textbf{Elancourt}  &\cellcolor{GAIN15} & \cellcolor{STBL}& \cellcolor{GAIN15}&\cellcolor{GAIN0515} \textbf{Geom.} & \cellcolor{STBL}& \cellcolor{LOSS1525}&\cellcolor{LOSS1525}&\cellcolor{GAIN0515}&\cellcolor{LOSS0515}\\
                & \textbf{Paris-13} $\rightarrow$ \textbf{Nantes}  &\cellcolor{LOSS0515} & \cellcolor{LOSS0515}& & \cellcolor{GAIN15} \textbf{Geom.}&\cellcolor{STBL} & \cellcolor{LOSS2535}& \cellcolor{LOSS1525}& & \cellcolor{LOSS1525}\\
                & \textbf{Paris-13} $\rightarrow$ \textbf{Elancourt}  &\cellcolor{GAIN15} &\cellcolor{GAIN0515} & \cellcolor{GAIN15}& \cellcolor{GAIN0515} \textbf{Geom.}& \cellcolor{STBL}& \cellcolor{LOSS3545}& \cellcolor{STBL} &\cellcolor{GAIN0515} & \cellcolor{LOSS0515}\\
                \cline{2-12}
                \multirow{6}{*}{\rotatebox{90}{\texttt{Projectivity}}}&\textbf{Elancourt} $\rightarrow$ \textbf{Nantes} &\cellcolor{LOSS0515} & \cellcolor{LOSS1525}&\cellcolor{LOSS0515}& \cellcolor{LOSS1525}& \cellcolor{STBL}& \cellcolor{STBL}&\cellcolor{GAIN15} \textbf{Im.} & \cellcolor{LOSS0515}&\cellcolor{STBL}\\
                & \textbf{Elancourt} $\rightarrow$ \textbf{Paris-13}  & \cellcolor{LOSS0515}& \cellcolor{LOSS1525}& \cellcolor{STBL}& \cellcolor{LOSS1525}& \cellcolor{STBL}& \cellcolor{GAIN0515}\textbf{Im.}& \cellcolor{GAIN0515}\textbf{Im.}& \cellcolor{LOSS0515}&\cellcolor{STBL}\\
                & \textbf{Nantes} $\rightarrow$ \textbf{Paris-13}  & \cellcolor{LOSS0515}& \cellcolor{LOSS1525}& & \cellcolor{GAIN15}& \cellcolor{STBL}& \cellcolor{GAIN0515}& \cellcolor{GAIN0515}& &\cellcolor{STBL}\\
                & \textbf{Nantes} $\rightarrow$ \textbf{Elancourt}  &\cellcolor{GAIN0515} & \cellcolor{LOSS0515}& \cellcolor{LOSS0515}&\cellcolor{GAIN0515}All & \cellcolor{STBL}& \cellcolor{LOSS1525}&\cellcolor{LOSS0515}\textbf{Im.} &\cellcolor{GAIN0515}\textbf{Im.} &\cellcolor{STBL}\\
                & \textbf{Paris-13} $\rightarrow$ \textbf{Nantes}  &\cellcolor{LOSS1525} & \cellcolor{LOSS0515}& & \cellcolor{GAIN0515}&\cellcolor{STBL} & \cellcolor{LOSS2535}& \cellcolor{STBL}& & \cellcolor{LOSS0515} \textbf{Hei.}\\
                & \textbf{Paris-13} $\rightarrow$ \textbf{Elancourt}  &\cellcolor{STBL} &\cellcolor{LOSS0515} & \cellcolor{GAIN0515}\textbf{Im.} & \cellcolor{STBL}& \cellcolor{STBL}& \cellcolor{LOSS3545}& \cellcolor{LOSS1525}\textbf{Im.} & & \cellcolor{STBL}\\
                \hline                                            
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption{
                \label{tab::transferability_comparison} Evolution of the F-score value, for each error, between each tested configuration and the best result per area (\textit{cf.} Sub-subsection~\ref{subsubsec::experiments::evaluation::baseline_feature_analysis::ablation}).
                Feature sets having a significant impact on the classification results are mentioned.
                Otherwise, \textbf{Geom.} \textbf{Im.}, and \textbf{Hei.} contribute equally.
                The color indicates the magnitude: \textcolor{LOSS3545}{$\blacksquare$}: $[-45,-35\%[$-- \textcolor{LOSS2535}{$\blacksquare$}: $[-35,-25\%[$ -- \textcolor{LOSS1525}{$\blacksquare$}: $[-25,15\%[$-- \textcolor{LOSS0515}{$\blacksquare$}: $[-15, 5\%[$ -- \textcolor{STBL}{$\blacksquare$}: $[-5,5\%[$-- \textcolor{GAIN0515}{$\blacksquare$}: $[5,15\%[$ -- \textcolor{GAIN15}{$\blacksquare$}: $[15,25\%]$ -- $\square$: statistics cannot be computed.
            }
        \end{table}


        To summarize the comparisons, error family wise, out of 22 \texttt{Building errors} possible \texttt{projectivity} comparisons, 14 yield worse results.
        This proves how hard it is, for this error family, to transfer learned classifiers.
        Conversely, \texttt{Facet errors}, only 8 out of 27 \texttt{projectivity} scores are worse compared to training and testing on the same test area.\\

        As mentioned earlier, additional modalities play a important role in prediction accuracy.
        We start with image-based attributes.
        In some cases, they were pivotal in obtaining better results for geometric errors (\texttt{FIB}, \texttt{BIB}), as well as for topological ones (\texttt{FUS}, \texttt{FIT}).
        These features have a significant \texttt{coherence} power when trained over \textbf{Elancourt} (\texttt{FIB} and \texttt{FUS}), and projects very well to other scenes (\texttt{FIB}, \texttt{FUS}, \texttt{BIB} and \texttt{FIT}, Table~\ref{tab::transferability_comparison}).
        On the other hand, as expected, geometric features alone are best for topological errors, when trained on dense areas, especially \texttt{BIT} (Table~\ref{tab::transferability_comparison}).
        Finally, although sticking out for \texttt{FIG} in a minor capacity (\t{cf.} Table~\ref{tab::transferability_comparison}), height-based features proved to be less transferable.
        In fact, adding height-based features leads, in most cases, to a small decrease in accuracy ($\approx 2\%$) for \texttt{atomic} errors.
        All these previous findings further justify why we did not leave out any modality, as they are more frequently critical for transferability than in the ablation study (Table~\ref{tab::ablation_f3}).\\

        An analysis can also be drawn for \texttt{atomic} errors with respect to the best training scene.
        We can see that for \texttt{BOS}, training on a dense urban scene like \textbf{Nantes}, is the best solution, as for topology errors (\texttt{FIT} and \texttt{BIT}).
        \textbf{Paris-13} represents also a dense downtown scene but with even more diverse building types.
        This is instrumental to achieve transferability for \texttt{BUS} and \texttt{BIB}.
        Conversely, \textbf{Elancourt} offers more heterogeneity on the \gls{acr::lod}-2 level.
        As a consequence, it is the best training zone for \texttt{FUS}, \texttt{FIB} and \texttt{FIG}.
        Finally, as one can obviously suspect, \texttt{FOS} learning is evenly transferable, as it is well detected when training on any scene.

    \subsection{\textsc{Generalization study}}
        \label{subsec::more_experiments::scalability::generalization}
        We try to find out how omitting one urban zone from the training dataset affects the test results on that same area.
        An other way to look at it is, from an operational point of view, to find out how much learning on a union of many urban scenes is helpful when testing on an unseen one.
        We also seek to confirm the outcome of the transferability experiments.
        Experiments that merge all zones except $Z_i$ ($\underset{\forall j \neq i}{\bigcup} Z_j$) for training and test on $Z_i$ are noted by the couple $(\underset{\forall j \neq i}{\bigcup} Z_j, Z_i)$ or by $ \underset{\forall j \neq i}{\bigcup} Z_j \rightarrow Z_i$.
        There are three possibilities: \textbf{Elancourt} $\cup$ \textbf{Nantes} $\rightarrow$ \textbf{Paris-13}, \mbox{\textbf{Paris-13}}\,$\cup$\,\textbf{Nantes}\,$\rightarrow$\,\textbf{Elancourt} and \mbox{\textbf{Paris-13}}\,$\cup$\,\textbf{Elancourt}\,$\rightarrow$\,\textbf{Nantes}.
        The F-score evolution per experiment and error is depicted in Figure~\ref{fig::f_score_generalization_f3}.\\
    
        \thisfloatsetup{subfloatrowsep=none}
        \begin{figure}[htbp]
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \centering
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=7.5cm]{figures/results/generalization/building}
                    }{
                        \caption{
                            \label{subfig::f_score_generalization_f3_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=7.5cm]{figures/results/generalization/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_generalization_f3_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{\label{fig::f_score_generalization_f3} Mean F-score and standard deviation for the generalization study per test zone.}
            }
        \end{figure}

        \begin{table}[htbp]
            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{c | c c c c |c c c c c}
                \hline
                & \texttt{BOS} & \texttt{BUS}&\texttt{BIB}&\texttt{BIT}&\texttt{FOS} & \texttt{FUS}&\texttt{FIB}&\texttt{FIT}&\texttt{FIG}\\
                \hline
                \textbf{Elancourt} & \cellcolor{LOSS1525}& \cellcolor{LOSS1525}\textbf{Im.}& \cellcolor{LOSS1525}&\cellcolor{LOSS1525} &\cellcolor{STBL} &\cellcolor{GAIN0515}\textbf{Im.} &\cellcolor{GAIN15}\textbf{Im.} &\cellcolor{LOSS0515}\textbf{Geom.} & \cellcolor{LOSS0515} Hei\\
                \textbf{Nantes} & \cellcolor{STBL}All& \cellcolor{LOSS1525}\textbf{Im.}&\cellcolor{LOSS0515}\textbf{Im.} &\cellcolor{GAIN15} &\cellcolor{STBL} & \cellcolor{STBL}& \cellcolor{LOSS1525}\textbf{Im.}& &\cellcolor{STBL}\\
                \textbf{Paris-13} &\cellcolor{LOSS1525}All &\cellcolor{LOSS1525} & &\cellcolor{GAIN0515}\textbf{Hei.} &\cellcolor{STBL} &\cellcolor{LOSS3545} &\cellcolor{LOSS1525}\textbf{Im.} & &\cellcolor{LOSS0515}\\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption{
                \label{tab::generalization_comparison} Evolution of the F-score value, for each error, between each tested configuration and the best result per area (\textit{cf.} Sub-subsection~\ref{subsubsec::experiments::evaluation::baseline_feature_analysis::ablation}).
                Feature sets having a significant impact on the classification results are mentioned.
                Otherwise, \textbf{Geom.} \textbf{Im.}, and \textbf{Hei.} contribute equally.
                The color indicates the magnitude: \textcolor{LOSS3545}{$\blacksquare$}: $[-45,-35\%[$-- \textcolor{LOSS2535}{$\blacksquare$}: $[-35,-25\%[$ -- \textcolor{LOSS1525}{$\blacksquare$}: $[-25,15\%[$-- \textcolor{LOSS0515}{$\blacksquare$}: $[-15, 5\%[$ -- \textcolor{STBL}{$\blacksquare$}: $[-5,5\%[$-- \textcolor{GAIN0515}{$\blacksquare$}: $[5,15\%[$ -- \textcolor{GAIN15}{$\blacksquare$}: $[15,25\%]$ -- $\square$: statistics cannot be computed.
            }
        \end{table}
            
        We compare these experiments with the ablation study on the same area (\t{cf.} Table~\ref{tab::generalization_comparison}).
        We analyse result along the same criteria as the transferability study.\\
    
        We start again with a comparison depending on error families.
        Out of the 11 possibilities for the \texttt{Building errors} family, 8 yield worse results.
        For the \texttt{Facet errors} family, 6 out of 13 comparisons exhibit the same trend.
        This is worst than the transferability comparisons in ratio.
        This results from the fact that fusing more datasets, that are not tailored for a specific error detection, does not help alleviating the problem.
        It only evens out the best (\t{resp.} worst) performances by including the best (\t{resp.} worst) urban scene in the training set.\\
        
        Similarly to the previous study, image and height modalities play a major role in error detection.
        Image-based features are crucial for \texttt{FIB}, \texttt{BIB}, \texttt{FUS} and \texttt{BUS} detection (Table~\ref{tab::generalization_comparison}).
        Height-based attributes, however, induce a larger improvement in predicting \texttt{FIG} and \texttt{BIT}, while geometric ones are relegated to playing a minor role.
        Otherwise, a curiosity can be noticed: only when fusing all modalities together, in \textbf{Paris-13} and \textbf{Nantes}, does predictions improve for \texttt{BOS}.\\
        
        We also confirm the observations about the best urban scene for error prediction training.
        In this case, the best zone should always give the worst scores.
        It is mostly the case with all atomic errors, with the exception of \texttt{BIT}.
        This outlier can be explained by the resemblance of the \textbf{Paris-13} \gls{acr::3d} models to \textbf{Nantes} (which were established to be the best) samples.
        Indeed, for most labels, \textbf{Nantes} and \textbf{Paris-13} reach the same scores.
        However, the discrepancy in F-scores proves the added value for each dataset.
    
    \subsection{\textsc{Representativeness study}}
        \label{subsec::more_experiments::scalability::representativeness}
        The objective is to find out, after merging all training samples from all datasets, what is the minimal amount of data that can guaranty stable predictions.
        We can, thereafter, understand how much learning on one scene typology can affect results compared to a mixed training set.
        Figure~\ref{fig::f_score_representativeness_f3} depicts F-score as a function of training ratios (between 20-70\%) and \texttt{atomic} errors.\\
                
        \thisfloatsetup{subfloatrowsep=none}
        \begin{figure}[htbp]
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/representativeness/building}
                    }{
                        \caption{
                            \label{subfig::f_score_representativeness_f3_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/representativeness/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_representativeness_f3_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{\label{fig::f_score_representativeness_f3} Mean F-score and standard deviation for the representativeness experiments depending on the training set size.}
            }
        \end{figure}
            
        We note the high stability of the F-score.
        This indicates that having a small heterogeneous dataset is not detrimental to the learning capacity and can be even the most suitable solution.
        \texttt{BOS}, \texttt{FOS}, and \texttt{FIG} have a standard deviation under 2\%, as opposed to \texttt{FIB}, \texttt{BIT} and \texttt{FIT}.
        Indeed, they have large variance, and even a larger standard deviation than mean value.
        Scalability is, hence, ensured with a limited training set.
        No standard logarithmic behavior can be found at the studied scales.
        20\% of the full label set is sufficient so as to retrieve results with a performance similar to the initial ablation study.
        The best results are observed for \texttt{BOS}, \texttt{BUS}, and \texttt{FUS}.
        These errors are topological defects of building roof facets which require a high diversity of training samples for their detection.
        More sophisticated features are however still required to help predicting less frequent and more semantic labels.
    
\section{\textsc{\acrlong*{acr::efin} study}}
    \label{sec::more_experiments::finesse}
    In this section, we reproduce the experimental settings described in Subsection~\ref{subsec::experiments::evaluation::baseline_feature_analysis} and Section~\ref{sec::more_experiments::scalability}.
    Once the \textbf{\gls{acr::efin}} is set to be 2, the goal is to find out how good, transferable and stable are the predictions of modeling error families: \texttt{Building errors} and \texttt{Facet errors}.
    Otherwise, if \textbf{\gls{acr::efin}} is fixed at level 1, the idea is to observe the predictability of defectuous model compared to \texttt{Valid} ones.
    
    \subsection{\textsc{Error family detection}}
        \label{subsec::more_experiments::finesse::2}
        \begin{table}[htbp]
            \centering
            \begin{tabular}{|c | c c | c c | c c | c c |}
                \hline
                \multicolumn{9}{|c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{1.8cm}|}{\textbf{All}}\\
                \cline{2-9}
                & $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ \\
                \hline
                Building errors & 99.76 & \textbf{85.96} & 99.82 & 85.88 & 99.88 & 85.57 & \textbf{100} & 85.55 \\
                \hline
                Facet errors & 91.79 & \textbf{89.79} & 92.65 & 89.40 & 93.21 & 89.45 & \textbf{93.46} & 89.16 \\
                \hline
                \hline
                \multicolumn{9}{|c|}{\textbf{Nantes}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{1.8cm}|}{\textbf{All}}\\
                \cline{2-9}
                & $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ \\
                \hline
                Building errors & 85.98 & 67.27 & 87.59 & 67.79 & 85.75 & 68.32 & \textbf{86.90} & \textbf{69.23} \\
                \hline
                Facet errors & 91.20 & 94.01 & 91.37 & \textbf{94.36} & 91.20 & 94.35 & \textbf{91.73} & 94.21\\
                \hline
                \hline
                \multicolumn{9}{|c|}{\textbf{Paris-13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{1.8cm}|}{\textbf{All}}\\
                \cline{2-9}
                & $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ \\
                \hline
                Building errors & 97.36 & 68.76 & 97.36 & 68.76 & 97.36 & 68.76 & 97.36 & 68.76 \\
                \hline
                Facet errors & 99.03 & 91.26 & 99.03 & 91.26 & 99.03 & 91.26 & 99.03 & 91.26 \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::ablation_f2}
                Feature ablation study on the three datasets for the \textbf{\gls{acr::efin}} = 2 case.
                Results are expressed in percentage.
            }
        \end{table}
    
        \begin{figure}[htbp]
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[0.5\textwidth]{
                        \centering
                        \includestandalone[mode=buildnew, width=.45\textwidth]{figures/results/finesse_2/ablation}
                    }{
                        \caption{
                            \label{subfig::f_score_ablation_f2}
                            Test scores for classifiers trained on the same zone.
                        }
                    }
                    \ffigbox[0.5\textwidth]{
                        \centering
                        \includestandalone[mode=buildnew, width=.45\textwidth]{figures/results/finesse_2/transferability}
                    }{
                        \caption{
                            \label{subfig::f_score_transferability_f2}
                            Transferability experiments.
                        }
                    }
                \end{subfloatrow}
                \vskip1em
                \begin{subfloatrow}
                    \ffigbox[\FBwidth]{
                        \centering
                        \includestandalone[mode=buildnew, width=.45\textwidth]{figures/results/finesse_2/representativeness}
                    }{
                        \caption{
                            \label{subfig::f_score_representativeness_f2}
                            Representativeness experiments.
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_f2}
                    F-score mean and standard deviation for the feature ablation study outcomes per zone for \texttt{finesse} level 2.
                }
            }
        \end{figure}
    
        We start by the ablation study.
        Table~\ref{tab::ablation_f2} reveals that inserting more remote sensing modalities do not change the prediction results dramatically.
        This is perfectly illustrated, in Figure~\ref{fig::f_score_f2}, by the low variance of F-scores for the three areas of interest.
        These results are in line with the conclusions at the \texttt{finesse} level 3.
        In the higher \texttt{finesse} level, only \texttt{BUS}, from all \texttt{atomic} errors, was highly impacted by a change in the feature configuration.
        This may explain the observed low variability.
        Moreover, we can note the prevalence of \texttt{FOS}\footnote{
            Between 60\% and 85\% of building models, in all areas, have this defect.
        } and \texttt{FIG}\footnote{
            They affect between 70\% and 95\% of building models, depending on the urban scene.
        } in the \texttt{Facet errors} family.
        This, added to the fact that they are, in a large capacity, easily detected individually\footnote{
            \texttt{FOS} (\textit{resp.} \texttt{FIG}) achieves more than 90\% (\textit{resp.} 80\%) in F-score, as depicted in Subfigure~\ref{subfig::f_score_ablation_f3_facet}
        } helps understanding why the F-score reaches at least 90\% for this family (Subfigure~\ref{subfig::f_score_ablation_f2}).
        As with \texttt{finesse} level 3 experiments, \texttt{Facet errors} yields higher prediction scores than on \texttt{Building errors}.
        Indeed, we can see a smaller discrepancy, that is below 5\%, between F-scores on different scenes for \texttt{Facet errors} than for \texttt{Building errors} with 15\%.\\

        The transferability study (Subfigure~\ref{subfig::f_score_transferability_f2}) compares the F-scores with the ablation study provided in Subfigure~\ref{subfig::f_score_ablation_f2}.
        Out of all 12 possible comparisons, only 2 exhibit a decrease in error discrimination.
        Both affect the \texttt{Building errors} family when trained on \textbf{Nantes}.
        \texttt{Facet errors}, on the other hand, confirms, its transferability and stability, with less than 5\% of discrepancy between the two extremal values.
        For this reason, we skip the generalization study, all together, at this section.\\

        The representativeness study conducted for the \texttt{finesse} level 2 results in the F-scores that are illustrated in Subfigure~\ref{subfig::f_score_representativeness_f2}.
        Family detection scores are very stable across all different tested split ratios.
        Moreover, in contrast to \texttt{atomic} errors results (cf. Figure~\ref{fig::f_score_representativeness_f3}), F-scores do not vary by more than 1\% in mean and standard deviation.
        This proves that at \texttt{finesse} level 2, error family prediction is evened out independent of different split ratios, as opposed to higher order errors.
        Again, it benefits from the higher heterogeneity of the training set with multiple areas.
    
    \subsection{\textsc{Detection of erroneous models}}
        \label{subsec::more_experiments::finesse::1}
        \begin{table}
            \renewcommand{\arraystretch}{1.5}
            \begin{tabular}{|c | c c | c c | c c | c c |}
                \hline
                \multicolumn{9}{|c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} \\
                \hline
                \texttt{Erroneous} & 99.95 & $\frac{1}{57}$ & 99.95 & $\frac{1}{57}$ & 99.95 & $\frac{0}{57}$ & 99.95 & $\frac{1}{57}$ \\
                \hline
                \hline
                \multicolumn{9}{|c|}{\textbf{Nantes}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} \\
                \hline
                \texttt{Erroneous} & 99.84 & $\frac{0}{55}$ & 99.84 & $\frac{0}{55}$ & 100 & $\frac{0}{55}$ & \textbf{100} & $\frac{0}{55}$ \\
                \hline
                \hline
                \multicolumn{9}{|c|}{\textbf{Paris-13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} &  $\bm{Rec}$ & \texttt{Valid} \\
                \hline
                \texttt{Erroneous} & 99.77 & $\frac{3}{21}$ & 99.77 & $\frac{3}{21}$ & 99.77 & $\frac{3}{21}$ & 99.77 & $\frac{3}{21}$ \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption{\label{tab::ablation_f1}Test results expressed in percentage for the \texttt{finesse}$ =1$ case. All four configurations are compared across both family errors.}
        \end{table}
        
        For the level 1 in \texttt{finesse}, we start with the feature ablation experiments.
        Since valid samples are very rare in our case, it is expected that it will be very difficult to detect these instances.
        In consequence, in Table~\ref{tab::ablation_f1}, we choose to report correctly \texttt{Valid} buildings instead of computing the precision score in percentage.\\
            
        At this level, even more that the error family semantic degree, feature configurations have virtually no impact on test results: \textbf{Elancourt} was the only exception when image features are added to geometric ones.
        Furthermore, we confirm expectations as, at most, only 1 out of 57 (\textit{resp.} 0 out of 55 and 3 out of 21) valid instances are detected for \textbf{Elancourt} (\textit{resp.} \textbf{Nantes} and \textbf{Paris-13}).
        As a consequence, we do not report the rest of previously conducted experiments for this \texttt{finesse} level.
        Indeed, it is senseless to compare detection transferability, generalization or representativeness if we hardly detect them at all on the same training scene.

\section{\textsc{Classifier analysis}}
    \subsection{\textsc{\acrshort*{acr::svm} compared to \acrshort*{acr::rf}}}
        \begin{table}[htpb]
            \small
            \begin{center}
                \begin{tabular}{| c | c c | c c | c c | c c |}
                    \hline
                    \multicolumn{9}{|c|}{\textbf{Elancourt}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                    \cline{2-9}
                    & $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ \\
                    \hline
                    \texttt{BOS} & \textbf{93.96} & 76.15 & 91.43 & \textbf{77.76} & 91.51 & 76.08 & 90.83 & 76.14 \\
                    \hline
                    \texttt{BUS} & 32.98 & \textbf{76.47} & \textbf{41.86} & 75.57 & 40.38 & 71.00 & 39.32 & 71.81 \\
                    \hline
                    \texttt{BIB} & 12.32 & 67.57 & 12.81 & \textbf{68.42} & 16.26 & 67.35 & \textbf{16.75} & 68.0 \\
                    \hline
                    \texttt{BIT} & \textbf{25.25} & 92.59 & 20.20 & 90.91 & 20.20 & \textbf{95.24} & 11.11 & 91.67 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.91 & 99.07 & 98.91 & \textbf{99.30} & \textbf{98.99} & 98.84 & 98.91 & 98.84 \\
                    \hline
                    \texttt{FUS} & \textbf{1.90} & 54.55 & 0.63 & \textbf{66.67} & 1.61 & 50 & 1.27 & \textbf{66.67} \\
                    \hline
                    \texttt{FIB} & \textbf{9.17} & 87.5 & 0 & --- & 8.30 & 82.61 & 7.42 & \textbf{100} \\
                    \hline
                    \texttt{FIT} & 6.67 & \textbf{100} & \textbf{8.73} & 95.24 & 3.33 & \textbf{100} & 3.33 & \textbf{100} \\
                    \hline
                    \texttt{FIG} & \textbf{80.54} & 73.14 & 80.45 & \textbf{72.62} & 78.69 & 72.12 & 79.02 & 71.82 \\
                    \hline
                    \hline
                    \multicolumn{9}{|c|}{\textbf{Paris-13} \(\cup\) \textbf{Nantes}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                    \cline{2-9}
                    & $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ \\
                    \hline
                    \texttt{BOS} & \textbf{51.65} & 78.93 & 47.84 & \textbf{81.75} & 39.50 & 72.45 & 47.43 & 78.57 \\
                    \hline
                    \texttt{BUS} & 19.85 & \textbf{100} & 22.90 & \textbf{100} & 7.69 & \textbf{100} & \textbf{34.61} & 93.75 \\
                    \hline
                    \texttt{BIB} & \textbf{1.96} & \textbf{100} & 0.65 & 100 & 0 & --- &  1.31 & 100 \\
                    \hline
                    \texttt{BIT} & \textbf{5.32} & \textbf{100} & 3.19 & 100 & 0 & --- & 1.06 & 100 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.62 & 98.22 & 98.62 & 98.21 & 98.48 & 75.47 & \textbf{98.62} & \textbf{98.76} \\
                    \hline
                    \texttt{FUS} & \textbf{68.80} & 77.44 & 68.18 & 77.10 & 23.14 & \textbf{84.85} & 67.83 & 78.15 \\
                    \hline
                    \texttt{FIB} & 55.23 & \textbf{78.60} & 53.59 & 78.47 & 18.95 & 71.60 & \textbf{65.35} & 74.63 \\
                    \hline
                    \texttt{FIT} & 6.25 & 100 & 6.25 & 100 & 5.88 & 100 & \textbf{11.76} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & 94.55 & 82.54 & 95.15 & 82.72 & \textbf{100} & 73.01 & 95.15 & \textbf{83.60} \\
                    \hline
                \end{tabular}
            \end{center}
            \caption{
                \label{tab::rf_f3}
                \gls{acr::rf} results on the two areas at \texttt{finesse} level 3.
                Test results are expressed in percentage.
                All \texttt{atomic} errors are considered over all possible configurations.
            }
        \end{table}
        \begin{table}[htpb]
            \small
            \begin{center}
                \begin{tabular}{| c | c c | c c | c c | c c |}
                    \hline
                    \multicolumn{9}{|c|}{\textbf{Elancourt}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                    \cline{2-9}
                    & $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ \\
                    \hline
                    \texttt{BOS} & \textbf{97.67} & 86.44 & 91.29 & \textbf{91.57} & 91.29 & 91.56 & 41.51 & 76.70 \\
                    \hline
                    \texttt{BUS} & 32.27 & 86.85 & 30.15 & 90.45 & 30.14 & 90.45 & \textbf{42.89} & \textbf{92.66} \\
                    \hline
                    \texttt{BIB} & \textbf{97.02} & 52.27 & 91.09 & \textbf{89.75} & 91.08 & \textbf{89.75} & 67.98 & 45.10 \\
                    \hline
                    \texttt{BIT} & 100 & 73.88 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 53.88 & \textbf{99.71} & 51.87 & 99.70 & 51.87 & 99.70 & \textbf{63.06} & 94.08 \\
                    \hline
                    \texttt{FUS} & 96.49 & \textbf{52.24} & \textbf{98.73} & 21.86 & \textbf{98.73} & 21.87 & 90.79 & 17.06 \\
                    \hline
                    \texttt{FIB} & 33.77 & 74.03 & 17.54 & 88.89 & 17.54 & 88.89 & \textbf{71.93} & \textbf{93.71} \\
                    \hline
                    \texttt{FIT} & 100 & 88.24 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & \textbf{84.57} & \textbf{88.47} & 65.59 & 83.14 & 65.76 & 83.08 & 52.20 & 62.99 \\
                    \hline
                    \hline
                    \multicolumn{9}{|c|}{\textbf{Paris-13} \(\cup\) \textbf{Nantes}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. $\cup$ Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                    \cline{2-9}
                    & $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ &  $\bm{Rec}$ & $\bm{Prec}$ \\
                    \hline
                    \texttt{BOS} & \textbf{44.86} & \textbf{54.09} & 29.98 & 42.69 & 29.98 & 42.69 & 29.98 & 42.69 \\
                    \hline
                    \texttt{BUS} & \textbf{98.46} & \textbf{27.35} & 36.15 & 13.51 & 41.54 & 15.21 & 30.0 & 11.44 \\
                    \hline
                    \texttt{BIB} & \textbf{82.35} & \textbf{17.31} & 70.59 & 13.53 & 70.58 & 13.53 & 70.59 & 13.53 \\
                    \hline
                    \texttt{BIT} & \textbf{95.74} & \textbf{30.93} & 50.26 & 16.67 & 50.26 & 16.67 & 50.26 & 16.67 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.90 & 75.08 & \textbf{99.31} & 74.77 & \textbf{99.31} & 74.69 & 99.17 & \textbf{81.98} \\
                    \hline
                    \texttt{FUS} & \textbf{87.40} & \textbf{65.08} & 30.79 & 43.70 & 30.79 & 43.70 & 30.79 & 43.70 \\
                    \hline
                    \texttt{FIB} & \textbf{97.06} & \textbf{38.17} & 71.90 & 27.88 & 71.90 & 27.88 & 70.36 & 27.07 \\
                    \hline
                    \texttt{FIT} & 100 & 89.47 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & \textbf{95.64} & 77.89 & 71.39 & \textbf{77.91} & 71.27 & 77.88 & 60.36 & 72.81 \\
                    \hline
                \end{tabular}
            \end{center}
            \caption{
                \label{tab::svm_f3}
                \gls{acr::svm} results on the two areas at \texttt{finesse} level 3.
            }
        \end{table}

        \begin{table}
            \footnotesize
            \begin{tabular}{c c c c}
                \toprule
                & \textbf{Elancourt} & \textbf{Nantes} \(\cup\) \textbf{Paris-13}\\
                \midrule
                \texttt{BOS} & 83.52 $\pm$ 0.66 & 45.71 $\pm$ 1.28 \\
                \midrule
                \texttt{BUS} & 50.56 $\pm$ 3.26 & 26.11 $\pm$ 15.12 \\
                \midrule
                \texttt{BIB} & 23.87 $\pm$ 3.10 & 0.99 $\pm$ 1.14 \\
                \midrule
                \texttt{BIT} & 31.47 $\pm$ 8.35 & 3.72 $\pm$ 2.82 \\
                \midrule
                \midrule
                \texttt{FOS} & 98.97 $\pm$ 0.10 & 98.20 $\pm$ 0.16 \\
                \midrule
                \texttt{FUS} & 2.63 $\pm$ 1.04 & 35.29 $\pm$ 2.37 \\
                \midrule
                \texttt{FIB} & 11.37 $\pm$ 7.67 & 50.13 $\pm$ 4.10 \\
                \midrule
                \texttt{FIT} & 10.35 $\pm$ 4.73 & 0 $\pm$ 0 \\
                \midrule
                \texttt{FIG} & 75.88 $\pm$ 0.73 & 81.64 $\pm$ 0.58 \\
                \bottomrule
            \end{tabular}
            \caption{\label{tab::f_score_rf_f3} Mean F-score and standard deviation using \gls{acr::rf}.}
        \end{table}

    \subsection{\textsc{Feature importances using \acrshort*{acr::mkl}}}
    
\section{\textsc{Richer features contributions}}
    \label{sec::more_experiments::richer_features}
    \subsection{\textsc{\acrshort*{acr::scatnet} to baseline comparison}}
        \label{subsec::more_experiments::richer_features::scatnet_baseline}
    \subsection{\textsc{Graph kernels to baseline comparison}}
    \subsection{\textsc{Graph kernels and \acrshort*{acr::scatnet} to baseline comparison}}
