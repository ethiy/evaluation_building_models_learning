\minitoc

\vfill

The goal of this chapter is to apply the feature configurations presented in Chapter~\ref{chap::better_representation} and analyse the experimental results.
First, in Section~\ref{sec::advanced_experiments::dataset}, we explain how the dataset is setup for the new experiments.
Next, in Section~\ref{sec::advanced_experiments::classifier}, both classifiers, \gls{acr::svm} and \gls{acr::rf}, are trained with this new dataset setup using always the baseline features (cf. Section~\ref{sec::learned_evaluation::baseline}).
Third, in Section~\ref{sec::advanced_experiments::better_features}, we present the results of the new representation for \gls{acr::3d} models and compare them to the baseline results.

\clearpage

\section{Fusing \textbf{Paris-13} and \textbf{Nantes}}
    \label{sec::advanced_experiments::dataset}
    According to the findings of the previous sections (cf. Sections~\ref{subsec::experiments::scalability::transferability} and~\ref{subsec::experiments::finesse::2}), \textbf{Paris-13} and \textbf{Nantes} are similar compared to \textbf{Elancourt}.
    Moreover, the latter area contains a lot more instances than the others which is not ideal for comparisons.
    As a consequence, both \textbf{Paris-13} and \textbf{Nantes} were fused in one set denoted from now by \textbf{Na-P13}.
    It contains \num{1226} buildings compared to \num{2007} instances of \textbf{Elancourt}.
    In Figure~\ref{fig::error_fused_statistics}, we remind the reader of the \textbf{Elancourt} area error distributions which are compared this time to statistics from the fused set \textbf{Na-P13}.\\

    \begin{figure}[htp]
        \centering
        \ffigbox[\textwidth]{
            \begin{subfloatrow}
                \ffigbox[\textwidth]{
                    \includestandalone[mode=buildnew, height=8cm]{figures/datasets/fused/lod1_stats}
                }{
                    \caption{
                        \label{subfig::lod1_errors_fused}
                        Occurence statistics for \texttt{Building errors}.
                    }
                }
            \end{subfloatrow}
            \vskip2em
            \begin{subfloatrow}
                \ffigbox[\textwidth]{
                    \includestandalone[mode=buildnew, width=.8\textwidth]{figures/datasets/fused/lod2_stats}
                }{
                    \caption{
                        \label{subfig::lod2_errors_fused}
                        Occurence statistics for \texttt{Facet errors}.
                    }
                }
            \end{subfloatrow}
        }{
            \caption[
                Detailed error statistics depending on the urban scenes.
            ]{
                \label{fig::error_fused_statistics}
                Detailed error statistics depending on the new experimental sets.
                The height of bars indicates the frequency of each errors while the number of occurences is displayed over.
            }
        }
    \end{figure}

    Naturally, as \textbf{Nantes} contains more (around \num{1.56} times more) instances than \textbf{Paris-13}, \textbf{Na-P13} error statistics profile looks a bit more like the one of \textbf{Nantes} than the other area as shown in Figure~\ref{fig::error_statistics}.
    According to Sections~\ref{subsec::experiments::scalability::transferability} and~\ref{subsec::experiments::finesse::2}, we can expect that \textbf{Na-P13} would be better suited to learn \texttt{Facet errors}, with the exception of \texttt{FIT}, compared to \textbf{Elancourt}, while the latter is also the best alternative for \texttt{Building errors}.

\section{Classifier choice analysis}
    \label{sec::advanced_experiments::classifier}
    The aim of this section is to find out how beneficial the use of \glspl{acr::svm} can be if used instead of \glspl{acr::rf}.
    Two reasons motivate this experimental comparison:
    \begin{enumerate}[label=\roman*)]
        \item \glspl{acr::svm} are more adapted to kernels, as we plan trying graph kernels as feature extractors;
        \item \glspl{acr::svm} are better suited for unbalanced labels, which is the case of \texttt{BIT} and \texttt{FIT} for instance.
    \end{enumerate}
    Hereafter, we first describe the urban scenes that are studied.
    Next, we compare the results obtained using the \gls{acr::svm} classifier to the ones resulting from the \gls{acr::rf}.
    We end with a comparison of feature importances computed for the two classifiers.

    \subsubsection{\texorpdfstring{\acrshort*{acr::rf}}{RF} results.}
        \label{subsec::advanced_experiments::classifier::rf}
        For \textbf{Elancourt} results remain unchanged using the \gls{acr::rf} classifier and are reported along the newer results on \textbf{Na-P13} in Table~\ref{tab::rf_f3}.\\

        \begin{table}[htpb]
            \footnotesize
            \centering
            \begin{tabular}{| c | c c | c c | c c | c c |}
                \hline
                & \multicolumn{8}{c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{93.96} & \textbf{76.15} & 91.43 & 77.76 & 91.51 & 76.08 & 90.83 & 76.14 \\
                \hline
                \texttt{BUS} & 32.98 & 76.47 & \textbf{41.86} & \textbf{75.57} & 40.38 & 71.00 & 39.32 & 71.81 \\
                \hline
                \texttt{BIB} & 12.32 & 67.57 & 12.81 & 68.42 & 16.26 & 67.35 & \textbf{16.75} & \textbf{68.0} \\
                \hline
                \texttt{BIT} & \textbf{25.25} & \textbf{92.59} & 20.20 & 90.91 & 20.20 & 95.24 & 11.11 & 91.67 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.91 & 99.07 & \textbf{98.91} & \textbf{99.30} & 98.99 & 98.84 & 98.91 & 98.84 \\
                \hline
                \texttt{FUS} & \textbf{1.90} & \textbf{54.55} & 0.63 & 66.67 & 1.61 & 50 & 1.27 & 66.67 \\
                \hline
                \texttt{FIB} & \textbf{9.17} & \textbf{87.5} & 0 & --- & 8.30 & 82.61 & 7.42 & 100 \\
                \hline
                \texttt{FIT} & 6.67 & 100 & \textbf{8.73} & \textbf{95.24} & 3.33 & 100 & 3.33 & 100 \\
                \hline
                \texttt{FIG} & \textbf{80.54} & \textbf{73.14} & 80.45 & 72.62 & 78.69 & 72.12 & 79.02 & 71.82 \\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Na-P13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{51.65} & \textbf{78.93} & 47.84 & 81.75 & 48.15 & 77.74 & 47.43 & 78.57 \\
                \hline
                \texttt{BUS} & 19.85 & 100 & 22.90 & 100 & \textbf{36.64} & \textbf{92.31} & 34.61 & 93.75 \\
                \hline
                \texttt{BIB} & \textbf{1.96} & \textbf{100} & 0.65 & 100 & 0.65 & 100 &  1.31 & 100 \\
                \hline
                \texttt{BIT} & \textbf{5.32} & \textbf{100} & 3.19 & 100 & 2.13 & 100 & 1.06 & 100 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.62 & 98.22 & 98.62 & 98.21 & 98.48 & 98.76 & \textbf{98.62} & \textbf{98.76} \\
                \hline
                \texttt{FUS} & 68.80 & 77.44 & 68.18 & 77.10 & \textbf{68.80} & \textbf{78.54} & 67.83 & 78.15 \\
                \hline
                \texttt{FIB} & 55.23 & 78.60 & 53.59 & 78.47 & 65.47 & 74.44 & \textbf{65.35} & \textbf{74.63} \\
                \hline
                \texttt{FIT} & 6.25 & 100 & 6.25 & 100 & 6.25 & 100 & \textbf{11.76} & \textbf{100} \\
                \hline
                \texttt{FIG} & 94.55 & 82.54 & 95.15 & 82.72 & 94.55 & 83.43 & \textbf{95.15} & \textbf{83.60} \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::rf_f3}
                \gls{acr::rf} results on the two datasets of interest at \textbf{\gls{acr::efin}} level 3.
                Test results are expressed in percentage.
                All possible modality configurations are tested using baseline features.
            }
        \end{table}

        Regarding \textbf{Na-P13}, one natural prediction is that scores would average out with the same ratios as the frequency of error labels (cf. Figure~\ref{fig::error_fused_statistics}).
        By accounting for the random nature of the choice in training instances during the cross validation, this could be argued to be true for \texttt{FOS} or \texttt{FIG} and, in a lesser extent, for \texttt{BUS} too.
        However, it is far from being true for the rest of errors as shown in Table~\ref{tab::all_f-scores_rf_f3}.
        In fact, for the other six error labels, F-scores, on the fused set, are better than those on both \textbf{Nantes} and \textbf{Paris-13}.
        Notably, some instances of \texttt{BIT} and \texttt{FIT} are now detected, in best cases, at arounf \SI{11}{\percent} and \SI{21}{\percent} respectively.
        On the contrary, they were not detected at all on the separate areas (cf. Table~\ref{tab::ablation_f3}).
        This can be explained by the fact that, although having around the same statistical distribution of errors as \textbf{Nantes} and \textbf{Paris-13} in a lesser extent, the fused set contains enough instances to better learn than before.
        It can also be the result of the fact that the two areas complement each other, as better shown in Figure~\ref{tab::transferability_comparison}, with \texttt{BIT}, where training on \textbf{Nantes} and testing on the other and vice versa proved to be better than training and testing on the same zone.\\
        
        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/rf/building}
                    }{
                        \caption{
                            \label{subfig::f_score_rf_bl_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/rf/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_rf_bl_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_rf_bl}
                    Mean F-score and standard deviation obtained with an \gls{acr::rf} using baseline features.
                }
            }
        \end{figure}
    
        Mean and standard deviation F-scores are vizualized in Figure~\ref{fig::f_score_rf_bl}.
        Obviously, everything remains unchanged for \textbf{Elancourt}.
        However, we see how the standard deviations on the fused set seem to be greater than what observed previously in Figure~\ref{fig::f_score_ablation_f3}.
        Added to the labels \texttt{BUS} and \texttt{FIB} that were previously improved by the use of image based features as shown in Table~\ref{tab::all_f-scores_ablation_f3}, \texttt{BIT} and \texttt{FIT} F-scores are also greatly impacted on the new fused set.
        In fact, the first better performs when only geometric features are used as previously explained before in Section~\ref{subsec::experiments::baseline_feature_analysis::ablation}.
        For the Second, it was image based features that proved to be better suited.
        This agrees with the fact that, for the \textbf{Elancourt} \(\rightarrow\) \textbf{Nantes} experiment, image based features were instrumental in better detecting \texttt{FIT} than training on \textbf{Nantes} itself (cf. Figure~\ref{tab::transferability_comparison}).
        
    \subsection{\texorpdfstring{\acrshort*{acr::svm}}{SVM} results}
        \label{subsec::advanced_experiments::classifier::svm}
        Now that we discussed the \gls{acr::rf} results on the fused set, we can move on to the \gls{acr::svm} experimental results on both identified urban sets: \textbf{Elancourt} and \textbf{Na-P13}.
        Results are reported in Table~\ref{tab::svm_f3}.\\

        \begin{table}[htpb]
            \footnotesize
            \centering
            \begin{tabular}{| c | c c | c c | c c | c c |}
                \hline
                & \multicolumn{8}{c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{97.67} & \textbf{86.44} & 91.29 & 91.57 & 91.29 & 91.56 & 41.51 & 76.70 \\
                \hline
                \texttt{BUS} & 32.27 & 86.85 & 30.15 & 90.45 & 30.14 & 90.45 & \textbf{42.89} & \textbf{92.66} \\
                \hline
                \texttt{BIB} & 97.02 & 52.27 & \textbf{91.09} & \textbf{89.75} & 91.08 & 89.75 & 67.98 & 45.10 \\
                \hline
                \texttt{BIT} & 100 & 73.88 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 53.88 & 99.71 & 51.87 & 99.70 & 51.87 & 99.70 & \textbf{63.06} & \textbf{94.08} \\
                \hline
                \texttt{FUS} & \textbf{96.49} & \textbf{52.24} & 98.73 & 21.86 & 98.73 & 21.87 & 90.79 & 17.06 \\
                \hline
                \texttt{FIB} & 33.77 & 74.03 & 17.54 & 88.89 & 17.54 & 88.89 & \textbf{71.93} & \textbf{93.71} \\
                \hline
                \texttt{FIT} & 100 & 88.24 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & \textbf{84.57} & \textbf{88.47} & 65.59 & 83.14 & 65.76 & 83.08 & 52.20 & 62.99 \\
                \hline
                \hline
                & \multicolumn{8}{c|}{\textbf{Na-P13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}}\\
                \cline{2-9}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{44.86} & \textbf{54.09} & 29.98 & 42.69 & 29.98 & 42.69 & 29.98 & 42.69 \\
                \hline
                \texttt{BUS} & \textbf{98.46} & \textbf{27.35} & 36.15 & 13.51 & 41.54 & 15.21 & 30.0 & 11.44 \\
                \hline
                \texttt{BIB} & \textbf{82.35} & \textbf{17.31} & 70.59 & 13.53 & 70.58 & 13.53 & 70.59 & 13.53 \\
                \hline
                \texttt{BIT} & \textbf{95.74} & \textbf{30.93} & 50.26 & 16.67 & 50.26 & 16.67 & 50.26 & 16.67 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.90 & 75.08 & 99.31 & 74.77 & 99.31 & 74.69 & \textbf{99.17} & \textbf{81.98} \\
                \hline
                \texttt{FUS} & \textbf{87.40} & \textbf{65.08} & 30.79 & 43.70 & 30.79 & 43.70 & 30.79 & 43.70 \\
                \hline
                \texttt{FIB} & \textbf{97.06} & \textbf{38.17} & 71.90 & 27.88 & 71.90 & 27.88 & 70.36 & 27.07 \\
                \hline
                \texttt{FIT} & 100 & 89.47 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & \textbf{95.64} & \textbf{77.89} & 71.39 & 77.91 & 71.27 & 77.88 & 60.36 & 72.81 \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::svm_f3}
                \gls{acr::svm} results on the two datasets at \textbf{\gls{acr::efin}} level 3.
            }
        \end{table}

        Two observations are worth noting herein:
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item First is the fact that, contrarily to \gls{acr::rf} results, adding either height or image based features alone to the intrinsic features produced the same scores.
                    However, when adding both, it yields different scores, as can be shown in Table~\ref{tab::all_f-scores_svm_f3}.
                    This was the case for all errors but \texttt{BIT} and \texttt{FIT} on both sets, as well as \texttt{FUS} on \textbf{Na-P13}.
                    One possible explanation is that both features have the same dynamic as they are both histogram values, as designed in Section~\ref{sec::learned_evaluation::baseline}.
                    As a consequence, in this case, the \gls{acr::svm} considers both feature configurations to be similar, unless when fed together to the \gls{acr::svm}.
                    This also could be the result of the fact that the parameterization of the classifier was not ideal and does not learn properly when external modalities are added.
            \item Secondly, in some cases, the \gls{acr::svm}, for some particular feature configurations, yields results that exceed the other ones by a large margin.
                    This was the case of \texttt{FUS} on \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}) with a jump of around \SI{31}{\percent} with the \textbf{Geom.} configuration (\textit{resp.} \SI{38}{\percent}) in F-score and \SI{35}{\percent} for \texttt{FIB} on \textbf{Na-P13} with the \textbf{All} configuration.
                    This could be owed to two possible reasons.
                    Either these feature configurations are actually the best alternatives which is not conflicting with previous findings.
                    Indeed, the extrinsic features were designed in the first place to detect fidelity errors such as \texttt{FIB} while \texttt{FUS} is a topological error that can be suitably detected using intrinsic features only.
                    However, these large margins could be also explained by the fact that the \gls{acr::svm} overfitted in these special cases.
                    The last reason cannot be ruled out either, as the \(\gamma\) hyper-parameter was not optimized for these features as seen in Section~\ref{subsec::experiments::setup::classification}, due to the high number of possible combinations.
            \end{itemize}
            
        As with the \gls{acr::rf} classifier, mean and standard deviation F-scores are computed and vizualized in Figure~\ref{fig::f_score_svm_bl}.
        Further commentary is left for the next sub-subsection as these results are compared to the \gls{acr::rf} ones.

        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/svm/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_bl_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/svm_rf/svm/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_bl_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_svm_bl}
                    Mean F-score and standard deviation obtained with an \gls{acr::svm} using baseline features.
                }
            }
        \end{figure}

    \subsection{\texorpdfstring{\acrshort*{acr::svm}}{SVM} compared to \texorpdfstring{\acrshort*{acr::rf}}{RF}}
        \label{subsec::advanced_experiments::classifier::svm_rf}
        In Table~\ref{tab::rf_vs_svm_comparison}, are compared the \gls{acr::svm} F-scores (cf. Table~\ref{tab::all_f-scores_svm_f3}) to \gls{acr::rf} ones (cf. Table~\ref{tab::all_f-scores_rf_f3}).
        The same color scheme is used as with the Tables~\ref{tab::transferability_comparison} and~\ref{tab::generalization_comparison}.\\

        \begin{table}[htbp]
            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                \hline
                & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                \hline
                \textbf{Elancourt} & \cellcolor{GAIN0515} & \cellcolor{STBL} \textbf{All} & \cellcolor{GAIN45} & \cellcolor{GAIN45} & \cellcolor{LOSS1525} \textbf{All} & \cellcolor{GAIN2535} & \cellcolor{GAIN2535}  \textbf{Geom.} & \cellcolor{GAIN45} & \cellcolor{GAIN0515} \textbf{Geom.} \\
                \textbf{Na-P13} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{GAIN1525} \textbf{Geom.} & \cellcolor{GAIN3545} \textbf{Geom.} & \cellcolor{LOSS0515} \textbf{All} & \cellcolor{LOSS3545} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{GAIN45} & \cellcolor{STBL} \textbf{Geom.} \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption[
                Evolution of the F-score value, for each error using \gls{acr::svm} compared to \gls{acr::rf} and based on baseline features.
            ]{
                \label{tab::rf_vs_svm_comparison}
                Evolution of the F-score value, for each error using \gls{acr::svm} compared to \gls{acr::rf} and based on baseline features.
                Feature sets having a significant impact on the classification results are mentioned in the corresponding cell (cf. Table~\ref{tab::all_f-scores_svm_f3}).
                The used color scheme is presented in figure~\ref{fig::comparison_bar}.
            }
        \end{table}

        As suspected, \gls{acr::svm} yields better, or at least stable, results on highly unbalanced labels.
        In fact, \texttt{BIB} (\textit{resp.} \texttt{BIT} and \texttt{FIT}) with less than \SI{16}{\percent} (\textit{resp.} \SI{16}{\percent} and \SI{1.5}{\percent}) occurence ratio in both sets gained in terms of F-scores when training was conducted using an \gls{acr::svm}.
        The same pattern is observed for \texttt{BUS} (\textit{resp.} \texttt{FUS} and \texttt{FIB}) with less than \SI{25}{\percent} (\textit{resp.} \SI{19}{\percent} and \SI{12}{\percent}) frequency but on \textbf{Elancourt} only.
        However, the same set of labels, as well as \texttt{BOS}, performs worse when an \gls{acr::svm} is used on \textbf{Na-P13}, eventhough they both have a presence ratio strictly under \SI{41}{\percent}.
        On the other hand, regarding the labels that are very frequent \footnote{
            These are \texttt{BOS} on \textbf{Elancourt}, in addition to \texttt{FOS} and \texttt{FIG} on both sets.
        } (with more than \SI{50}{\percent}), it is expected that they would underperform with the use of \glspl{acr::svm}.
        Although this is true for \texttt{FOS}, on both sets, it is not the case for the other error labels.\\

        \begin{table}[htbp]
            \footnotesize
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{| c | x{2cm} x{2cm} | x{2cm} x{2cm} |}
                \hline
                & \multicolumn{2}{c |}{\textbf{Elancourt}} & \multicolumn{2}{c |}{\textbf{Na-P13}}\\
                \hline
                & \gls{acr::rf} & \gls{acr::svm} & \gls{acr::rf} & \gls{acr::svm}\\
                \hline
                \texttt{BOS} & \textbf{Geom.} & \textbf{Geom.} & \textbf{Geom.} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{BUS} & \textbf{Hei.} & \underline{\textbf{All}} & \underline{\textbf{Im.}} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{BIB} & \underline{\textbf{All}} & \underline{\textbf{Hei.}}, \textbf{Im.} & \textbf{Geom.} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{BIT} & \underline{\textbf{Geom.}} & \textbf{Hei.}, \textbf{Im.} &  \underline{\textbf{Geom.}} & \underline{\textbf{Geom.}} \\
                \hline
                \hline
                \texttt{FOS} & \textbf{Hei.} & \underline{\textbf{All}} & \textbf{All} & \underline{\textbf{All}} \\
                \hline
                \texttt{FUS} & \textbf{Geom.} & \textbf{Geom.} & \textbf{Im.} & \textbf{Geom.} \\
                \hline
                \texttt{FIB} & \textbf{Geom.} & \textbf{All} & \underline{\textbf{All}} & \underline{\textbf{Geom.}} \\
                \hline
                \texttt{FIT} & \textbf{Hei.} & \textbf{Hei.}, \textbf{Im.} & \underline{\textbf{All}} &  \textbf{Hei.}, \textbf{Im.} \\
                \hline
                \texttt{FIG} & \textbf{Geom.} & \underline{\textbf{Geom.}} & \textbf{All} & \underline{\textbf{Geom.}} \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption{
                \label{tab::svm_rf_best_features_f3}
                The best performing feature configuration per zone, label and classifier.
                This summarizes all comparisons between Tables~\ref{tab::all_f-scores_rf_f3} and~\ref{tab::all_f-scores_svm_f3}.
                The features, that stand out compared to the others in these last tables, are underlined.
            }
        \end{table}

        In order to explain these exceptions, we tried to look at the best performing modalities, per label and set.
        Hence, we can compare between the two classifiers easily as shown in Table~\ref{tab::svm_rf_best_features_f3}.
        Most labels perform best with the same kind of feature configurations.
        Most notably, we have previously suspected that using only geometric features resulted in overfitting with an \gls{acr::svm} when training for \texttt{FUS} on both sets.
        Comparing to the \gls{acr::rf}, \textbf{Geom.} was also the best alternative.
        This leads us to speculate that, contrarily to what was discussed in Section~\ref{subsec::advanced_experiments::classifier::svm}, \textbf{Geom.} did not lead to overfitting.
        It was rather the \gls{acr::svm} that did not learn properly (underfitting) using the other feature configurations.\\

        Hereafter, we analyse the cases where \gls{acr::rf} and \gls{acr::svm} did not share the same best performing feature configuration.
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item On \textbf{Elancourt}, for \texttt{BUS}, \texttt{FOS} and \texttt{FIT}, the \gls{acr::rf} performed better with height based features, compared to the \gls{acr::svm} which works better with other feature configurations containing extrinsic features.
                    Moreover, \texttt{BIT} was better detected with intrinsic features only (\textit{resp.} any extrinsic feature based configuration) when trained with an \gls{acr::rf} (\textit{resp.} \gls{acr::svm}).
                    This does not contradict previous findings in Sections~\ref{sec::experiments::baseline_feature_analysis} and~\ref{sec::advanced_experiments::scalability}.
            \item On \textbf{Na-P13}, the only labels where the better performing feature configuration changed were \texttt{BUS}, \texttt{FUS}, \texttt{FIB} and \texttt{FIG}.
                    In fact, only geometric features yielded better results using an \gls{acr::svm}, in contrast with the fact that \textbf{All} or \textbf{Im.} were the best alternative with an \gls{acr::rf}.
        \end{itemize}
        This reinforces the idea that the \gls{acr::svm} was actually not adequately parameterized for the extrinsic features as was announced in Section~\ref{subsec::experiments::setup::classification}.
        In fact, as discussed earlier in Section~\ref{subsubsec::state_of_the_art::mlpr::classifiers::svm}, \glspl{acr::svm} are very hard to parameterize compared to \glspl{acr::rf}.
        This seems to be a reasonable cause why the \gls{acr::svm} classifier underperforms compared to the \gls{acr::rf} one on \textbf{Na-P13}.

    \subsection{Modality contributions comparison}
        \label{subsec::advanced_experiments::classifier::feature_importance}
        In a linear \gls{acr::svm}, feature importance can be natively computed by looking at the weight vector \(\bm{w}\) as shown by~\textcite{guyon2002gene}.
        This is, unfortunatly, not the case.
        Instead we operate with Kernel \glspl{acr::svm} where features are fused using \gls{acr::mkl} with weights \(\left(\mu_i\right)_{i=1, 2, 3}\) (cf. Equation~\ref{eq::mkl}) that are on the simplex, as discussed in Section~\ref{subsubsec::state_of_the_art::mlpr::classifiers::svm}.
        Consequently, when training with the last feature configuration, the resulting weights could be interpreted as feature importance ratios.
        These are vizualized in Figure~\ref{fig::feature_importances_svm_bl}.\\

        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/svm_rf/feature_importance/building}
                    }{
                        \caption{
                            \label{subfig::feature_importances_svm_bl_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/svm_rf/feature_importance/facet}
                    }{
                        \caption{
                            \label{subfig::feature_importances_svm_bl_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::feature_importances_svm_bl}
                    Modality contribution for the \gls{acr::svm} classifier based on the coefficients computed by EasyMKL.
                    The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                }
            }
        \end{figure}

        We can see how these ratios stay mostly around \num[fraction-function = \sfrac]{1/3} with more leeway compared to \gls{acr::rf} (cf. Figure~\ref{fig::feature_importances_rf_bl}).
        This actually confirms how these \gls{acr::mkl} weights could be interpreted as features importances.
        The larger margins to the \num[fraction-function = \sfrac]{1/3} ratio could be explained by the sensibility of the weights to the noise when selecting training instances.
        There is, however, one exception that could be noted between Figures~\ref{fig::feature_importances_svm_bl} and~\ref{fig::feature_importances_rf_bl}.
        In fact, for \texttt{FOS}, geometric features have a larger importance when training with \gls{acr::svm} compared to the \gls{acr::rf} case.
        This is actually not an issue as it could be explained by the topological nature of the error label.\\

        Eventhough geometric features alone yield better results in some cases, the \gls{acr::mkl} cannot ignore the extrinsic features.
        As in the \gls{acr::rf} case, this may prove to be helpful for the transferability of learning.
        Consequently, it would be interesting to rerun the same scalanility experiments, as in Section~\ref{sec::advanced_experiments::scalability}, with an \gls{acr::svm}.
        This was not the case, due to time limitations.

    \subsection{Summary}
        \label{subsec::advanced_experiments::classifier::summary}
        To summarize, the aim of this study was to assess the impact of the classifier choice.
        Based on previous findings (cf. Sections~\ref{sec::experiments::baseline_feature_analysis},~\ref{sec::advanced_experiments::scalability} and~\ref{sec::advanced_experiments::finesse}), we fused both sets \textbf{Nantes} and \textbf{Paris-13} into one: \textbf{Na-P13}.
        Consequently, we learned that:
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item Fusing \textbf{Nantes} and \textbf{Paris-13} helps better train the \gls{acr::rf} classifier for previously difficult labels \texttt{FIT} and \texttt{BIT};
            \item As previously expected, the \gls{acr::svm} was not well parameterized to take full advantage of the extrinsic features;
            \item The \gls{acr::svm} classifier is much better than the \gls{acr::rf} one, for highly unbalanced labels: \texttt{BIB}, \texttt{BIT} and \texttt{FIT}.
            \item \texttt{FOS}, being very present in both sets, loses in F-score when the \gls{acr::svm} is used;
            \item Just as the \gls{acr::rf} classifier, it is hard for the \gls{acr::svm} to learn on \textbf{Na-P13};
            \item Like when using \gls{acr::rf}, all modalities are equally important for the \gls{acr::svm} classifier.
        \end{itemize}


\section{Advanced features contributions}
    \label{sec::advanced_experiments::better_features}
    The goal, herein, is to find out if it is possible to achieve better results than the ones obtained with our handcrafted baseline features.
    As we do not have enough learning instances to leverage deep learning methods, we choose instead to use graph kernels as well as \glspl{acr::scatnet} as shown in Section~\ref{sec::better_representation::evaluation} and~\ref{subsec::experiments::setup::feature_configurations}.\\

    Three possibilities are investigated:
    \begin{enumerate}[label=\roman*)]
        \item We keep the baseline geometric features and replace the basic image and height based features by the \gls{acr::scatnet} derived extractors, as explained in Section~\ref{subsec::better_representation::evaluation::image};
        \item We compare both the baseline features and graph kernels for geometric features alone;
        \item We combine both the graph kernels and the \gls{acr::scatnet} derived extractors and compare them to the previous results.
    \end{enumerate}
    As with the previous section (cf. Section~\ref{sec::advanced_experiments::classifier}), the experiments are conducted on the two sets: \textbf{Elancourt} and \textbf{Na-P13}.

    \subsection{\texorpdfstring{\acrshort*{acr::scatnet}}{ScatNet} to baseline comparison}
        \label{subsec::advanced_experiments::better_features::scatnet_baseline}
        We start by the \gls{acr::scatnet} comparisons.
        We run the same experiments, as in Section~\ref{sec::advanced_experiments::classifier}, where this time height and image based features are replaced by derived ones.
        There are two options when employing \gls{acr::scatnet} with image based features: \texttt{channel} and \texttt{deletion} (cf. Section~\ref{subsec::experiments::setup::feature_configurations}).
        This makes the number of possible feature configurations equal to six.\\

        Both the \gls{acr::rf} and \gls{acr::svm} classifiers are used.
        Results from both are first compared to the baseline results reported in Section~\ref{sec::advanced_experiments::classifier}.
        Afterwhat, we examine the differences between results from both classifiers.

        \subsubsection{\texorpdfstring{\acrshort*{acr::rf}}{RF} results.}
            \label{subsubsec::advanced_experiments::better_features::scatnet_baseline::rf}
            Results, using the \gls{acr::rf} classifier, on both sets, are reported in Table~\ref{tab::stats_scat_rf_f3}.
            In general, we can see how external modalities seem to play a more important role in detecting errors.
            This can be confirmed with the larger standard deviations depicted in Figure~\ref{fig::f_score_rf_scat_bl} compared to the baseline features (cf. Figure~\ref{fig::f_score_rf_bl}).\\

            \begin{sidewaystable}[htpb]
                \footnotesize
                \centering
                \begin{tabular}{| c | c c | c c | c c | c c | c c | c c |}
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Elancourt}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{S(d)-All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & 93.96 & 76.15 & 91.97 & 79.13 & 97.75 & 74.49 & 94.52 & 78.49 & 96.55 & 77.24 & \textbf{94.89} & \textbf{78.80} \\
                    \hline
                    \texttt{BUS} & 32.98 & 76.47 & \textbf{52.44} & \textbf{84.88} & 29.51 & 92.67 & 50.32 & 87.13 & 36.09 & 91.89 & 49.79 & 90.38 \\
                    \hline
                    \texttt{BIB} & 12.32 & 67.57 & 11.38 & 100 & 5.45 & 100 & 5.45 & 100 & 13.37 & 100 & \textbf{14.36} & \textbf{100} \\
                    \hline
                    \texttt{BIT} & 25.25 & 92.59 & \textbf{42.86} & \textbf{100} & 20.41 & 100 & 39.80 & 100 & 34.69 & 100 & 36.73 & 100 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.91 & 99.07 & 99.14 & 99.14 & 99.30 & 97.25 & 99.46 & 96.82 & 99.61 & 99.23 & \textbf{99.69} & \textbf{99.23} \\
                    \hline
                    \texttt{FUS} & 1.90 & 54.55 & 3.18 & 100 & 4.78 & 100 & 5.73 & 100 & 5.41 & 100 & \textbf{12.42} & \textbf{100} \\
                    \hline
                    \texttt{FIB} & 9.17 & 87.5 & 0.87 & 100 & 0.44 & 100 & 0 & --- & \textbf{12.28} & \textbf{100} & 11.84 & 100 \\
                    \hline
                    \texttt{FIT} & 6.67 & 100 & 20.69 & 100 & \textbf{27.59} & \textbf{100} & 20.69 & 100 & 10.34 & 100 & 6.90 & 100 \\
                    \hline
                    \texttt{FIG} & 80.54 & 73.14 & 94.83 & 74.85 & 96.27 & 73.57 & \textbf{97.12} & \textbf{74.13} & 93.98 & 74.23 & 95.17 & 74.87 \\
                    \hline
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Na-P13}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) Im.}} & \multicolumn{2}{x{2.4cm}|}{\textbf{All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & \textbf{51.65} & \textbf{78.93} & 39.92 & 85.84 & 39.71 & 96.5 & 40.95 & 90.87 & 44.65 & 95.59 & 43.21 & 93.75 \\
                    \hline
                    \texttt{BUS} & 19.85 & 100 & 0.76 & 100 & \textbf{27.69} & \textbf{100} & 25.95 & 100 & \textbf{27.69} & \textbf{100} & 26.15 & 100 \\
                    \hline
                    \texttt{BIB} & 1.96 & 100 & \textbf{2.61} & \textbf{100} & 0 & --- & 0 & --- & 2.60 & 100 & 1.96 & 100 \\
                    \hline
                    \texttt{BIT} & 5.32 & 100 & 0 & --- & \textbf{8.51} & \textbf{100} & 6.91 & 100 & 7.41 & 100 & 5.82 & 100 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.62 & 98.22 & 99.31 & 95.87 & 99.31 & 94.99 & 99.17 & 92.65 & 98.62 & 98.62 & \textbf{98.62} & \textbf{98.75} \\
                    \hline
                    \texttt{FUS} & 68.80 & 77.44 & 70.10 & 76.75 & 61.57 & 84.18 & 61.57 & 82.55 & \textbf{65.08} & \textbf{84.0} & 65.57 & 82.81 \\
                    \hline
                    \texttt{FIB} & 55.23 & 78.60 & 26.80 & 95.35 & 46.41 & 94.04 & 43.46 & 94.33 & 68.62 & 85.71 & \textbf{68.95} & \textbf{86.12} \\
                    \hline
                    \texttt{FIT} & 6.25 & 100 & 6.25 & 100 & 12.5 & 100 & 12.5 & 100 & \textbf{25.0} & \textbf{100} & \textbf{25.0} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & 94.55 & 82.54 & 98.42 & 83.45 & 97.70 & 83.71 & 98.18 & 83.85 & 97.58 & 85.10 & \textbf{97.94} & \textbf{84.96} \\
                    \hline
                \end{tabular}
                \caption{
                    \label{tab::stats_scat_rf_f3}
                    \gls{acr::rf} applied to \gls{acr::scatnet} based features.
                    Results are expressed in percentage on the two datasets at \textbf{\gls{acr::efin}} level 3.
                }
            \end{sidewaystable}
            
            In addition, based on the same Figure~\ref{fig::f_score_rf_scat_bl}, we can deduce the best option to be used for \gls{acr::scatnet} image based features.
            In most cases, the best options seems to be almost always \texttt{channel}.
            This is understandable as this option does not modify the signal, from the get go, and preserves both the image and model information until the last possible opportunity letting the classifier handle the fusion.
            The early fusion conducted when choosing the \texttt{deletion} option deforms the input signal.
            As the \gls{acr::scatnet} convolves filters and applies non linear functions to the input, the classifier cannot separate, in this case, between information that is derived from the image or from the evaluated model.\\

            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/deletion/building}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_del_bl_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/deletion/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_del_bl_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip1em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/channel/building}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_chan_bl_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/rf/channel/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_rf_scat_chan_bl_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption{
                        \label{fig::f_score_rf_scat_bl}
                        Mean F-score and standard deviation obtained with an \gls{acr::rf} based on \gls{acr::scatnet} features.
                        This is a vizualization of scores recorder in Table~\ref{tab::f_score_scat_bl}.
                    }
                }
            \end{figure}

            There are however exceptions that we divide into two cases.
            The first is where \texttt{deletion} was better with a small margin that can be explained by the noise of training data selection.
            This was the case of \texttt{BIT} on \textbf{Elancourt} and \texttt{FIG} on \textbf{Na-P13}.
            On the contrary, the Second, and more important, case is where the margin is large which was the case of \texttt{FIT} on \textbf{Elancourt} where \textbf{Geom. \(\oplus\) S(d)-Im.} had at least \SI{9}{\percent} more in F-score than the other configurations.
            At this point we do not have any explication to why this is the case.\\

            \begin{table}[htbp]
                \footnotesize 
                \centering
                \renewcommand{\arraystretch}{2}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S-Hei.} & \cellcolor{LOSS0515} \textbf{Geom.} & \cellcolor{GAIN1525} \textbf{S-Hei.} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S(d)-Im.} & \cellcolor{LOSS1525} \textbf{Geom.} & \cellcolor{GAIN2535} \textbf{S(d)-Im.} & \cellcolor{GAIN0515} \\
                        \textbf{Na-P13} & \cellcolor{STBL} \textbf{Geom.} & \cellcolor{LOSS0515} \textbf{S(d)-Im.} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S(d)-Im.} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{LOSS0515} & \cellcolor{STBL} \textbf{S(d)-Im.} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::rf_scat_bl_comparison_del}
                        Comparison with \texttt{deletion} option.
                    }
                \end{subtable}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S-Hei.} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{S-Hei.} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{S(c)-All} & \cellcolor{GAIN0515} \textbf{S(c)-Im.} & \cellcolor{GAIN1525} \textbf{S-Hei.} & \cellcolor{GAIN0515} \\
                        \textbf{Na-P13} & \cellcolor{STBL} & \cellcolor{LOSS0515} \textbf{S(c)-Im.} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S(c)-Im.} & \cellcolor{GAIN1525} \textbf{S(c)-Im.} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::rf_scat_bl_comparison_chan}
                        Comparison with \texttt{channel} option.
                    }
                \end{subtable}
                \renewcommand{\arraystretch}{1}
                \caption[
                    Evolution of the F-score value, for each error, with the \gls{acr::rf} classifier, using \gls{acr::scatnet} compared to baseline features.
                ]{
                    \label{tab::rf_scat_bl_comparison}
                    Evolution of the F-score value, for each error, with the \gls{acr::rf} classifier, using \gls{acr::scatnet} compared to baseline features.
                    Feature sets having a significant impact on the classification results are mentioned in the corresponding cell (cf. Table~\ref{tab::all_f-scores_scat_rf_f3}).
                    The color indicates the magnitude:
                    \textcolor{LOSS45}{\(\blacksquare\)}: (\SIrange[range-phrase={,  }]{-100}{-45}{\percent}]--
                    \textcolor{LOSS3545}{\(\blacksquare\)}: [\SIrange[range-phrase={,  }]{-45}{-35}{\percent})--
                    \textcolor{LOSS2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-15}{-5}{\percent}) --
                    \textcolor{STBL}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-5}{5}{\percent}) --
                    \textcolor{GAIN0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{5}{15}{\percent}) --
                    \textcolor{GAIN1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{15}{25}{\percent}) --
                    \textcolor{GAIN2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{25}{35}{\percent}) --
                    \textcolor{GAIN3545}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{35}{45}{\percent}) --
                    \textcolor{GAIN45}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{45}{100}{\percent}] --
                    When two null F-scores are compared, the cell is colored in white \(\square\): neither positive nor negative.
                }
            \end{table}

            The new results are compared, for each \gls{acr::scatnet} option, error label and set, to the baseline configuration scores (cf. Table~\ref{tab::all_f-scores_scat_rf_f3}).
            These are shown in Table~\ref{tab::rf_scat_bl_comparison} with the same color scheme as in previous comparisons.
            On \texttt{Building errors}, \gls{acr::scatnet} with \texttt{deletion} yields better (\textit{resp.} worse and stable) results 3 (\textit{resp.} 3 and 2) times.
            On \texttt{Facet errors}, the same option yields better (\textit{resp.} worse and stable) results 3 (\textit{resp.} 2 and 5) times.
            In constrast, regarding \texttt{Building errors}, the \texttt{channel} option helps \gls{acr::scatnet} achieve, better (\textit{resp.} worse and stable) results 2 (\textit{resp.} 1 and 5) times.
            On \texttt{Facet errors}, the same option yields better (\textit{resp.} worse and stable) results 6 (\textit{resp.} 0 and 3) times.
            This confirms again the fact that the \texttt{channel} option was better than \texttt{deletion}.\\

            We can also observe how height based features with \gls{acr::scatnet} are more instrumental than the baseline features, especially for \texttt{BIT} and \texttt{FIT} on \textbf{Elancourt}.
            For \texttt{BUS} on \textbf{Elancourt}, baseline height based features were also instrumental but in the same capacity as image based ones.
            It is not the case anymore when adding \gls{acr::scatnet} based ones: it is better than both baseline and advanced image based features.
            On \textbf{Na-P13}, this modality does not play an important role even with the more advanced feature extractor.
            This may be attributed to the fact that on both zones, \textbf{Nantes} and \textbf{Paris-13}, building height profiles are mostly the same, especially since the types of these buildings are less heterogeneous than on \textbf{Elancourt}.\\

            Regarding image based features with \gls{acr::scatnet}, they proved to be more decisive in error prediction, for both options.
            Although it fails to give better results for \texttt{BIB} for both sets and \texttt{BUS} on \textbf{Na-P13}, it is more helpful than baseline features, especially for topological error labels.
            In fact, contrarily to baseline image features, it is crucial in better detecting \texttt{BIT} on \textbf{Na-P13} (with \texttt{deletion}), \texttt{FUS} and \texttt{FIG} on \textbf{Elancourt}, as well as, \texttt{FIB} (with \texttt{channel}) and \texttt{FIT} on both sets.\\

            As with previous experiments, we also computed feature importances using the new configurations.
            These are normalized and shown for both sets and options in Figure~\ref{fig::feature_importances_scat_rf}.
            The normalization consists in weighting the importance ratios by the inverse of the corresponding feature vector length.
            This is conducted in order to put all modalities at an equal footing eventhough the \gls{acr::scatnet} produces a lot of features.\\

            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/deletion/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_rf_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/deletion/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_rf_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip2em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/channel/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_rf_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/rf/channel/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_rf_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption[
                        Normalized modality importance using the \gls{acr::rf} classifier and \gls{acr::scatnet} features.
                    ]{
                        \label{fig::feature_importances_scat_rf}
                        Normalized modality importance using the \gls{acr::rf} classifier and \gls{acr::scatnet} features.
                        Height and Image based features use the \gls{acr::scatnet} with both \texttt{deletion} and \texttt{channel} options.
                        The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                    }
                }
            \end{figure}

            First of all, we see how the depth based modality plays a more important role on \textbf{Elancourt} compared to the other set as seen in Table~\ref{tab::rf_scat_bl_comparison}.
            In general, it has an importance ratio less than \SI{20}{\percent} except for \texttt{BOS} and \texttt{FIB} on both sets, as well as \texttt{BUS} on \textbf{Elancourt}.\\

            Regarding image based features, we can notice that the \texttt{channel} option results in less importance than the \texttt{deletion} one.
            This can be explained by the fact that the latter contains 3/4 times less coefficients than the other case.
            However, this reason fails to describe the case where the discrepancy between the two cases is too important.
            In fact, for \texttt{FIB} on \textbf{Na-P13}, as well as \texttt{FIT} and \texttt{FIG} on both sets, the image based features are too important with \texttt{deletion} than with \texttt{channel}.
            This is actually not beneficial for learning as height based features were essential to achieve better results.
            This means that the fact that the \texttt{channel} option was better not only because it is on its own better than the alternative, but also because it works better with height based features.\\

            When examnining the \texttt{channel} option (cf. Figures~\ref{subfig::feature_importances_scat_chan_rf_building} and~\ref{subfig::feature_importances_scat_chan_rf_facet}) more closely, we see how for both sets the image based modality importance falls in the range \SIrange{5}{10}{\percent} in most cases.
            There are two cases where this is not the case:
            \begin{itemize}[label=\(\blacktriangleright\)]
                \item \texttt{FOS}, on both sets, with an importance below \SI{1}{\percent}.
                        This is understandable as this error is of topological nature and is better detected with intrinsic features.
                        Indeed, for this error even height based features score less than \SI{1}{\percent} in importance ratio.
                \item \texttt{FIB}, on \textbf{Na-P13} with a ration around \num[fraction-function = \sfrac]{1/3}.
                        This can be explained by the fact that this modality was decisive in getting the best F-score possible (cf. Table~\ref{tab::rf_scat_bl_comparison}).
            \end{itemize}
            On another note, building typology is so important that, even with advanced extrinsic features, baseline geometric features have a very large importance ratio.

        \subsubsection{\texorpdfstring{\acrshort*{acr::svm}}{SVM} results.}
            \label{subsubsec::advanced_experiments::better_features::scatnet_baseline::svm}
            We run the same experiments and follow the same layout as in Section~\ref{subsubsec::advanced_experiments::better_features::scatnet_baseline::rf}.
            Results are reported in Table~\ref{tab::stats_scat_svm_f3}.
            The mean and standard deviation F-scores are shown in Figure~\ref{fig::f_score_svm_scat}.
            Just as with the \gls{acr::rf} experiments, external modalities seem to play a more crucial role in error prediction than with baseline features (cf. Figure~\ref{fig::f_score_svm_bl}).\\

            \begin{sidewaystable}[htpb]
                \footnotesize
                \centering
                \begin{tabular}{| c | c c | c c | c c | c c | c c | c c |}
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Elancourt}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{S(d)-All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & \textbf{97.67} & \textbf{86.44} & 97.07 & 81.99 & 95.12 & 86.43 & 97.15 & 83.16 & 91.67 & 89.71 & 93.32 & 86.56 \\
                    \hline
                    \texttt{BUS} & 32.27 & 86.85 & 89.81 & 46.48 & \textbf{60.08} & \textbf{92.18} & 89.60 & 47.42 & 30.79 & 90.63 & 89.41 & 44.42 \\
                    \hline
                    \texttt{BIB} & 97.02 & 52.27 & 89.60 & 44.80 & 98.51 & 25.68 & 97.03 & 30.53 & \textbf{91.13} & \textbf{90.69} & 94.06 & 71.43 \\
                    \hline
                    \texttt{BIT} & 100 & 73.88 & 91.84 & 38.96 & 98.98 & 82.20 & 94.90 & 46.5 & \textbf{100} & \textbf{100} & 93.88 & 49.46 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 53.88 & 99.71 & 70.22 & 87.42 & \textbf{94.17} & \textbf{97.04} & 78.46 & 88.05 & 54.59 & 99.72 & 69.98 & 83.96 \\
                    \hline
                    \texttt{FUS} & 96.49 & 52.24 & 96.82 & 40.92 & \textbf{95.59} & \textbf{62.26} & 94.27 & 58.85 & 98.09 & 51.85 & 97.13 & 60.52 \\
                    \hline
                    \texttt{FIB} & 33.77 & 74.03 & 31.58 & 74.23 & \textbf{89.04} & \textbf{81.53} & 87.72 & 81.97 & 17.98 & 89.13 & 18.42 & 89.36 \\
                    \hline
                    \texttt{FIT} & 100 & 88.24 & 100 & 88.24 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & \textbf{84.57} & \textbf{88.47} & 86.28 & 85.56 & 64.92 & 91.30 & 68.31 & 91.49 & 68.56 & 83.75 & 63.05 & 76.00 \\
                    \hline
                    \hline
                    \multicolumn{13}{|c|}{\textbf{Na-P13}}\\
                    \hline
                    &\multicolumn{2}{c|}{\textbf{Geom.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{S(d)-All}} & \multicolumn{2}{c|}{\textbf{Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{S(c)-All}}\\
                    \cline{2-13}
                    & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                    \hline
                    \texttt{BOS} & \textbf{44.86} & \textbf{54.09} & 44.65 & 53.71 & 29.98 & 42.69 & 29.98 & 42.69 & 29.98 & 42.69 & 29.98 & 42.69 \\
                    \hline
                    \texttt{BUS} & 98.46 & 27.35 & 97.69 & 25.76 & 93.08 & 29.30 & \textbf{93.07} & \textbf{34.28} & 37.40 & 14.04 & 71.54 & 27.84 \\
                    \hline
                    \texttt{BIB} & \textbf{82.35} & \textbf{17.31} & 79.74 & 16.55 & 70.59 & 13.55 & 70.59 & 13.55 & 70.59 & 13.53 & 70.59 & 13.53 \\
                    \hline
                    \texttt{BIT} & \textbf{95.74} & \textbf{30.93} & 95.21 & 30.76 & 53.49 & 17.63 & 52.66 & 17.71 & 50.26 & 16.67 & 50.26 & 16.67 \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.90 & 75.08 & 95.45 & 82.78 & \textbf{99.72} & \textbf{74.54} & 99.17 & 81.89 & 99.45 & 74.64 & 98.34 & 81.67 \\
                    \hline
                    \texttt{FUS} & \textbf{87.40} & \textbf{65.08} & 87.60 & 60.31 & 30.79 & 43.70 & 59.50 & 61.02 & 30.79 & 43.70 & 45.45 & 54.73 \\
                    \hline
                    \texttt{FIB} & \textbf{97.06} & \textbf{38.17} & 97.39 & 36.79 & 79.08 & 30.75 & 81.37 & 31.60 & 71.90 & 27.88 & 71.90 & 27.78 \\
                    \hline
                    \texttt{FIT} & 100 & 89.47 & 100 & 89.47 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                    \hline
                    \texttt{FIG} & 95.64 & 77.89 & 95.39 & 78.62 & 97.70 & 78.63 & \textbf{97.82} & \textbf{78.65} & 72.12 & 78.08 & 83.76 & 81.68 \\
                    \hline
                \end{tabular}
                \caption{
                    \label{tab::stats_scat_svm_f3}
                    \gls{acr::svm} applied to \gls{acr::scatnet} based features.
                    Results are expressed in percentage on the two datasets at \textbf{\gls{acr::efin}} level 3.
                }
            \end{sidewaystable}

            Based on Figure~\ref{fig::f_score_svm_scat}, we can compare the options of \gls{acr::scatnet} derived image based features based on the produced results.
            In constrast with \gls{acr::rf}, in most cases, it is this time the \texttt{deletion} option that yields the best results.
            The opposite occurs only three times: the \texttt{channel} option was best on \textbf{Elancourt} for \texttt{BOS} and \texttt{BIT} with a small margin, and for \texttt{BIB} with a jump of more that \SI{20}{\percent} in terms of F-score.
            As in the previous sub-subsection, we do not have any theory explaining why the last exception occurs.\\
    
            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/deletion/building}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_del_bl_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/deletion/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_del_bl_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip1em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/channel/building}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_chan_bl_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=6.5cm]{figures/results/scat_vs_bl/svm/channel/facet}
                        }{
                            \caption{
                                \label{subfig::f_score_svm_scat_chan_bl_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption{
                        \label{fig::f_score_svm_scat}
                        Mean F-score and standard deviation obtained with an \gls{acr::svm} based on \gls{acr::scatnet} features.
                    }
                }
            \end{figure}

            On the other hand, regarding the inversion of fusion scheme preference, there are two possible explanations:
            \begin{itemize}[label=\(\blacktriangleright\)]
                \item The analysis that was presented in Section~\ref{subsubsec::advanced_experiments::better_features::scatnet_baseline::rf} is false;
                \item The issue is rather with the \gls{acr::svm} that was not well parameterized.
            \end{itemize}
            The last reason seems to be more probable as the late fusion schemes induces larger feature vector dimensions ( 1085 more than the other option) which may have caused problems for the \gls{acr::svm}.
            In addition, this is consistent with the large drop in the performances of the \gls{acr::svm} classifier when baseline external features are added, as shown in Table~\ref{tab::svm_f3}.\\

            \begin{table}[htbp]
                \footnotesize 
                \centering
                \renewcommand{\arraystretch}{2}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{S(d)-Im.} & \cellcolor{LOSS1525} \textbf{Geom.} & \cellcolor{LOSS0515} \textbf{S(d)-Im.} & \cellcolor{GAIN1525} \textbf{S(d)-Im.} & \cellcolor{GAIN3545} \textbf{S(d)-Im.} & \cellcolor{GAIN3545} \textbf{S(d)-Im.} & \cellcolor{STBL} \textbf{S(d)-Im.} & \cellcolor{STBL} \\
                        \textbf{Na-P13} & \cellcolor{STBL} & \cellcolor{GAIN0515} \textbf{S(d)-All} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{GAIN3545} & \cellcolor{STBL} & \cellcolor{STBL} \textbf{S(d)-Im.} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::svm_scat_bl_comparison_del}
                        Comparison with \texttt{deletion} option.
                    }
                \end{subtable}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{STBL} \textbf{S-Hei.} & \cellcolor{STBL} \textbf{S(c)-Im.} & \cellcolor{STBL} \textbf{S(c)-Im.} & \cellcolor{STBL} & \cellcolor{GAIN3545} \textbf{S(c)-All} & \cellcolor{STBL} & \cellcolor{STBL} \textbf{S(c)-Im.} & \cellcolor{STBL} \textbf{Geom.} \\
                        \textbf{Na-P13} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{GAIN3545} & \cellcolor{STBL} & \cellcolor{STBL} \textbf{S(c)-Im.} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::svm_scat_bl_comparison_chan}
                        Comparison with \texttt{channel} option.
                    }
                \end{subtable}
                \renewcommand{\arraystretch}{1}
                \caption[
                    Evolution of the F-score value, for each error, with the \gls{acr::svm} classifier, using \gls{acr::scatnet} compared to baseline features.
                ]{
                    \label{tab::svm_scat_bl_comparison}
                    Evolution of the F-score value, for each error, with the \gls{acr::svm} classifier, using \gls{acr::scatnet} compared to baseline features.
                    Feature sets having a significant impact on the classification results are mentioned in the corresponding cell (cf. Table~\ref{tab::all_f-scores_scat_svm_f3}).
                    The color indicates the magnitude:
                    \textcolor{LOSS45}{\(\blacksquare\)}: (\SIrange[range-phrase={,  }]{-100}{-45}{\percent}]--
                    \textcolor{LOSS3545}{\(\blacksquare\)}: [\SIrange[range-phrase={,  }]{-45}{-35}{\percent})--
                    \textcolor{LOSS2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-15}{-5}{\percent}) --
                    \textcolor{STBL}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-5}{5}{\percent}) --
                    \textcolor{GAIN0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{5}{15}{\percent}) --
                    \textcolor{GAIN1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{15}{25}{\percent}) --
                    \textcolor{GAIN2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{25}{35}{\percent}) --
                    \textcolor{GAIN3545}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{35}{45}{\percent}) --
                    \textcolor{GAIN45}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{45}{100}{\percent}] --
                    When two null F-scores are compared, the cell is colored in white \(\square\): neither positive nor negative.
                }
            \end{table}

            The scores obtained using \gls{acr::svm} and \gls{acr::scatnet} features are compared to the baseline (cf. Table~\ref{tab::all_f-scores_scat_svm_f3}) with the same color scheme as earlier.
            These comparisons are summarized in Table~\ref{tab::svm_scat_bl_comparison}.
            On \texttt{Building errors}, \gls{acr::scatnet} with \texttt{deletion} yields better (\textit{resp.} worse and stable) results 2 (\textit{resp.} 4 and 2) times.
            On \texttt{Facet errors}, the same option yields better (\textit{resp.} worse and stable) results 4 (\textit{resp.} 6 and 0) times.
            In constrast, regarding \texttt{Building errors}, the \texttt{channel} option helps \gls{acr::scatnet} achieve, better (\textit{resp.} worse and stable) results 0 (\textit{resp.} 0 and 6) times.
            On \texttt{Facet errors}, the same option yields better (\textit{resp.} worse and stable) results 2 (\textit{resp.} 0 and 8) times.
            This confirms the fact that than \texttt{deletion} was, in general, the best option when using an \gls{acr::svm}.\\

            Just as with the \gls{acr::rf} classifier, height based features with \gls{acr::scatnet} help yield better results.
            This was the case for \texttt{BUS} on \textbf{Elancourt} with the \texttt{channel} option.
            This confirms once again the results fo earlier experiments where the same modality always proved to be important for predicting \texttt{BUS} on \textbf{Elancourt}.
            Regarding image based features, Table~\ref{tab::svm_scat_bl_comparison} shows that they are, as always, crucial for error detection.
            In the \gls{acr::svm} case, it was with the \texttt{deletion} option compared to \gls{acr::rf}, as discussed earlier in the previous paragraph.
            This was suspected to be due to the fact \gls{acr::svm} was not well parameterized, as was the case with baseline features in Section~\ref{sec::advanced_experiments::classifier}.\\

            Once again, feature importances are computed, using the \gls{acr::mkl} weights, and vizualized in Figure~\ref{fig::feature_importances_scat_svm}.
            There is little difference, this time, between both fusion schemes, eventhough thery produce different results.
            Geometric and image based features are the most important modalities with ratios around \SI{45}{\percent} in all sets and for both options.\\

            \begin{figure}[htpb]
                \centering
                \ffigbox[\textwidth]{
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/deletion/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_svm_building}
                                \texttt{Building errors} (with \texttt{deletion}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/deletion/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_del_svm_facet}
                                \texttt{Facet errors} (with \texttt{deletion}).
                            }
                        }
                    \end{subfloatrow}
                    \vskip2em
                    \begin{subfloatrow}[2]
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/channel/feature_importance/building}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_svm_building}
                                \texttt{Building errors} (with \texttt{channel}).
                            }
                        }
                        \ffigbox[.5\textwidth]{
                            \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/scat_vs_bl/svm/channel/feature_importance/facet}
                        }{
                            \caption{
                                \label{subfig::feature_importances_scat_chan_svm_facet}
                                \texttt{Facet errors} (with \texttt{channel}).
                            }
                        }
                    \end{subfloatrow}
                }{
                    \caption{
                        \label{fig::feature_importances_scat_svm} Modality contribution for the \gls{acr::svm} classifier based on the coefficients computed by EasyMKL.
                        Height and Image based features use the \gls{acr::scatnet} with both \texttt{deletion} and \texttt{channel} options.
                        The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                    }
                }
            \end{figure}

            Height based features, on the other hand, have a ratio of importance below \SI{10}{\percent}.
            Actually, in most cases, it is below \SI{4}{\percent}, except for \texttt{BIT}, \texttt{FOS} and \texttt{FIB} on \textbf{Elancourt}.
            This is different from baseline features, where height based features had comparable importance ratios to other modalities (\texttt{cf.} Figure~\ref{fig::feature_importances_svm_bl}).
            It seems to be contradictory with the fact that \gls{acr::scatnet} derived height features performed overall better than baseline features (cf. Tables~\ref{tab::all_f-scores_rf_f3} and~\ref{tab::all_f-scores_svm_f3}).
            It is, however, consistent with the findings of the scalability analysis in Section~\ref{sec::advanced_experiments::scalability}, where height based features proved to not be important.
            This further bolsters the fact that the \gls{acr::svm} was not well parameterized for the image based features.\\

        \subsubsection{\texorpdfstring{\acrshort*{acr::svm}}{SVM} compared to \texorpdfstring{\acrshort*{acr::rf}}{RF}.}
            \label{subsubsec::advanced_experiments::better_features::scatnet_baseline::svm_rf}
            Just as in the Section~\ref{sec::advanced_experiments::classifier}, we will compare preditction scores resulting from both classifiers.
            These are compiled in Table~\ref{tab::scat_rf_vs_svm_comparison}.

            \begin{table}[htbp]
                \footnotesize 
                \centering
                \renewcommand{\arraystretch}{2}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{GAIN0515} & \cellcolor{GAIN0515} & \cellcolor{GAIN3545} & \cellcolor{GAIN2535} & \cellcolor{STBL} & \cellcolor{GAIN45} & \cellcolor{GAIN45} & \cellcolor{GAIN45} & \cellcolor{STBL} \\
                        \textbf{Na-P13} & \cellcolor{LOSS0515} & \cellcolor{GAIN0515} & \cellcolor{GAIN1525} & \cellcolor{GAIN2535} & \cellcolor{LOSS0515} & \cellcolor{STBL} & \cellcolor{LOSS0515} & \cellcolor{GAIN45} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::svm_scat_bl_comparison_del}
                        Comparison with \texttt{deletion} option.
                    }
                \end{subtable}
                \begin{subtable}{\textwidth}
                    \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                        \hline
                        & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                        \hline
                        \textbf{Elancourt} & \cellcolor{GAIN0515} & \cellcolor{STBL} & \cellcolor{GAIN45} & \cellcolor{GAIN45} & \cellcolor{LOSS1525} & \cellcolor{GAIN45} & \cellcolor{GAIN2535} & \cellcolor{GAIN45} & \cellcolor{STBL} \\
                        \textbf{Na-P13} & \cellcolor{LOSS0515} & \cellcolor{STBL} & \cellcolor{GAIN1525} & \cellcolor{GAIN2535} & \cellcolor{LOSS0515} & \cellcolor{STBL} & \cellcolor{LOSS1525} & \cellcolor{GAIN45} & \cellcolor{STBL} \\
                        \hline
                    \end{tabular}
                    \caption{
                        \label{subtab::svm_scat_bl_comparison_chan}
                        Comparison with \texttt{channel} option.
                    }
                \end{subtable}
                \renewcommand{\arraystretch}{1}
                \caption[
                    Evolution of the F-score value, for each error using \gls{acr::svm} compared to \gls{acr::rf} and based on \gls{acr::scatnet} features.
                ]{
                    \label{tab::scat_rf_vs_svm_comparison}
                    Evolution of the F-score value, for each error using \gls{acr::svm} compared to \gls{acr::rf} and based on \gls{acr::scatnet} features.
                    The color indicates the magnitude:
                    \textcolor{LOSS45}{\(\blacksquare\)}: (\SIrange[range-phrase={,  }]{-100}{-45}{\percent}]--
                    \textcolor{LOSS3545}{\(\blacksquare\)}: [\SIrange[range-phrase={,  }]{-45}{-35}{\percent})--
                    \textcolor{LOSS2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-35}{-25}{\percent}) --
                    \textcolor{LOSS0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-15}{-5}{\percent}) --
                    \textcolor{STBL}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{-5}{5}{\percent}) --
                    \textcolor{GAIN0515}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{5}{15}{\percent}) --
                    \textcolor{GAIN1525}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{15}{25}{\percent}) --
                    \textcolor{GAIN2535}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{25}{35}{\percent}) --
                    \textcolor{GAIN3545}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{35}{45}{\percent}) --
                    \textcolor{GAIN45}{\(\blacksquare\)}: [\SIrange[range-phrase={, }]{45}{100}{\percent}] --
                    When two null F-scores are compared, the cell is colored in white \(\square\): neither positive nor negative.
                }
            \end{table}

            On \textbf{Elancourt}, the \gls{acr::svm} classifier yields better results than the \gls{acr::rf} one, especially with the \texttt{deletion} option.
            On \textbf{Na-P13}, the \gls{acr::svm} classifier is not always better, but \texttt{deletion} remains the best option.
            On both sets, using \gls{acr::scatnet} derived features helped the \gls{acr::svm} score better than \gls{acr::rf} compared to the baseline features.
            Hereafter, in Table~\ref{tab::best_scat_bl_svm_rf}, based on this comparison, as well as, the ones in Sections~\ref{subsubsec::advanced_experiments::better_features::scatnet_baseline::rf} and~\ref{subsubsec::advanced_experiments::better_features::scatnet_baseline::svm}, we can deduce the best F-score, feature configuration and classifier, per error label.\\

            \begin{table}[htpb]
                \footnotesize
                \centering
                \begin{tabular}{c c c c}
                    \toprule
                    & \multicolumn{3}{c}{\textbf{Elancourt}}\\
                    \midrule
                    & \(\bm{F_{score}}\) & Feature configuration & Classifier \\
                    \midrule
                    \texttt{BOS} & 91.71 & \textbf{Geom.} & \gls{acr::svm} \\
                    \midrule
                    \texttt{BUS} & 72.75 & \textbf{Geom. \(\oplus\) S(d)-Im.} & \gls{acr::svm} \\
                    \midrule
                    \texttt{BIB} & 90.91 & \textbf{Geom. \(\oplus\) S(c)-Im.} & \gls{acr::svm} \\
                    \midrule
                    \texttt{BIT} & 100 & \makecell{\textbf{Geom. \(\oplus\) Hei.}\\ \textbf{Geom. \(\oplus\) Im.}\\ \textbf{All}\\ \textbf{Geom. \(\oplus\) S(c)-Im.}} & \gls{acr::svm} \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 99.46 & \textbf{S(c)-All} & \gls{acr::rf} \\
                    \midrule
                    \texttt{FUS} & 75.41 & \textbf{Geom. \(\oplus\) S(d)-Im.} & \gls{acr::svm} \\
                    \midrule
                    \texttt{FIB} & 85.12 & \textbf{Geom. \(\oplus\) S(d)-Im.} & \gls{acr::svm} \\
                    \midrule
                    \texttt{FIT} & 100 & \makecell{\textbf{Geom. \(\oplus\) Hei.}\\ \textbf{Geom. \(\oplus\) Im.}\\ \textbf{All}\\ \textbf{Geom. \(\oplus\) S(d)-Im.}\\ \textbf{S(d)-All}\\ \textbf{Geom. \(\oplus\) S(c)-Im.}\\ \textbf{S(c)-All}} & \gls{acr::svm}\\
                    \midrule
                    \texttt{FIG} & 86.48 & \textbf{Geom.} & \gls{acr::svm} \\
                    \bottomrule
                    \toprule
                    & \multicolumn{3}{c}{\textbf{Na-P13}}\\
                    \midrule
                    & \(\bm{F_{score}}\) & Feature configuration & Classifier \\
                    \midrule
                    \texttt{BOS} & 62.44 & \textbf{Geom.} & \gls{acr::rf} \\
                    \midrule
                    \texttt{BUS} & 52.46 & \textbf{Geom. \(\oplus\) Im.} & \gls{acr::rf} \\
                    \midrule
                    \texttt{BIB} & 28.61 & \textbf{Geom.} & \gls{acr::svm} \\
                    \midrule
                    \texttt{BIT} & 46.76 & \textbf{Geom.} & \gls{acr::svm} \\
                    \specialrule{.2em}{.1em}{.1em}
                    \texttt{FOS} & 98.69 & \textbf{All.} & \gls{acr::rf} \\
                    \midrule
                    \texttt{FUS} & 74.61 & \textbf{Geom.} & \gls{acr::svm} \\
                    \midrule
                    \texttt{FIB} & 76.58 & \textbf{S(c)-All.} & \gls{acr::rf} \\
                    \midrule
                    \texttt{FIT} & 100 & \makecell{\textbf{Geom. \(\oplus\) Hei.}\\ \textbf{Geom. \(\oplus\) Im.}\\ \textbf{All}\\ \textbf{Geom. \(\oplus\) S(d)-Im.}\\ \textbf{S(d)-All}\\ \textbf{Geom. \(\oplus\) S(c)-Im.}\\ \textbf{S(c)-All}} & \gls{acr::svm}\\
                    \midrule
                    \texttt{FIG} & 90.99 & \textbf{S(c)-All} & \gls{acr::rf} \\
                    \bottomrule
                \end{tabular}
                \caption{
                    \label{tab::best_scat_bl_svm_rf}
                    For each set and each error label, we report the best F-score as well as feature configurations and classifiers.
                }
            \end{table}

            We can observe that for \textbf{Elancourt} the \gls{acr::svm} classifier yields always the best results, with the exception of \texttt{FOS}.
            This exception could be explained easily by the fact that \texttt{FOS} is very frequent, as discussed in Section~\ref{subsec::advanced_experiments::classifier::svm_rf}.
            Moreover, on this set, most error labels achieve an F-score is over \SI{85}{\percent}, excluding \texttt{BUS} and \texttt{FUS} with at least \SI{70}{\percent}.
            Regarding feature configurations, the \gls{acr::scatnet} derived ones are always the best with two exceptions: \texttt{BOS} and \texttt{FIG}.
            In these two cases, geometric baseline features yield slightly better results than the more advanced \gls{acr::scatnet} based ones.\\

            On \textbf{Na-P13}, results are not as positive as in the other set.
            Although for \texttt{Facet errors}, F-scores are as good\footnote{
                Over \SI{74}{\percent}, for \texttt{FUS} and \texttt{FIB}, and \SI{90}{\percent} for the rest.
            } as in \textbf{Elancourt}, they are poor for \texttt{Buiding errors}.
            Besides, advanced features did not yield the best results for most cases (6 over 9 labels).
            Added to that the fact that the \gls{acr::svm} classifier was only the best for rare errors.\\

            Overall, we can see how \texttt{Building errors} are best learned on \textbf{Elancourt}.
            This is in accord with what we have seen in Section~\ref{sec::experiments::baseline_feature_analysis},~\ref{sec::advanced_experiments::scalability} and~\ref{sec::advanced_experiments::classifier}.
            On the other hand, for \texttt{Facet errors}, according to the same sections, we would expect \textbf{Na-P13} to be the best alternative.
            However, there is one exception as \texttt{FIB} yield a better F-score on \textbf{Elancourt} than on \textbf{Na-P13}.
            We do not have any explanation to why this happens other than the fact that the \gls{acr::svm} was not better parameterized.

    \subsection{Graph kernels to baseline comparison}
        \label{subsec::advanced_experiments::better_features::graph_kernel_baseline}
        Herein, we aim at assessing the added value of graph kernels in error classification, compared to the baseline.
        This means that we have only one feature configuration to experiment with.
        We only use the \gls{acr::svm} which naturally takes kernels into account.
        Results are reported in Table~\ref{tab::stats_gk_svm_f3} and visualized in Figure~\ref{fig::f_score_svm_gk}.\\

        \begin{table}[htpb]
            \footnotesize
            \centering
            \begin{tabular}{| c | c c c | c c c |}
                \hline
                \multicolumn{7}{|c|}{\textbf{Elancourt}}\\
                \hline
                & \multicolumn{3}{c|}{\textbf{Geom.}} & \multicolumn{3}{c|}{\textbf{K-Geom.}} \\
                \cline{2-7}
                & \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) \\
                \hline
                \texttt{BOS} & 97.67 & 86.44 & \textbf{91.71} & 87.99 & 86.11 & 87.04 \\
                \hline
                \texttt{BUS} & 32.27 & 86.85 & 47.06 & 92.99 & 51.29 & \textbf{66.11} \\
                \hline
                \texttt{BIB} & 97.02 & 52.27 & \textbf{67.94} & 72.28 & 60.08 & 65.62 \\
                \hline
                \texttt{BIT} & 100 & 73.88 & \textbf{84.98} & 96.94 & 41.67 & 58.29 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 53.88 & 99.71 & 69.96 & 97.12 & 99.60 & \textbf{98.34} \\
                \hline
                \texttt{FUS} & 96.49 & 52.24 & \textbf{67.78} & 84.39 & 30.18 & 44.46 \\
                \hline
                \texttt{FIB} & 33.77 & 74.03 & 46.38 & 94.74 & 32.34 & \textbf{48.22} \\
                \hline
                \texttt{FIT} & 100 & 88.24 & 93.75 & 100 & 100 & \textbf{100} \\
                \hline
                \texttt{FIG} & 84.57 & 88.47 & \textbf{86.48} & 69.66 & 80.91 & 74.86 \\
                \hline
                \hline
                \multicolumn{7}{|c|}{\textbf{Na-P13}}\\
                \hline
                & \multicolumn{3}{c|}{\textbf{Geom.}} & \multicolumn{3}{c|}{\textbf{K-Geom.}} \\
                \cline{2-7}
                & \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) & \(\bm{F_{score}}\) \\
                \hline
                \texttt{BOS} & 44.86 & 54.09 & 49.04 & 43.42 & 61.34 & \textbf{50.85} \\
                \hline
                \texttt{BUS} & 98.46 & 27.35 & 42.81 & 86.15 & 31.73 & \textbf{46.38} \\
                \hline
                \texttt{BIB} & 82.35 & 17.31 & 28.61 & 75.16 & 31.94 & \textbf{44.83} \\
                \hline
                \texttt{BIT} & 95.74 & 30.93 & 46.76 & 87.23 & 32.67 & \textbf{47.54} \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.90 & 75.08 & 85.36 & 95.45 & 98.86 & \textbf{97.13} \\
                \hline
                \texttt{FUS} & 87.40 & 65.08 & \textbf{74.61} & 79.34 & 67.13 & 72.73 \\
                \hline
                \texttt{FIB} & 97.06 & 38.17 & 54.79 & 92.16 & 53.71 & \textbf{67.87} \\
                \hline
                \texttt{FIT} & 100 & 89.47 & 94.44 & 100 & 100 & \textbf{100} \\
                \hline
                \texttt{FIG} & 95.64 & 77.89 & \textbf{85.86} & 80.73 & 89.40 & 84.84 \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::stats_gk_svm_f3}
                Comparison between the baseline geometric features and graph kernels using \gls{acr::svm}.
                Results, expressed in percentage, on the two datasets at \textbf{\gls{acr::efin}} level 3.
            }
        \end{table}
        \begin{figure}[htpb]
            \centering
            \ffigbox[\FBwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk_vs_bl/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_building}
                            \texttt{Building errors.}
                        }
                    }
                    \ffigbox[\FBwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk_vs_bl/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_facet}
                            \texttt{Facet errors.}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_svm_gk}
                    F-score obtained with an \gls{acr::svm} using graph kernels features.
                }
            }
        \end{figure}

        As in the previous subsection, we draw comparisons between the graph kernel and baseline results.
        These are summarized in Table~\ref{tab::gk_svm_comparison}.\\

        \begin{table}[htbp]
            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                \hline
                & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                \hline
                \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{GAIN1525} & \cellcolor{STBL} & \cellcolor{LOSS2535} & \cellcolor{GAIN2535} & \cellcolor{LOSS1525} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{LOSS0515} \\
                \textbf{Na-P13} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{GAIN1525} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{GAIN0515} & \cellcolor{STBL} \\
                \hline
            \end{tabular}
            \renewcommand{\arraystretch}{1}
            \caption[
                Evolution of the F-score value, for each error using on graph kernels compared to baseline features.
            ]{
                \label{tab::gk_svm_comparison}
                Evolution of the F-score value, for each error using on graph kernels compared to baseline features.
                The used color scheme is presented in figure~\ref{fig::comparison_bar}.
            }
        \end{table}

        We can see that on \textbf{Na-P13}, grapĥ kernels yield better scores than baseline features for \texttt{BIB}, \texttt{FOS}, \texttt{FIB} and \texttt{FIG}, while they are stable on the rest.
        On \textbf{Elancourt}, it is a different situation.
        In fact, while it is, same as on the other set, better on \texttt{BOS} and \texttt{FIT}, the F-score remains stable for \texttt{BIB} and \texttt{FIB}.
        Moreover, it is better on \texttt{BUS} but worse on \texttt{BIT}, \texttt{FUS} and \texttt{FIG}.\\

        These discrepancies could be explained by looking at each set composition.
        Actually, \texttt{FIT} is a purely topological error label in nature.
        This justifies why advanced geometric features which take into account the structure of the building models, are better in prediction.
        The same could be said about \texttt{FOS}, which explains why both these labels benefited from the usage of graph kernels.
        However, it was not the case of \texttt{BIT} which is contradictory with the previous explanation.\\

        The second factor that should be taken into account is the frequency of the label.
        The more the latter is large, the larger the impact of graph kernels.
        In fact, this, added to the statistics shown in Figure~\ref{fig::error_fused_statistics}, can explain the change in prediction scores for each error:
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item \texttt{BUS} was better on \textbf{Elancourt};
            \item \texttt{BIB} was better on \textbf{Na-P13};
            \item \texttt{BIT} was worse on \textbf{Elancourt};
            \item \texttt{FOS} was better on \textbf{Elancourt};
            \item \texttt{FUS} was worse on \textbf{Elancourt};
            \item \texttt{FIB} was better on \textbf{Na-P13};
            \item \texttt{FIT} was as better on \textbf{Elancourt} as on \textbf{Na-P13};
            \item \texttt{FIG} was as worse on \textbf{Elancourt}.
        \end{itemize}
        ~\\

        Based on these comparisons, and the Table~\ref{tab::best_scat_bl_svm_rf}, we can try to anticipate the changes that will occur when using both graph kernels and \gls{acr::scatnet} features.
        In fact, baseline geometric features were the best configuration for some error labels.
        Improving on these with graph kernels, we can only expect better scores for \texttt{BIB} and \texttt{FIT} on \textbf{Na-P13}.
        However, on the rest, we cannot predict how these features will interact with the other modalities.

    \subsection{Graph kernels and \texorpdfstring{\acrshort*{acr::scatnet}}{ScatNet} to baseline comparison}
        \label{subsec::advanced_experiments::better_features::graph_kernel_scatnet_baseline}
        Now that we have studied the contribution of each of the proposed advanced features, these are combined in order to assess their impact on the prediction results.
        As in the previous subsection, using graph kernels forces us to abandon \glspl{acr::rf} and to experiment only with \glspl{acr::svm}.
        Results are reported in Table~\ref{tab::stats_gk_scat_svm_f3}.\\
        
        \begin{sidewaystable}[htpb]
            \footnotesize
            \begin{tabular}{| c | c c | c c | c c | c c | c c | c c |}
                \hline
                \multicolumn{13}{|c|}{\textbf{Elancourt}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{K-Geom.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(d)-All}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(c)-All}}\\
                \cline{2-13}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & 87.99 & 86.11 & 87.16 & 86.19 & 88.29 & 86.85 & 87.84 & 86.80 & \textbf{88.66} & \textbf{86.58} & 88.30 & 86.54 \\
                \hline
                \texttt{BUS} & 92.99 & 51.29 & \textbf{93.42} & \textbf{53.27} & 93.84 & 49.50 & 93.86 & 50.92 & 93.63 & 49.94 & 93.63 & 51.04 \\
                \hline
                \texttt{BIB} & 72.28 & 60.08 & 73.40 & 59.36 & 70.94 & 61.80 & 72.77 & 61.51 & 72.27 & 61.86 & \textbf{73.76} & \textbf{61.83} \\
                \hline
                \texttt{BIT} & 96.94 & 41.67 & 96.94 & 41.67 & \textbf{97.96} & \textbf{42.86} & 96.94 & 42.60 & 96.94 & 42.22 & 95.96 & 42.41 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & \textbf{97.12} & \textbf{99.60} & \textbf{97.12} & \textbf{99.60} & \textbf{97.12} & \textbf{99.60} & \textbf{97.12} & \textbf{99.60} & 97.05 & 99.60 & 97.05 & 99.60 \\
                \hline
                \texttt{FUS} & 84.39 & 30.18 & 84.39 & 30.08 & \textbf{85.03} & \textbf{30.83} & 84.71 & 30.75 & 84.71 & 30.61 & 84.71 & 30.61 \\
                \hline
                \texttt{FIB} & 94.74 & 32.34 & 94.32 & 32.24 & \textbf{96.49} & \textbf{32.31} & 96.05 & 32.06 & 96.49 & 32.26 & 96.51 & 32.12 \\
                \hline
                \texttt{FIT} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & 69.66 & 80.91 & 69.92 & 81.28 & 70.93 & 80.64 & 71.10 & 80.91 & 71.19 & 80.92 & \textbf{71.44} & \textbf{81.06} \\
                \hline
                \hline
                \multicolumn{13}{|c|}{\textbf{Na-P13}}\\
                \hline
                &\multicolumn{2}{c|}{\textbf{K-Geom.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S-Hei.}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(d)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(d)-All}} & \multicolumn{2}{c|}{\textbf{K-Geom. \(\oplus\) S(c)-Im.}} & \multicolumn{2}{c|}{\textbf{K-S(c)-All}}\\
                \cline{2-13}
                & \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) &  \(\bm{Rec}\) & \(\bm{Prec}\) \\
                \hline
                \texttt{BOS} & \textbf{43.42} & \textbf{61.34} & 43.42 & 60.81 & 38.89 & 56.59 & 38.89 & 56.59 & 38.89 & 56.93 & 38.89 & 56.59 \\
                \hline
                \texttt{BUS} & 86.15 & 31.73 & 85.38 & 31.36 & 87.69 & 32.11 & 87.69 & 32.11 & \textbf{87.02} & \textbf{32.39} & 87.69 & 32.11 \\
                \hline
                \texttt{BIB} & 75.16 & 31.94 & 75.16 & 31.86 & 79.74 & 33.80 & 79.74 & 33.80 & \textbf{79.74} & \textbf{33.89} & 79.74 & 33.80 \\
                \hline
                \texttt{BIT} & 87.23 & 32.67 & 87.23 & 32.48 & 89.89 & 33.40 & 89.89 & 33.40 & \textbf{89.89} & \textbf{33.53} & 89.89 & 33.40 \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 95.45 & 98.86 & \textbf{96.59} & \textbf{98.86} & 95.86 & 98.86 & 95.86 & 98.86 & 95.59 & 98.86 & 95.86 & 98.86 \\
                \hline
                \texttt{FUS} & 79.34 & 67.13 & 79.55 & 66.61 & 80.79 & 66.61 & 80.79 & 66.61 & \textbf{80.79} & \textbf{66.95} & 80.79 & 66.61 \\
                \hline
                \texttt{FIB} & \textbf{92.16} & \textbf{53.71} & 92.16 & 53.51 & 92.81 & 53.18 & 92.81 & 53.18 & 92.51 & 53.48 & 92.81 & 53.18 \\
                \hline
                \texttt{FIT} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} \\
                \hline
                \texttt{FIG} & 80.73 & 89.40 & 81.33 & 89.35 & \textbf{83.03} & \textbf{89.19} & \textbf{83.03} & \textbf{89.19} & 81.94 & 89.18 & \textbf{83.03} & \textbf{89.19} \\
                \hline
            \end{tabular}
            \caption{
                \label{tab::stats_gk_scat_svm_f3}
                \gls{acr::svm} results using graph kernels and \glspl{acr::scatnet}, expressed in percentage, on the two datasets at \textbf{\gls{acr::efin}} level 3.
            }
        \end{sidewaystable}

        In figure~\ref{fig::f_score_svm_gk_scat}, we can see how standard deviations are very low\footnote{
            It is under \SI{1}{\percent}, in most cases, and \SI{3}{\percent}, in the worst case.
        } for all errors and on both sets.
        The only exception is when image based features with \gls{acr::scatnet} are added on \textbf{Na-P13} and the F-score imporves by almost \SI{4.5}{\percent} for \texttt{BIB}.
        This is consistent with the design of the image based features as well as the previous experimental results.\\

        \begin{figure}[htpb]
            \centering
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/deletion/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_del_bl_building}
                            \texttt{Building errors} (with \texttt{deletion}).
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/deletion/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_del_bl_facet}
                            \texttt{Facet errors (with \texttt{deletion}).}
                        }
                    }
                \end{subfloatrow}
                \vskip1em
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/channel/building}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_chan_bl_building}
                            \texttt{Building errors (with \texttt{channel}).}
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=6.5cm]{figures/results/gk-scat_vs_bl/channel/facet}
                    }{
                        \caption{
                            \label{subfig::f_score_svm_gk_scat_chan_bl_facet}
                            \texttt{Facet errors (with \texttt{channel}).}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption{
                    \label{fig::f_score_svm_gk_scat}
                    Mean F-score and standard deviation obtained with an \gls{acr::svm}.
                    The geometric modality is based on graph kernels while height and image based features use the \gls{acr::scatnet} with \texttt{deletion} and \texttt{channel} options.
                }
            }
        \end{figure}

        Since graph kernels did not prove to always be better than the baseline features, this situation could not be explained by the fact that geometric features with graph kernels were sufficient enough, as in Section~\ref{sec::experiments::baseline_feature_analysis}.
        Indeed, \texttt{FIB} proved to be stable when changing the baseline features for the graph kernels.
        However, when the latter was put together with \gls{acr::scatnet} features it yielded a worse F-score compared the \textbf{Geom. \(\oplus\) S(d)-Im.} configuration.\\

        Table~\ref{tab::gk_scat_svm_comparison} compiles comparisons, for both options, sets and all labels, between graph kernel and baseline geometric features when used with \gls{acr::scatnet} features.
        We can see how, no matter the chosen option, the set nor the label, follow tightly the evolution of \textbf{K-Geom.}.\\

        \begin{table}[htbp]
            \footnotesize 
            \centering
            \renewcommand{\arraystretch}{2}
            \begin{subtable}{\textwidth}
                \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                    \hline
                    & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                    \hline
                    \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{LOSS0515} & \cellcolor{LOSS1525} & \cellcolor{LOSS3545} & \cellcolor{STBL} & \cellcolor{LOSS2535} & \cellcolor{LOSS3545} & \cellcolor{STBL} & \cellcolor{LOSS0515} \\
                    \textbf{Na-P13} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{S(d)-Im.} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{STBL} & \cellcolor{STBL} \\
                    \hline
                \end{tabular}
                \caption{
                    \label{subtab::gk_scat_svm_comparison_del}
                    Comparison with \texttt{deletion} option.
                }
            \end{subtable}
            \begin{subtable}{\textwidth}
                \begin{tabular}{| c | x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} x{1.1cm} |}
                    \hline
                    & \texttt{BOS} & \texttt{BUS} & \texttt{BIB} & \texttt{BIT} & \texttt{FOS} & \texttt{FUS} & \texttt{FIB} & \texttt{FIT} & \texttt{FIG}\\
                    \hline
                    \textbf{Elancourt} & \cellcolor{STBL} & \cellcolor{LOSS0515} & \cellcolor{LOSS1525} & \cellcolor{LOSS3545} & \cellcolor{GAIN1525} & \cellcolor{LOSS2535} & \cellcolor{LOSS3545} & \cellcolor{STBL} & \cellcolor{LOSS0515} \\
                    \textbf{Na-P13} & \cellcolor{STBL} & \cellcolor{STBL} & \cellcolor{GAIN1525} \textbf{S(c)-Im.} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{STBL} & \cellcolor{GAIN0515} & \cellcolor{STBL} & \cellcolor{STBL} \\
                    \hline
                \end{tabular}
                \caption{
                    \label{subtab::gk_scat_svm_comparison_chan}
                    Comparison with \texttt{channel} option.
                }
            \end{subtable}
            \renewcommand{\arraystretch}{1}
            \caption[
                Evolution of the F-score value, for each error using on graph kernels and \gls{acr::scatnet} compared to when \gls{acr::scatnet} was used with geometric baseline features.
            ]{
                \label{tab::gk_scat_svm_comparison}
                Evolution of the F-score value, for each error using on graph kernels and \gls{acr::scatnet} compared to when \gls{acr::scatnet} was used with geometric baseline features.
                Feature sets having a significant impact on the classification results are mentioned in the corresponding cell.
                The used color scheme is presented in figure~\ref{fig::comparison_bar}.
            }
        \end{table}

        There are two reasons that can explain this situation.
        First, as seen before, the \gls{acr::svm} classifier was not well parameterized for the \gls{acr::scatnet} derived features.
        Most importantly, the second explanation involves the high number of kernel used for geometric features compared to one for each extrinsic feature.
        In fact, when adding up the importance ratio of all graph kernels, the intrinsic feature ratio was always over \SI{80}{\percent}.
        This explains very well how geometric features faded out the contributions of the other modalities.\\

        \begin{figure}[htpb]
            \centering
            \ffigbox[\textwidth]{
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/deletion/feature_importance/building}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_del_svm_building}
                            \texttt{Building errors} (with \texttt{deletion}).
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/deletion/feature_importance/facet}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_del_svm_facet}
                            \texttt{Facet errors (with \texttt{deletion}).}
                        }
                    }
                \end{subfloatrow}
                \begin{subfloatrow}[2]
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/channel/feature_importance/building}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_chan_svm_building}
                            \texttt{Building errors (with \texttt{channel}).}
                        }
                    }
                    \ffigbox[.5\textwidth]{
                        \includestandalone[mode=buildnew, height=.23\textheight]{figures/results/gk-scat_vs_bl/channel/feature_importance/facet}
                    }{
                        \caption{
                            \label{subfig::feature_importances_gk_scat_chan_svm_facet}
                            \texttt{Facet errors (with \texttt{channel}).}
                        }
                    }
                \end{subfloatrow}
            }{
                \caption[
                    Normalized modality importance for the \gls{acr::svm} classifier using graph kernels and \gls{acr::scatnet}.
                ]{
                    \label{fig::feature_importances_gk_scat_svm}
                    Normalized modality importance for the \gls{acr::svm} classifier based on the coefficients computed by EasyMKL.
                    The geometric modality uses on graph kernels while height and image based features use the \gls{acr::scatnet} with \texttt{deletion} and \texttt{channel} options.
                    The first (\textit{resp.} second) column represents \textbf{Elancourt} (\textit{resp.} \textbf{Na-P13}).
                }
            }
        \end{figure}

        In Figure~\ref{fig::feature_importances_gk_scat_svm}, we illustrated the normalized (in the number of graph kernels) \gls{acr::mkl} weights.
        There is no notable difference between the two options.
        Once again, as with \gls{acr::scatnet} features and baseline geometric features, the height based modality is not as important as the others.\\

        Based on this study, we can update Table~\ref{tab::best_scat_bl_svm_rf} based on the recent results.
        The graph kernel based feature combinations were helpful twice.
        Both cases where on \textbf{Na-P13} as shown in Table~\ref{tab::best_scat_gk_bl_svm_rf}.

        \begin{table}[htpb]
            \footnotesize
            \centering
            \begin{tabular}{c c c c}
                \toprule
                & \multicolumn{3}{c}{\textbf{Elancourt}}\\
                \midrule
                & \(\bm{F_{score}}\) & Feature configuration & Classifier \\
                \midrule
                \texttt{BOS} & 91.71 & \textbf{Geom.} & \gls{acr::svm} \\
                \midrule
                \texttt{BUS} & 72.75 & \textbf{Geom. \(\oplus\) S(d)-Im.} & \gls{acr::svm} \\
                \midrule
                \texttt{BIB} & 90.91 & \textbf{Geom. \(\oplus\) S(c)-Im.} & \gls{acr::svm} \\
                \midrule
                \texttt{BIT} & 100 & \makecell{\textbf{Geom. \(\oplus\) Hei.}\\ \textbf{Geom. \(\oplus\) Im.}\\ \textbf{All}\\ \textbf{Geom. \(\oplus\) S(c)-Im.}} & \gls{acr::svm} \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 99.46 & \textbf{S(c)-All} & \gls{acr::rf} \\
                \midrule
                \texttt{FUS} & 75.41 & \textbf{Geom. \(\oplus\) S(d)-Im.} & \gls{acr::svm} \\
                \midrule
                \texttt{FIB} & 85.12 & \textbf{Geom. \(\oplus\) S(d)-Im.} & \gls{acr::svm} \\
                \midrule
                \texttt{FIT} & 100 & \makecell{\textbf{Geom. \(\oplus\) Hei.}\\ \textbf{Geom. \(\oplus\) Im.}\\ \textbf{All}\\ \textbf{Geom. \(\oplus\) S(d)-Im.}\\ \textbf{S(d)-All}\\ \textbf{Geom. \(\oplus\) S(c)-Im.}\\ \textbf{S(c)-All}\\ \textbf{K-Geom.}\\ \textbf{K-Geom. \(\oplus\) S-Hei.}\\ \textbf{K-Geom. \(\oplus\) S(d)-Im.}\\ \textbf{K-S(d)-All}\\ \textbf{K-Geom. \(\oplus\) S(c)-Im.}\\ \textbf{K-S(c)-All}} & \gls{acr::svm}\\
                \midrule
                \texttt{FIG} & 86.48 & \textbf{Geom.} & \gls{acr::svm} \\
                \bottomrule
                \toprule
                & \multicolumn{3}{c}{\textbf{Na-P13}}\\
                \midrule
                & \(\bm{F_{score}}\) & Feature configuration & Classifier \\
                \midrule
                \texttt{BOS} & 62.44 & \textbf{Geom.} & \gls{acr::rf} \\
                \midrule
                \texttt{BUS} & 52.46 & \textbf{Geom. \(\oplus\) Im.} & \gls{acr::rf} \\
                \midrule
                \texttt{BIB} & 47.56 & \textbf{K-Geom. \(\oplus\) S(c)-Im.} & \gls{acr::svm} \\
                \midrule
                \texttt{BIT} & 48.84 & \textbf{K-Geom. \(\oplus\) S(c)-Im.} & \gls{acr::svm} \\
                \specialrule{.2em}{.1em}{.1em}
                \texttt{FOS} & 98.69 & \textbf{All.} & \gls{acr::rf} \\
                \midrule
                \texttt{FUS} & 74.61 & \textbf{Geom.} & \gls{acr::svm} \\
                \midrule
                \texttt{FIB} & 76.58 & \textbf{S(c)-All.} & \gls{acr::rf} \\
                \midrule
                \texttt{FIT} & 100 & \makecell{\textbf{Geom. \(\oplus\) Hei.}\\ \textbf{Geom. \(\oplus\) Im.}\\ \textbf{All}\\ \textbf{Geom. \(\oplus\) S(d)-Im.}\\ \textbf{S(d)-All}\\ \textbf{Geom. \(\oplus\) S(c)-Im.}\\ \textbf{S(c)-All}\\ \textbf{K-Geom.}\\ \textbf{K-Geom. \(\oplus\) S-Hei.}\\ \textbf{K-Geom. \(\oplus\) S(d)-Im.}\\ \textbf{K-S(d)-All}\\ \textbf{K-Geom. \(\oplus\) S(c)-Im.}\\ \textbf{K-S(c)-All}} & \gls{acr::svm}\\
                \midrule
                \texttt{FIG} & 90.99 & \textbf{S(c)-All} & \gls{acr::rf} \\
                \bottomrule
            \end{tabular}
            \caption{
                \label{tab::best_scat_gk_bl_svm_rf}
                For each set and each error label, we report the best F-score as well as feature configurations and classifiers.
            }
        \end{table}

        \FloatBarrier
    \subsection{Summary}
        \label{subsec::advanced_experiments::better_features::summary}
        In this section, we experimentally examined the added value of the advanced features that we proposed:
        We have seen how:
        \begin{itemize}[label=\(\blacktriangleright\)]
            \item For both sets, \gls{acr::scatnet} proved to be more helpful than baseline features in predicting errors; 
            \item In contrast to the baseline version, height based features with \gls{acr::scatnet} proved to be crucial for detecting error labels;
            \item The \texttt{channel} option was best when used with an \gls{acr::rf}, while \texttt{deletion} worked best with the \gls{acr::svm} classifier;
            \item Using \gls{acr::scatnet} derived features, \gls{acr::svm} proved to be better overall, especially on \textbf{Elancourt}, than using an \gls{acr::rf};
            \item Once again, the \gls{acr::svm} proved to be not well parameterized to take full advantage of the extrinsic features;
            \item Graph kernels, compared to baseline features, improve prediction results of labels provided they are frequent enough;
            \item Although being helpful, height based features have, by a large margin, the least importance ratio compared to the other modalities for both classifiers;
            \item Graph kernels, outnumbering the kernels for extrinsic modalities, have reduced the potency of the \gls{acr::scatnet} derived features;
            \item \textbf{Elancourt} the easiest to learn on with a minimum F-score of \SI{72}{\percent} and \SI{85}{\percent} on 7 out of 9 of the error labels;
            \item \texttt{Building errors} proved again to yield worse scores on \textbf{Na-P13}.
        \end{itemize}

