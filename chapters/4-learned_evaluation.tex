\minitoc

\vfill

In the previous chapter (Chapter~\ref{chap::semantic_evaluation}), we developed a hierarchical and modular taxonomy of errors for the overhead imagery modeling case.
Based on this taxonomy, and depending on the particular needs, specific error labels are extracted and considered during the quality evaluation step.\\

In this chapter, we present the second part of our proposed approach, as we cast the problem as a supervised classification one.
Issues related to the latter are detailed in Section~\ref{sec::learned_evaluation::classification}.
Next, Section~\ref{sec::learned_evaluation::baseline} presents in details the baseline of features extracted out of \gls{acr::3d} building models.
Third and last, implementation details regarding the baseline features are discussed in Section~\ref{sec::learned_evaluation::implementation}.

\clearpage

\section{Quality evaluation as a classification task}
    \label{sec::learned_evaluation::classification}
    In order to satisfy the \textbf{large-scale} condition imposed at Section~\ref{subsec::introduction::contributions::positioning}, we propose to formulate the problem as a supervised learning one.
    Errors are considered as labels while features are computed so as to describe the observed buildings.
    Actually, as a first approach, the existence of all errors is predicted at the \textbf{building level}, even for \texttt{Facet Errors} labels\footnote{These errors are, in fact, by definition more local.}.
    Determining which facet is affected by an error is a much more challenging problem than the previous one.
    That is why the facet level prediction of errors is not explored in this work.
    The goal is, instead, to test the feasibility of the learning approach.\\

    Errors are predicted based on learned statistical characteristics of the evaluated models.
    The learned approach is usually used to take care of highly semantic tasks, such as ours, that are otherwise hard to process using engineered metrics.\\
    Provided an initial manual annotation effort, the prediction phase is fully automatic, as will be proved by experimental results in Chapter~\ref{chap::experiments}.
    In order for this approach to be scalable, and hence verify the second constraint in Section~\ref{subsec::introduction::contributions::positioning} ``\textbf{automation}'', prediction results should be stable enough independently of the urban scene.
    This is fully studied in Chapter~\ref{chap::more_experiments}.\\

    Two aspects should be discussed to in order to apply this approach to the building \gls{acr::3d} model quality evaluation.
    First, as seen in Section~\ref{sec::semantic_evaluation::label_extraction}, the parametric nature of the taxonomy leads to a varying set of labels.
    For this purpose, we describe in Section~\ref{subsec::learned_evaluation::classification::different_porblems} the different classification problems depending on the evaluation parameters.
    Secondly, the feature extraction procedure should also be compliant with the \textbf{large-scale} objective set beforehand.
    This aspect and more are detailed in Section~\ref{subsec::learned_evaluation::classification::feature_extraction}.
    Third, the classifier should be able to handle the heterogeneity of the feature vector and must adapt to different input vectors types and sizes.
    The choice of classifiers is, hence, discussed in Section~\ref{subsec::learned_evaluation::classification::classifiers}.

    \subsection{Different classification problems}
        \label{subsec::learned_evaluation::classification::different_porblems}
        The nature of the different classification problems are presented in Table~\ref{tab::classification_problems} depending on the three evaluation parameters defined in Section~\ref{sec::semantic_evaluation::label_extraction}.
        As a reminder, these are: the \textbf{\gls{acr::efin}}, the \textbf{\gls{acr::elod}} and the \textbf{exclusivity}.\\

        \begin{table}[htbp]
            \small
            \begin{tabular}{c c c x{11cm}}
                \toprule
                \textbf{\gls{acr::efin}} & \textbf{\gls{acr::elod}} & \textbf{exclusivity} & \textbf{Classification output}\\
                \midrule
                1 & --- & --- & $binary(\texttt{Valid}, \texttt{Erroneous})$\\
                2 & \gls{acr::lod}-1 & --- & $binary(\texttt{Valid}, \texttt{Building Errors})$\\
                2 & \gls{acr::lod}-2 & \textsc{on} & $multi\_class(\texttt{Valid}, \texttt{Building Errors}, \texttt{Facet Errors})$\\
                2 & \gls{acr::lod}-2 & \textsc{off} & $multi\_label(\texttt{Building Errors}, \texttt{Facet Errors})$\\
                3 & \gls{acr::lod}-1 & \textsc{on} & $multi\_stage(\texttt{Valid}, \texttt{Building Errors})$\\
                3 & \gls{acr::lod}-2 & \textsc{on} & $multi\_stage(\texttt{Valid}, \texttt{Building Errors}, \texttt{Facet Errors})$\\
                3 & \gls{acr::lod}-1 & \textsc{off} & $multi\_label(\operatorname{children}(\texttt{Building Errors}))$\\
                3 & \gls{acr::lod}-2 & \textsc{off} & $multi\_label(\operatorname{children}(\texttt{Building Errors})\cup \operatorname{children}(\texttt{Facet Errors}))$\\
                \bottomrule
            \end{tabular}
            \caption{
                \label{tab::classification_problems}
                All possible classification problem types depending of the evaluation parameters:
                \textbf{\gls{acr::efin}}, \textbf{\gls{acr::elod}} and \textbf{exclusivity}.
            }
        \end{table}

        In Table~\ref{tab::classification_problems}, $multi\_class(l_1, l_2, \dots, l_c)$ (\textit{resp.} $multi\_label(l_1, l_2, \dots, l_c)$) corresponds to the multi-class (\textit{resp.} multi-label) setting.
        We note that:
        \begin{equation*}
            multi\_label(\texttt{Valid}, l_1, l_2, \dots, l_c) \equiv multi\_label(l_1, l_2, \dots, l_c).
        \end{equation*}
        $binary$ refers to the special case of $multi\_class$ where $c = 2$: i.e.,
        \begin{equation*}
            binary(l_1, l_2) \equiv multi\_class(l_1, l_2)
        \end{equation*}
        Two consecutive classification problems can be concatenated in a hierarchical multi-stage classification:
        depending on the class that is predicted in the first stage multi-class classifier, a second classification problem predicts the existence of some corresponding labels.
        This denoted by:
        \begin{equation*}
            multi\_stage(l_1, l_2, \dots, l_3) \equiv multi\_label(\operatorname{children}(multi\_class(l_1, l_2, \dots, l_3))).
        \end{equation*}
            
        \textbf{\gls{acr::efin}} = 1 level corresponds to the standard binary classification problem: \texttt{Valid} or \texttt{Erroneous}.
        At \textbf{\gls{acr::efin}} = 2, the \textbf{\gls{acr::elod}} can then take two values in the aerial reconstruction case: \gls{acr::lod}-1 or \gls{acr::lod}-2.
        If set at \gls{acr::lod}-1, it is a binary classification problem: \texttt{Valid} or \texttt{Building Errors}.
        For \gls{acr::lod}-2, if the \textbf{exclusivity} is on, it will be a multi-class problem: \texttt{Valid}, \texttt{Building Errors} or \texttt{Facet Errors}.
        If set off, it becomes a multi-label one with the same labels.
        At \textbf{\gls{acr::efin}} = 3 level, if the \textbf{exclusivity} is on, it is a 2-stage classification problem.
        In the first stage, a multi-class task\footnote{It is binary in the special case \gls{acr::elod} = \gls{acr::lod}-1, problem, like in the previous semantic degree.}
        predicts the error family, after which a second multi-label problem decides between the predicted error family children.
        If the \textbf{exclusivity} is off, it turns into one stage multi-label problem that predicts the existence of each atomic error corresponding to the chosen \gls{acr::elod}.
    
    \subsection{Feature extraction}
        \label{subsec::learned_evaluation::classification::feature_extraction}
        The proposed quality evaluation approach, being formulated as a supervised classification problem, requires extracting feature vectors describing characteristics of the evaluated model.
        This is possible through the use of the intrinsic properties of the building model, as well as comparisons with external data.\\
    
        Intrinsic feature extraction consists in make use of the geometric structure of the \gls{acr::3d} model.
        Equally, semantics, as well as building model meta-data, could be utilized for the purpose of intrinsically evaluating a model.
        This case corresponds to the minimal amount of information one can use for building model evaluation.
        In this case, we talk about self-evaluation of building models.
        Since we are considering all possible cases, especially automatically reconstructed building models, only the model geometry is guaranteed to be always available.\\
    
        Extrinsic feature extraction relies on comparing the model to an available external data.
        Obviously, high quality reference models are the best type of data to compare the evaluated model with.
        However, taking into consideration the \textbf{large-scale} objective that was fixed earlier (Section~\ref{subsec::introduction::contributions::use}), this is not a viable solution.
        We rely then on more basic reference data such as remote sensing acquired data that are the basis of large-scale modeling of urban scenes.\\
        For instance, raw depth information can be used in quality evaluation.
        It can prove helpful in detecting geometric defects that are intrinsically of \gls{acr::3d} nature.
        This was illustrated in Figure~\ref{fig::fig}, as comparing the projected model to the orthoimage did not yield anything out of place, contrarily to the \gls{acr::dsm} comparison.
        Depth data can take multiple forms: unstructured point clouds, originating for instance from \gls{acr::lidar} sensors, or dense depth maps such as \gls{acr::dsm} for the overhead case.\\
        Optical images can also be employed in this framework.
        These provide complementary information such as edges (high frequencies, in general) and textures which are suited for inner defect detection as an example (cf. Figure~\ref{subfig::fus_2d}).
        This type of data comes usually in two different shapes: overhead images or orthoimages.
    
    \subsection{Classifier choice}
        \label{subsec::learned_evaluation::classification::classifiers}
        The choice of classifiers shoud take into consideration the highly modular nature of the framework with multimodal features involving many parameters.
        Two classifiers where chosen in this study: \gls{acr::rf} and \gls{acr::svm}.
        Both were discussed in details in Section~\ref{sec::state_of_the_art::mlpr}.
        Hereafter, we explain how each one is used in this setting.\\

        \subsubsection{\acrlong*{acr::rf}}
            \gls{acr::rf} classifiers is a natural choice in our case.
            As seen in Section~\ref{subsubsec::state_of_the_art::mlpr::classifiers::rf}, this type of classifiers can manage a large number of features with different dynamics and coming from multiple modalities.
            In fact, the computed features could be geometric, image based or height based.
            Each one of these modalities can also be heterogeneous in terms of extracted value types, as will be discussed later in Section~\ref{sec::learned_evaluation::baseline}.
            Relying on their bagging property, a high number of trees is required to cover most of the feature space, while a limited tree depth is needed to avoid overfitting during training.
            While the multi-class case is natively taken into account by the \gls{acr::rf} classifier, the multi-label one requires adopting a one-vs-all approach so as to address each label separately.

        \subsubsection{\acrshort*{acr::svm}}
            \glspl{acr::svm} do not manage well enough heterogeneity in feature vectors.
            Moreover, only binary classification is inherently handled.
            However, it offers other advantages that are not met by \gls{acr::rf}.
            In fact, \glspl{acr::svm} can be useful when labels are not equally distributed in the training set.
            This is actually the case of some errors that are rare in our dataset: specifically the inaccurate topology ones (cf. Section~\ref{subsec::experiments::datasets::stats}).
            It also manages to learn efficiently on sets with limited sets, as will prove to be the situation (cf. Section~\ref{subsec::experiments::datasets::scenes}). 
            This type of classifiers naturally encorporates kernels such as the ones presented later in Section~\ref{subsec::better_representation::evaluation::graph}.
            Finally, it is also preferred when dealing with high dimensional feature vectors like those produced by \glspl{acr::scatnet} (cf. Section~\ref{subsec::better_representation::evaluation::image}).

\section{Feature baseline}
    \label{sec::learned_evaluation::baseline}
    Since there is no comparable work that studied the learned detection of errors defined in Chapter~\ref{chap::semantic_evaluation}, we propose a baseline for each one of the three modalities: geometric, height based and image based features.
    Attributes are kept simple so as to be used in most situations relying on generally available data.
    We avoid computing and comparing \gls{acr::3d} lines~\parencite{michelin2013quality}, correlation scores~\parencite{boudet2006supervised} or any \gls{acr::sfm} based metric~\parencite{kowdle2011active}.
    In addition of being very costly, these features are methodologically redundant with the \gls{acr::3d} modeling techniques.
    They are, hence, vulnerable to the same defects.
    Conversely, evaluation metrics used in the \gls{acr::3d} building reconstruction literature (e.g., \gls{acr::rmse}) are too weak for such a complex task.
    This will be proven later on in Section~\ref{sec::experiments::baseline_feature_analysis}.

    In Section~\ref{subsec::learned_evaluation::baseline::geometric}, we describe the used baseline for geometric features.
    Next, in Section~\ref{subsec::learned_evaluation::baseline::height}, height based features extraction is explained.
    We end with image based features in Section~\ref{subsec::learned_evaluation::baseline::image}.

    \subsection{Geometric features}
        \label{subsec::learned_evaluation::baseline::geometric}
        Given a building model \(\mathsf{M}\), the facet set is denoted by $\mathsf{F_M}$.
        $\forall (f, g) \in \mathsf{F_M} \times \mathsf{F_M} \; f \sim g$ correspond to facets \(f\) and \(g\) being adjacent: 
        i.e., they share a common edge. As the roof topology graph in~\parencite{verma20063d}, the input building model $\mathsf{M}$ can be seen as a facet (dual) graph:

        \begin{equation}
        	\label{eq::model_graph}
        	\mathsf{M} \triangleq \left(\mathsf{F_M}, \mathsf{E_M} \triangleq \left\{ \left\{f, g\right\} \subset \mathsf{F_M} : f \sim g \right\} \right).
        \end{equation}

        \begin{figure}[htb]
            \centering
            \includestandalone[mode=buildnew, width=.75\textwidth]{figures/features/geometry/baseline}
            \caption[
                Computed geometric attributes represented on the dual graph, for facets \(f\) and \(g\).
            ]{
                \label{fig::geometric_baseline_features}
                Computed geometric attributes represented on the dual graph, for facets \(f\) and \(g\).
                The green vector groups the node (facet) attributes while the blue one shows the edge features.
            }
        \end{figure}

        The dual graph is illustrated in Figure~\ref{fig::geometric_baseline_features}.
        For each facet $f \in \mathsf{F_M}$, we compute its degree (i.e., number of vertices; $f \mapsto d\left(f\right) \triangleq \left\lvert\left\{v : v\text{ is a vertex of }f\right\}\right\rvert$), its area $f \mapsto \mathscr{A}\left(f\right)$ and its circumference $f \mapsto \mathscr{C}\left(f\right)$.
        These are all geometric invariants with respect to $\mathbb{R}^3$ isometries, contrarily to facet centroids $\mathscr{G}\left(f\right)$ and normals $\vec{n}\left(f\right)$.
        This is countersteped by looking, for each graph edge $e=\left\{f, g\right\} \in \mathsf{E_M}$, for the distance between facet centroids $\left\{f, g\right\} \mapsto \left\lVert \mathscr{G}\left(f\right) - \mathscr{G}\left(g\right) \right\rVert$ and the angle formed by their normals $\left\{f, g\right\} \mapsto \arccos\left(\vec{n}\left(f\right) \cdot \vec{n}\left(g\right)\right)$.
        Statistical characteristics are then computed over building model facets using specific functions \(\chi\), like a histogram:        

        \begin{equation}
            \label{eq::histogram_extractor}
        	\chi = \chi^p_{\operatorname{histogram}}: l \mapsto \operatorname{histogram}(l, p),
        \end{equation}
        with $p$ standing for histogram parameters. A simpler option could be:
        \begin{equation}
            \label{eq::max_min_mean_med_extractor}
            \chi = \chi_{\max,\min,\operatorname{mean},\operatorname{med}}: l \mapsto \begin{bmatrix}
                \max(l)\\
                \min(l)\\
                \operatorname{mean}(l)\\
                \operatorname{median}(l)
            \end{bmatrix}.
        \end{equation}

        These features are designed for general topological errors.
        For instance, over-segmentation may result in small facet areas and small angles between their normals.
        Conversely, an undersegmented facet would have a large area.
        Later on, the importance of these features will be discussed in details based on experimental results.
        
        Each building $\mathsf{M}$ can consequently be characterized by a geometric feature vector that accounts for its geometric characteristics:

        \begin{equation}
        	\label{eq::geometric_baseline_features}
            v_{\text{geometry}}(\mathsf{M}) = \begin{bmatrix}
            	\chi \left(\left(d\left(f\right)\right)_{f \in \mathsf{F_M}}\right)\\
                \chi \left(\left(\mathscr{A}\left(f\right)\right)_{f \in \mathsf{F_M}}\right)\\
                \chi \left(\left(\mathscr{C}\left(f\right)\right)_{f \in \mathsf{F_M}}\right)\\
                \chi \left(\left( \left\lVert \mathscr{G}\left(f\right) - \mathscr{G}\left(g\right) \right\rVert \right)_{\left\{f, g\right\} \in \mathsf{E_M}}\right)\\
                \chi \left(\left( \arccos\left(\vec{n}\left(f\right) \cdot \vec{n}\left(g\right)\right) \right)_{\left\{f, g\right\} \in \mathsf{E_M}}\right)
            \end{bmatrix}.
        \end{equation}

        Additionally to individual facet statistics, regularity is taken into account by looking into adjacent graph nodes as in~\parencite{zhou20102}.
        Such features express a limited  part of structural information.
        Dealing with this type of information implies graph comparisons which are not a genuinely simple task to achieve.
        Since our objective is to build a baseline, this approach has not yet been considered.

    \subsection{Height based features}
        \label{subsec::learned_evaluation::baseline::height}
        Regarding this modality, raw depth information is provided, for a building model \(\mathsf{M}\), by a \gls{acr::dsm} as a \gls{acr::2d} height grid that is cropped to fit the building footprint: $dsm \in \mathbb{R}^{h_{\mathsf{M}} \times w_{\mathsf{M}}}$\footnote{\label{note::w_h}\(w_{\mathsf{M}}\) (\textit{resp.} \(h_{\mathsf{M}}\)) is the grid width (\textit{resp.} height) and is determined by the size of the building and the resolution of the \gls{acr::dsm}.}.
        This type of reference data must date back to the same time where the building models where produced.
        Otherwise a lot of defects will result simply from change in the scenery.\\
        
        \begin{figure}[htb]
            \centering
            \includestandalone[width=\textwidth, mode=buildnew]{figures/features/height/height_baseline}
            \caption[
                Illustration of the baseline for height based features.
            ]{
                \label{fig::height_baseline_features}
                Illustration of the baseline for height based features.
                First, residuals (cf. Figure~\ref{subfig::residuals}) are computed by substracting the model height maps (cf. Figure~\ref{subfig::height_map}) from the \glspl{acr::dsm} (cf. Figure~\ref{subfig::dsm}).
                Histograms are then computed out of these residuals and taken as feature vectors as shown in Figure~\ref{subfig::histogram}.
            }
        \end{figure}

        The \gls{acr::dsm} is compared to the model height~\parencite{bredif20073d,zebedin2008fusion}.
        The latter is inferred from its facets plane equations.
        It is rasterized into an grid structure $alt_{\mathsf{M}} \in \mathbb{R}^{h_{\mathsf{M}} \times w_{\mathsf{M}}}$ using the same spatial resolution as $dsm_{\mathsf{M}}$.
        The difference between the two height grids provides a discrepancy map as shown in Figure~\ref{subfig::residuals}).
        A baseline approach is herein proposed relying on the statistics of pixel values computed using the $\chi$ functions (cf. Figure~\ref{fig::height_baseline_features}).\\

        \begin{equation}
            \label{eq::height_baseline_features}
            v_{\text{height}}\left(\mathsf{M}\right) = \chi \left( dsm_{\mathsf{M}} - alt_{\mathsf{M}} \right).
        \end{equation}

        The histogram could actually be computed for the building alone without taking into account the terrain arround it.
        However, since reference data is unavailable, cropping out the terrain height values implies that the building model footrpint is flawless, which is not the case.
        As a consequence, the heigth discrepancies around the building model are also computed in order to provide some information on the footprint shape, and hence detect \texttt{BIT} or \texttt{BIB} errors.\\

        Equation~\ref{eq::height_baseline_features} summarizes how building height based features are computed.
        Different from a \gls{acr::rmse}~\parencite{lafarge2012creating,poullis2013framework}, the histogram captures the discrepancy distribution, which is particularly helpful in detecting undersegmentation defects or geometric imprecision.
        However, as for the previous geometric attributes, the grid structure information coming from the model is lost.
        Errors cannot be spatialized and linked to a specific facet.

    \subsection{Image based features}
        \label{subsec::learned_evaluation::baseline::image}
        The framework is general enough to encompass both orthorectified images and overhead ones.
        For now, we rely only on more accessible orthorectified images, eventhough they can be riddled with artifacts.
        In an ideal scenario, using oriented images is better for edge verification (as already shown in~\parencite{michelin2013quality}) as orthoimages are a byproduct of earlier ones.
        However, in practice, overlapping overhead imagery would give rise to other issues, especially, registration problems.\\

        We aim to benefit from the high frequencies in \gls{acr::vhr} optical images.
        Building edges, like any image edge, correspond to sharp discontinuities in images~\parencite{ortner2007building}.
        The latter are detected using gradient filters on images.
        Gradient based features are advantageous compared to any radiometry based ones.
        This is due to the fact that the latter are much less invariant to changes in the than the first ones.
        Indeed, classic Computer Vision techniques rely on gradient based features:~\parencite{lowe2004distinctive,dalal2005histograms}.
        This is adequate to our case where models and images are part of large heterogeneous datasets.\\

        \begin{figure}[htb]
            \centering
            \includestandalone[width=\textwidth, mode=buildnew]{figures/features/image/nadir_projection}
            \caption[
                Nadir projection of \gls{acr::3d} models to image comparison.
            ]{
                \label{fig::nadir_projection}
                Nadir projection of \gls{acr::3d} models to image comparison.
                The input model (cf. Figure~\ref{subfig::input_model}) is projected in the nadir direction as shown in Figure~\ref{subfig::nadir_projection}.
                It is then superposed on the orthoimage (cf. Figure~\ref{subfig::orthoimage}) and compared to it, as shown in Figure~\ref{subfig::image_model_comparison}.
            }
        \end{figure}
        
        We apply this to our context by comparing building model edges to local image gradients.
        We start by projecting building models in the nadir direction, as shown in Figure~\ref{fig::nadir_projection}.
        For each facet \(f \in \mathsf{F_M}\), this operation takes into account occlusions and results in:
        \begin{description}
            \item[A \gls{acr::2d} polygon:] If the facet is not vertical (i.e., \(\vec{n}(f) \cdot \vec{z} \neq 0\)) and not completly occluded by other facets;
            \item[A segment:] If the facet is vertical (i.e., \(\vec{n}(f) \cdot \vec{z} = 0\));
            \item[An empty polygon:] If the facet is completly occluded by other facets.
        \end{description}
        The last two cases are filtered out and the final projection consists in a set of polygons forming a partition of the building footprint denoted:
        \begin{equation}
            \label{eq::facet_projections}
            \mathsf{F_M}^q \triangleq \{q(f): f \in \mathsf{F_M}\}
        \end{equation}
        where:
        \begin{description}
            \item[\(q\)] is the function that yields the projection of a facet if it is a polygon or an empty polygon othewise.
        \end{description}

        This projection is compared with a corresponding orthorectified image \(I_{\mathsf{M}} \in \mathbb{R}^{h_{\mathsf{M}} \times w_{\mathsf{M}} \times 3}\) as shown in Figure~\ref{subfig::edges_image_comparison}.
        For each facet projection, we isolate each edge \(s\) (Figure~\ref{subfig::edge_image_comparison}).
        In an ideal setting, gradients computed at pixels $g \in I_{\mathsf{M}}$ that intersect \(s\) need to be collinear with its normal $\vec{n}(s)$.
        In consequence, applying a statistics functions $\chi$\footnote{
            For instance, the functions defined in Equations~\ref{eq::max_min_mean_med_extractor} or~\ref{eq::histogram_extractor}.
        }, we compute a distribution of the cosine similarity between the local gradient and the normal all along each edge \(s\) (cf. Figure~\ref{subfig::edge_histograms}):
        
        \begin{equation}
            \label{eq::corr_seg}
            \mathsf{D}_{\chi}(s, I_{\mathsf{M}}) \triangleq \chi \left( \left(\frac{\nabla I_{\mathsf{M}}\left(g\right) \cdot \vec{n}(s)}{\left\rVert \nabla I_{\mathsf{M}}\left(g\right)\right\rVert}\right)_{\substack{g \in I_{\mathsf{M}} \\ g \cap s \neq \emptyset}} \right).
        \end{equation}

        \begin{figure}[htb]
            \begin{center}
                \floatbox{figure}{
                    \begin{subfloatrow}[2]
                        \ffigbox[\FBwidth]{
                            \caption{
                                \label{subfig::edge_image_comparison}
                                Local gradients (in purple), on intersecting pixels (in green), are compared to the edge (in red) normal (in black).
                            }
                        }{
                            \includestandalone[mode=buildnew, width=.45\textwidth]{figures/features/image/edge_image_comparison}
                        }
                        \quad
                        \ffigbox[\FBwidth]{
                            \caption{
                                \label{subfig::edge_histograms}
                                Histograms describing the similarity between edges and the orthoimage.
                            }
                        }{
                            \includestandalone[mode=buildnew, width=.45\textwidth]{figures/features/image/edges_histograms}
                        }
                    \end{subfloatrow}
                }{
                    \caption{
                        \label{fig::edges_image_comparison}
                        Illustration of how edges from the projected model are compared to the orthoimage.
                    }
                }
            \end{center}
        \end{figure}

        For each polygon \(f^q \in \mathsf{F_M}^q\), the distributions over all its edges\footnote{The empty polygons are ignored as they have no edges.} \(s \in f^q\)\footnote{Abuse of notation.} are stacked to yield a distribution over the whole projected facet (cf. Figure~\ref{subfig::facets_histograms}).
        In the case of histograms $\chi^p_{\operatorname{histogram}}$ with the same parameters \(p\) (and thus the same bins), it is equivalent to summing out the previous vectors $\mathsf{D}_{\chi^p_{\operatorname{histogram}}}(s, I_{\mathsf{M}})$.
        In order to take into account the variability of segment dimensions, this sum is weighted by segment lengths.\\
        \begin{equation}
            \label{eq::corr_fac}
            D_{\chi^p_{\operatorname{histogram}}}\left(f^q, I_{\mathsf{M}}\right) \triangleq \sum_{s \in q\left(f\right)} \left\rVert s \right\lVert \cdot \mathsf{D}_{\chi^p_{\operatorname{histogram}}}(s, I_{\mathsf{M}}).
        \end{equation}

        The same can be done over all facets of a building $\mathsf{M}$ (cf. Figure~\ref{subfig::whole_histogram}).
        The weights are added in order to take into account the geometry heterogeneity.
        The gradient to normal comparison is similar to the \gls{acr::3d} data fitting term formulated in~\parencite{li2016manhattan}.
        Once again, the model structure is partially lost when simply summing (weighted by the projected facet area) histograms over all segments.

        \begin{equation}
            \label{eq::corr_bul}
            v_{\text{image}}\left(\mathsf{M}\right) = D_{\chi^p_{\operatorname{histogram}}}\left(\mathsf{M}, I_{\mathsf{M}}\right) \triangleq \sum_{f^q \in \mathsf{F_M}^q} \mathscr{A}\left(f^q\right) \cdot \mathsf{D}_{\chi^p_{\operatorname{histogram}}}(f^q, I_{\mathsf{M}}).
        \end{equation}
        
        \begin{figure}[htb]
            \begin{center}
                \floatbox{figure}{
                    \begin{subfloatrow}[2]
                        \ffigbox[\FBwidth]{
                            \caption{
                                \label{subfig::facets_histograms}
                                Depiction of histograms describing the similarity of each of the model's projected facet to the orthoimage.
                            }
                        }{
                            \includestandalone[mode=buildnew, width=.45\textwidth]{figures/features/image/facets_histograms}
                        }
                        \quad
                        \ffigbox[\FBwidth]{
                            \caption{
                                \label{subfig::whole_histogram}
                                Illustration of the final histogram describing the similarity between the model edges and the orthoimage.
                            }
                        }{
                            \includestandalone[mode=buildnew, width=.45\textwidth]{figures/features/image/whole_histogram}
                        }
                    \end{subfloatrow}
                }{
                    \caption[
                        Illustration of the baseline for image based features.
                    ]{
                        \label{fig::image_baseline}
                        Illustration of the baseline for image based features.
                        The comparison between a model and the orthoimage is conducted by aggregating the edge similarity histograms (cf. Figure~\ref{subfig::edge_histograms}):
                        First at facet level, which results in a histogram per each of the projected facets (cf. Figure~\ref{subfig::facets_histograms}), and then at the whole model level, with a final output describing the similarity between the input \gls{acr::3d} model and the orthoimage as shown in Figure~\ref{subfig::whole_histogram}.
                    }
                }
            \end{center}
        \end{figure}

        These image based attributes (cf. Figure~\ref{fig::image_baseline}) are helpful for precision error detection.
        As example, facet imprecise borders can be detected as local gradients direction will be expected to differ greatly from the inaccurate edge.
        It can also be instrumental in under-segmentation detection as colors can change considerably from one facet or one building to another inducing an gradient orthogonal to edge normals.

\section{Implementation details}
    \label{sec::learned_evaluation::implementation}
    In this section, we give a detailed account of how every ingredient of our pipeline is parameterized.
    We first start, in Section~\ref{subsec::learned_evaluation::implementation::feature_configurations}, by specifying the different feature configurations that are used in the experimental study and how there were implemented.
    Next, in Section~\ref{subsec::learned_evaluation::implementation::classification}, we discuss in detail how the classification process was conducted.

    \subsection{Feature configurations}
        \label{subsec::learned_evaluation::implementation::feature_configurations}
        We present herein the features that were used in experiments.
        The main objective herein is to prove the efficiency of the proposed learning framework.
        Hence, we test different configurations using the baseline features.\\

        The geometric features are intrinsic and are always available.
        As a consequence, we tested four feature configurations: ``geometric features'' (\textbf{Geom.}) only, ``geometric and height features''(\textbf{Geom. \(\oplus\) Hei.}), ``geometric and image features''(\textbf{Geom. \(\oplus\) Im.}) as well as ``geometric, height and image features''(\textbf{All}).
        In order to have a better understanding of how important each modality is, their feature vectors are by design drawn to be of the same size: 20.\\

        Actually, using the function \(\chi_{\max,\min,\operatorname{mean},\operatorname{med}}\) defined in Equation~\ref{eq::max_min_mean_med_extractor}, the geometric feature vector in Equation~\ref{eq::geometric_features} is of dimension
        \begin{equation*}
            \underbrace{4}_{\substack{\text{the output dimension}\\\text{of the function } \chi_{\max,\min,\operatorname{mean},\operatorname{med}} }} \times \underbrace{5}_{\substack{\text{the number of the facet}\\\text{graph attributes}}} = 20.
        \end{equation*}
        Regarding height based feature vectors, their dimension depends on the histogram parameters.
        Since we compute differences between observed and model height at terrain level also (cf. Section~\ref{subsec::learned_evaluation::baseline::height}), and because these are virtually unbounded and can differ from one scene to another, the maximum possible discrepancy is limited to \SI{\pm 50}{\m} for all scenes.
        In order to have the same feature vector dimension for this modality as for the geometric one, the number of bins is fixed manually to \num{20}.\\
        For image based features, the cosine similarity between normal vectors, used in Equation~\ref{eq::corr_fac}, is by definition bounded in the interval [-1, 1].
        This interval is consequently divided evenly into \num{20} bins for the histogram computation.
        The \glspl{acr::dsm} and orthorectified images used to derive height and image features have the same spatial resolution as the reconstruction input data.\\

        These features implementation is conducted in \verb!Python! and is not optimized.
        Geometric features are computed in average in \SI{0.05}{\s \per \building}, height based ones take in average \SI{1.4}{\s \per \building} and finally image based ones need more than \SI{69}{\s \per \building}.
        In order to reduce the runtime of each experiment, the last two types of features are cached once computed for a building model and retrieved latter for tests.
    \subsection{Classification settings}
        \label{subsec::learned_evaluation::implementation::classification}
        Feature extractors being dicussed above, we give now extensive details how the considered classifiers (cf. Section~\ref{subsec::learned_evaluation::classification::classifiers}) are applied.
        We also provide the metrics that will help measure the accuracy of predictions.

        \subsubsection{Considered labels}
            All \textbf{\gls{acr::efin}} levels were tested.
            The input models are generalized to \gls{acr::lod}-2.
            As a consequence, we chose \textbf{\gls{acr::elod}} = \gls{acr::lod}-2.
            If the \textbf{exclusivity} is \textsc{on}, at \textbf{\gls{acr::efin}} level 3, the second stage classification results depend on the first one.
            After a brief experimental study, the first stage did not yield good enough results in order to test this configuration.
            In fact, we remind the reader that, at this stage, our goal is, first and foremost, to prove the feasibility of the proposed approach.
            That is why we limited the experiments to the case where the \textbf{exclusivity} is set to be \textsc{off}.

        \subsubsection{Classifiers}
            As discussed in Section~\ref{subsec::learned_evaluation::classification::classifiers}, two classifier types were considered in the experimental study.
            As already explained beforehand in Section~\ref{subsec::learned_evaluation::classification::classifiers}, a one-vs-all approach is used to adapt \glspl{acr::rf} to multi-label settings.
            For now, we use only a \gls{acr::rf} classifier with baseline features to conduct the first sets of experiments since it was the easiest to parameterize.
            We relied upon \verb!scikit-learn! the already available and ubequitous implementation in \verb!Python!~\parencite{scikit-learn}.\\
            
            A standard grid search involving a smaller set of building models and only baseline geometric features~\parencite{ennafii2018qualificationunannotated} yielded comparable results for the number of trees set in the range \numrange{850}{1000} and a maximum tree depth from \numrange{3}{5}.
            Given the already immense parameter search space involving all possible feature configurations, feature extraction parameters and label possibilities, the \gls{acr::rf} parameters are set to \num{1000} for the tree number and 4 for the maximum tree depth for all other experiments without performing any grid search.
    
        \subsubsection{Metrics for quantitative assessment}
            The overall accuracy is not interesting due to the highly unbalanced label distribution.
            We prefer reporting recall \(\bm{Rec}\) and precision \(\bm{Prec}\) ratios.
            As a reminder these metrics are defined as follows:
            \begin{align}
                \label{eq::recall_precision}
                \bm{Rec} &\triangleq \frac{tp}{tp + fp}\\
                \bm{Prec} &\triangleq \frac{tp}{tp + fn},
            \end{align}
            where:
            \begin{conditions}
                tp & the number of instances predicted positive that are positive in reality;\\
                fp & the number of instances predicted positive that are negative in reality;\\
                fn & the number of instances predicted negative that are positive in reality.
            \end{conditions}
            Recall expresses, from a number of samples of a given class, the proportion that was rightfully detected as such.
            Precision indicates how much samples, amongst the detected ones, were, in truth, part of the studied class~\parencite{powers2011evaluation}.
            We also summarize these two ratios with their harmonic mean, the F-score:
            \begin{equation}
                \label{eq::f_score}
                \bm{F_{score}} \triangleq \frac{2}{\frac{1}{\bm{Rec}} + \frac{1}{\bm{Prec}}}.
            \end{equation}
            Unless said otherwise, all experiments were conducted performing a 10-fold cross validation to avoid overfitting or underfitting issues.
            Only test results are reported.
