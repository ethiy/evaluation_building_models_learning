\minitoc

\vfill

\clearpage

\section{Automatic building \gls*{acr::3d} modelling}
    \label{sec::state_of_the_art::building_modeling}

    \subsection{Input data classification}
        \label{subsec::state_of_the_art::building_modeling::input}

    \subsection{Strategy classification}
        \label{subsec::state_of_the_art::building_modeling::strategy}

    \subsection{Scale classification}
        \label{subsec::state_of_the_art::building_modeling::scale}

\section{Quality evaluation of \gls*{acr::3d} building models}
    \label{sec::state_of_the_art::quality}
    We have seen previously the various methods used to automatically model building.
    The goal of this section is to describe the available approaches that evaluate the quality of such models.
    These could be distinguished based on two criteria.
    In subsection~\ref{subsec::state_of_the_art::quality::output}, are presented the quality evaluation methods based on their output.
    An alternative prespective to charecterize quality evaluation methods: it relies upon the type of reference data (\textit{cf.} subsection~\ref{subsec::state_of_the_art::quality::reference}).

    \subsection{Output types}
        \label{subsec::state_of_the_art::quality::output}
        We distinguish herein quality evaluation methods based on the output they produce.
        Fidelity metrics is a first instance of output types.
        The second is semantic labels.
        In what follows, we explain, in details, the differences between the two method types.

        \subsubsection{Fidelity metrics based methods}
            One way to charecterize the quality of a building model is to compute indices or metrics reporting its accuracy.\\

            Most metrics provide information on the geometric precision of the model.
            These are computed at different levels.
            They are categorized here depending on the geometric dimension of the objects in question.\\
            We start with zero dimensional objects: \textit{i.e.} points.
            In this case, metrics are based on their coordinates.
            The goal is to detect positional inaccuraccies~\parencite{kaartinen2005accuracy}.
            In constrast, the choice of points to be inspected is not simple.
            Corner points resulting from the intersection of edges in the model is one choice.
            In fact,~\textcite{zeng2014multicriteria} registers corner points from the evaluated model and the corresponding reference.
            Based on this registration, a comparison is drawn using \gls{acr::rmse}, just as in~\parencite{landes2012quality} and~\parencite{you2011quality}.
            The same points are used as a proxy for manual quality inspection by~\textcite{elberink2011quality}.
            Another alternative is to sample points from lines or surfaces to be compared.
            These could be predetermined manually as in~\textcite{kaartinen2005accuracy} or sampled regularely as demonstrated by~\textcite{vogtle2003quality} or by~\textcite{tran2019geometric}.
            Imprecisions are not computed only relying the \gls{acr::rmse}, but can also be separated into planimetric and height inaccuraccies~\parencite{vogtle2003quality, kaartinen2005accuracy}.\\
            Second comes edges and all one dimensional objects in general.
            These convey structural, in addition to positional, informations.
            \textcite{kaartinen2005accuracy} compares lengths as well as slopes of edges formed by reference points.
            Edges metrics are also used as a intermediary as shown by~\textcite{elberink2011quality} and~\textcite{michelin2013quality}.
            They are both interested in intersection edges.
            While the first relies on \gls{acr::rmse}, the second computes more complex metrics that compares model edges to ones that are extracted based on sensor data.\\
            Next, at the second dimension, are compared surfaces, bounded (for example, polygons) or not (like planes).
            These hold more topological information than the first ones and hence are widely used for evaluation.
            \textcite{rottensteiner2014results} used height discrepency of roof planes so as to evaluate building models.
            This ideal for Manhattan-world assumptions as was the case of~\textcite{zebedin2008fusion}.
            In addition to height discrepency, normal displacement is computed using always the same metric by~\textcite{henricsson19973}.
            Conversely,~\textcite{zeng2014multicriteria} uses also a \gls{acr::rmse} for comparison, but not in the Euclidean space.
            In fact, after mapping the evaluated and reference models to a sphere, they compare their spherical harmonic~\parencite{brechbuhler1995parametrization} representations.
            Just as with edges, planes can be evaluated using angular measurements, as was proposed by~\textcite{landes2012quality} and~\textcite{henricsson19973}.
            Another alternative is to compare reconstructed and reference models based on surface area comparisons.
            These are mostly based on ratios like completeness and correctness\footnote{in other words, precision and recall}~\parencite{rottensteiner2014results,landes2012quality,henricsson19973,schuster2003new}.\\
            Last, are three dimensional (\textit{i.e.} volume) evaluation.
            The same detection ratios that were computed for surfaces are again calculated to evaluate volumes this times, as shown by~\textcite{mohamed2013quality, zeng2014multicriteria,jaynes2003recognition,nguatem2017modeling}.
            These are the only metrics used for volume that we are aware of.\\

            Regarding \textit{implicit} semantics, as far as we are aware, only one metric is widely used to evaluate its impact.
            As discussed previously in subsection~\ref{subsec::introduction::urban_3d_reconstruction::building_3d_modeling}, compaction is one byproduct of semantics.
            As a consequence, it was used as a index to evaluate reconstructions\footnote{It is sometimes called by its antonym: complexity.}: the more a model was compact the better it was.
            This is reflected, for instance, in the work of~\textcite{lafarge_ijcv12},~\textcite{zhang2017deep},~\textcite{duan_eccv16} and~\textcite{zhu2018large}.

        \subsubsection{Semantic labels based methods}

    \subsection{Reference data types}
        \label{subsec::state_of_the_art::quality::reference}

        \subsubsection{High resolution ground truth}

        \subsubsection{Raw remote sensing data}

    \subsection{Objective statement}
\section{Supervised learning and pattern recognition}
    \subsection{Classifiers}
        \subsubsection{Perceptron}
        \subsubsection{SVM}
            \paragraph{Linear SVM}
            \paragraph{Kernel SVM}
                \subparagraph{Usual kernels}
                \subparagraph{MKL}
            \paragraph{Properties}
        \subsubsection{Random Forest}
            \paragraph{Decision trees}
            \paragraph{Bagging decision trees}
            \paragraph{Properties}
    \subsection{Computer vision}
        \subsection{Edges and corners}
        \subsubsection{Scattering}
        \subsubsection{Convolutional neural networks}
    \subsection{Graph classification}
        \subsubsection{Graph kernels}
        \subsubsection{Graph neural networks}
